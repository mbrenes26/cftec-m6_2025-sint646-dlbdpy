{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42591e1",
   "metadata": {},
   "source": [
    "# Laboratorio 1 ‚Äì Comparaci√≥n de Bases de Datos NoSQL (MongoDB, Redis, HBase)\n",
    "\n",
    "**Objetivo:**\n",
    "Este laboratorio tiene como objetivo montar servidores de **MongoDB**, **Redis** y **HBase** en contenedores Docker, y luego interactuar con ellos desde Python para:\n",
    "\n",
    "1. Cargar un dataset real de compras electr√≥nicas.\n",
    "2. Ejecutar consultas espec√≠ficas en cada base de datos:\n",
    "   - ¬øCu√°l es la **categor√≠a** m√°s vendida?\n",
    "   - ¬øCu√°l **marca** gener√≥ m√°s ingresos brutos?\n",
    "   - ¬øQu√© **mes** tuvo m√°s ventas? (en UTC)\n",
    "3. Medir y comparar los tiempos de ejecuci√≥n entre las tres bases de datos.\n",
    "\n",
    "**Dataset utilizado:**\n",
    "[Ecommerce Purchase History from Electronics Store](https://www.kaggle.com/datasets/mkechinov/ecommerce-purchase-history-from-electronics-store)\n",
    "\n",
    "**Pasos principales:**\n",
    "1. Preparar el entorno de Python en esta m√°quina virtual.\n",
    "2. Descargar y preparar el dataset.\n",
    "3. Insertar datos en **MongoDB**, **Redis** y **HBase**.\n",
    "4. Ejecutar las consultas en cada base de datos.\n",
    "5. Analizar los tiempos de respuesta.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a390b808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.local/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: pymongo in ./.local/lib/python3.8/site-packages (4.10.1)\n",
      "Requirement already satisfied: redis in ./.local/lib/python3.8/site-packages (6.1.1)\n",
      "Requirement already satisfied: happybase in ./.local/lib/python3.8/site-packages (1.2.0)\n",
      "Requirement already satisfied: thriftpy2 in ./.local/lib/python3.8/site-packages (0.5.3)\n",
      "Requirement already satisfied: kaggle in ./.local/lib/python3.8/site-packages (1.7.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in ./.local/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.local/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in ./.local/lib/python3.8/site-packages (from pymongo) (2.6.1)\n",
      "Requirement already satisfied: async-timeout>=4.0.3; python_full_version < \"3.11.3\" in ./.local/lib/python3.8/site-packages (from redis) (5.0.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from happybase) (1.14.0)\n",
      "Requirement already satisfied: ply<4.0,>=3.4 in ./.local/lib/python3.8/site-packages (from thriftpy2) (3.11)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /usr/lib/python3/dist-packages (from kaggle) (1.25.8)\n",
      "Requirement already satisfied: bleach in ./.local/lib/python3.8/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from kaggle) (2.8)\n",
      "Requirement already satisfied: charset-normalizer in ./.local/lib/python3.8/site-packages (from kaggle) (3.4.2)\n",
      "Requirement already satisfied: protobuf in ./.local/lib/python3.8/site-packages (from kaggle) (5.29.5)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/lib/python3/dist-packages (from kaggle) (2019.11.28)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from kaggle) (2.22.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/lib/python3/dist-packages (from kaggle) (45.2.0)\n",
      "Requirement already satisfied: python-slugify in ./.local/lib/python3.8/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: text-unidecode in ./.local/lib/python3.8/site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: webencodings in ./.local/lib/python3.8/site-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (from kaggle) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# Instalaci√≥n de librer√≠as necesarias\n",
    "!pip3 install --user pandas pymongo redis happybase thriftpy2 kaggle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8aa4cf",
   "metadata": {},
   "source": [
    "## üì• Descarga del Dataset desde Kaggle\n",
    "\n",
    "Para trabajar con el dataset real, utilizaremos la API de Kaggle.  \n",
    "Esto requiere que tengamos un **token de acceso** que puedes obtener en tu cuenta de Kaggle:\n",
    "\n",
    "1. Inicia sesi√≥n en [Kaggle](https://www.kaggle.com/).\n",
    "2. Ve a **Account** ‚Üí **Create New API Token**.\n",
    "3. Esto descargar√° un archivo llamado `kaggle.json`.\n",
    "4. Lo subiremos a esta m√°quina en la carpeta `~/.kaggle/`.\n",
    "\n",
    "Este archivo contiene:\n",
    "- `username`: tu usuario en Kaggle.\n",
    "- `key`: clave de acceso para autenticaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d45ac5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Carpeta .kaggle lista. Sube tu archivo kaggle.json a esta ruta en la VM:\n",
      "/home/azureuser/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Crear carpeta .kaggle si no existe\n",
    "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Carpeta .kaggle lista. Sube tu archivo kaggle.json a esta ruta en la VM:\")\n",
    "\n",
    "# Mostrar ruta destino\n",
    "print(os.path.expanduser(\"~/.kaggle/kaggle.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a60aab",
   "metadata": {},
   "source": [
    "## üîì Ajustar permisos y descargar dataset\n",
    "\n",
    "Despu√©s de subir el archivo `kaggle.json` a la carpeta `.kaggle/` debemos ajustar los permisos para que sea seguro.  \n",
    "Luego utilizaremos la librer√≠a `kaggle` para descargar el dataset directamente en esta m√°quina.\n",
    "\n",
    "El dataset se guardar√° en la carpeta `datasets/ecommerce/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6b62cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/mkechinov/ecommerce-purchase-history-from-electronics-store\n",
      "License(s): copyright-authors\n",
      "Downloading ecommerce-purchase-history-from-electronics-store.zip to ./datasets/ecommerce\n",
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 36.0M/50.5M [00:00<00:00, 377MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50.5M/50.5M [00:00<00:00, 377MB/s]\n",
      "‚úÖ Dataset descargado y descomprimido en ./datasets/ecommerce\n"
     ]
    }
   ],
   "source": [
    "# Ajustar permisos del token Kaggle\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Descargar el dataset desde Kaggle\n",
    "!kaggle datasets download -d mkechinov/ecommerce-purchase-history-from-electronics-store -p ./datasets/ecommerce --unzip\n",
    "\n",
    "print(\"‚úÖ Dataset descargado y descomprimido en ./datasets/ecommerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a67195c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kz.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"./datasets/ecommerce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f1791",
   "metadata": {},
   "source": [
    "## üìÇ Carga y vista previa del Dataset\n",
    "\n",
    "El dataset descargado desde Kaggle contiene informaci√≥n sobre el historial de compras en una tienda de electr√≥nica.  \n",
    "El archivo descargado se llama `kz.csv` y lo cargaremos usando **Pandas** para inspeccionarlo antes de insertarlo en las bases de datos.\n",
    "\n",
    "Pasos:\n",
    "1. Leer el archivo CSV.\n",
    "2. Mostrar las primeras filas para validar el contenido.\n",
    "3. Revisar la estructura de las columnas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300570e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_code</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-24 11:50:39 UTC</td>\n",
       "      <td>2294359932054536986</td>\n",
       "      <td>1515966223509089906</td>\n",
       "      <td>2.268105e+18</td>\n",
       "      <td>electronics.tablet</td>\n",
       "      <td>samsung</td>\n",
       "      <td>162.01</td>\n",
       "      <td>1.515916e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-24 11:50:39 UTC</td>\n",
       "      <td>2294359932054536986</td>\n",
       "      <td>1515966223509089906</td>\n",
       "      <td>2.268105e+18</td>\n",
       "      <td>electronics.tablet</td>\n",
       "      <td>samsung</td>\n",
       "      <td>162.01</td>\n",
       "      <td>1.515916e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-24 14:37:43 UTC</td>\n",
       "      <td>2294444024058086220</td>\n",
       "      <td>2273948319057183658</td>\n",
       "      <td>2.268105e+18</td>\n",
       "      <td>electronics.audio.headphone</td>\n",
       "      <td>huawei</td>\n",
       "      <td>77.52</td>\n",
       "      <td>1.515916e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-24 14:37:43 UTC</td>\n",
       "      <td>2294444024058086220</td>\n",
       "      <td>2273948319057183658</td>\n",
       "      <td>2.268105e+18</td>\n",
       "      <td>electronics.audio.headphone</td>\n",
       "      <td>huawei</td>\n",
       "      <td>77.52</td>\n",
       "      <td>1.515916e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-24 19:16:21 UTC</td>\n",
       "      <td>2294584263154074236</td>\n",
       "      <td>2273948316817424439</td>\n",
       "      <td>2.268105e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>karcher</td>\n",
       "      <td>217.57</td>\n",
       "      <td>1.515916e+18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                event_time             order_id           product_id  \\\n",
       "0  2020-04-24 11:50:39 UTC  2294359932054536986  1515966223509089906   \n",
       "1  2020-04-24 11:50:39 UTC  2294359932054536986  1515966223509089906   \n",
       "2  2020-04-24 14:37:43 UTC  2294444024058086220  2273948319057183658   \n",
       "3  2020-04-24 14:37:43 UTC  2294444024058086220  2273948319057183658   \n",
       "4  2020-04-24 19:16:21 UTC  2294584263154074236  2273948316817424439   \n",
       "\n",
       "    category_id                category_code    brand   price       user_id  \n",
       "0  2.268105e+18           electronics.tablet  samsung  162.01  1.515916e+18  \n",
       "1  2.268105e+18           electronics.tablet  samsung  162.01  1.515916e+18  \n",
       "2  2.268105e+18  electronics.audio.headphone   huawei   77.52  1.515916e+18  \n",
       "3  2.268105e+18  electronics.audio.headphone   huawei   77.52  1.515916e+18  \n",
       "4  2.268105e+18                          NaN  karcher  217.57  1.515916e+18  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo descargado\n",
    "file_path = \"./datasets/ecommerce/kz.csv\"\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Vista previa de las primeras filas\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9eebab",
   "metadata": {},
   "source": [
    "## üìä Inspecci√≥n de la estructura del Dataset\n",
    "\n",
    "Antes de proceder a insertar los datos en las bases de datos, es importante entender su estructura y contenido.\n",
    "\n",
    "Pasos:\n",
    "1. Revisar la informaci√≥n general (`info()`) para conocer n√∫mero de filas, columnas y tipos de datos.\n",
    "2. Verificar estad√≠sticas descriptivas (`describe()`) para entender rangos y distribuci√≥n de datos num√©ricos.\n",
    "3. Detectar posibles valores nulos o faltantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c49529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Informaci√≥n General del Dataset ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2633521 entries, 0 to 2633520\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   event_time     object \n",
      " 1   order_id       int64  \n",
      " 2   product_id     int64  \n",
      " 3   category_id    float64\n",
      " 4   category_code  object \n",
      " 5   brand          object \n",
      " 6   price          float64\n",
      " 7   user_id        float64\n",
      "dtypes: float64(3), int64(2), object(3)\n",
      "memory usage: 160.7+ MB\n",
      "None\n",
      "\n",
      "=== Estad√≠sticas Descriptivas ===\n",
      "           order_id    product_id   category_id         price       user_id\n",
      "count  2.633521e+06  2.633521e+06  2.201567e+06  2.201567e+06  5.641690e+05\n",
      "mean   2.361783e+18  1.674080e+18  2.273827e+18  1.540932e+02  1.515916e+18\n",
      "std    1.716538e+16  3.102249e+17  2.353247e+16  2.419421e+02  2.377083e+07\n",
      "min    2.294360e+18  1.515966e+18  2.268105e+18  0.000000e+00  1.515916e+18\n",
      "25%    2.348807e+18  1.515966e+18  2.268105e+18  1.456000e+01  1.515916e+18\n",
      "50%    2.353254e+18  1.515966e+18  2.268105e+18  5.553000e+01  1.515916e+18\n",
      "75%    2.383131e+18  1.515966e+18  2.268105e+18  1.967400e+02  1.515916e+18\n",
      "max    2.388441e+18  2.388434e+18  2.374499e+18  5.092590e+04  1.515916e+18\n",
      "\n",
      "=== Valores Nulos por Columna ===\n",
      "event_time             0\n",
      "order_id               0\n",
      "product_id             0\n",
      "category_id       431954\n",
      "category_code     612202\n",
      "brand             506005\n",
      "price             431954\n",
      "user_id          2069352\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Informaci√≥n general del dataset\n",
    "print(\"=== Informaci√≥n General del Dataset ===\")\n",
    "print(df.info())\n",
    "\n",
    "# Estad√≠sticas descriptivas de columnas num√©ricas\n",
    "print(\"\\n=== Estad√≠sticas Descriptivas ===\")\n",
    "print(df.describe())\n",
    "\n",
    "# Conteo de valores nulos por columna\n",
    "print(\"\\n=== Valores Nulos por Columna ===\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a94187",
   "metadata": {},
   "source": [
    "## Limpieza y Preprocesamiento del Dataset\n",
    "\n",
    "Antes de insertar en las bases de datos, limpiamos y normalizamos la informaci√≥n para:\n",
    "- Manejar valores nulos.\n",
    "- Convertir `event_time` a formato `datetime` UTC.\n",
    "- Garantizar compatibilidad con los formatos de cada base de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68313f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Procesando chunk 1...\n",
      "‚úÖ Chunk 1 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 2...\n",
      "‚úÖ Chunk 2 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 3...\n",
      "‚úÖ Chunk 3 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 4...\n",
      "‚úÖ Chunk 4 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 5...\n",
      "‚úÖ Chunk 5 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 6...\n",
      "‚úÖ Chunk 6 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 7...\n",
      "‚úÖ Chunk 7 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 8...\n",
      "‚úÖ Chunk 8 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 9...\n",
      "‚úÖ Chunk 9 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 10...\n",
      "‚úÖ Chunk 10 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 11...\n",
      "‚úÖ Chunk 11 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 12...\n",
      "‚úÖ Chunk 12 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 13...\n",
      "‚úÖ Chunk 13 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 14...\n",
      "‚úÖ Chunk 14 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 15...\n",
      "‚úÖ Chunk 15 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 16...\n",
      "‚úÖ Chunk 16 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 17...\n",
      "‚úÖ Chunk 17 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 18...\n",
      "‚úÖ Chunk 18 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 19...\n",
      "‚úÖ Chunk 19 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 20...\n",
      "‚úÖ Chunk 20 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 21...\n",
      "‚úÖ Chunk 21 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 22...\n",
      "‚úÖ Chunk 22 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 23...\n",
      "‚úÖ Chunk 23 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 24...\n",
      "‚úÖ Chunk 24 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 25...\n",
      "‚úÖ Chunk 25 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 26...\n",
      "‚úÖ Chunk 26 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 27...\n",
      "‚úÖ Chunk 27 limpio y listo (33521 filas)\n",
      "‚è± Tiempo total de limpieza: 73.03 segundos\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "CSV_PATH = \"./datasets/ecommerce/kz.csv\"\n",
    "CHUNK_SIZE = 100_000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(CSV_PATH, chunksize=CHUNK_SIZE)):\n",
    "    print(f\"üîÑ Procesando chunk {i+1}...\")\n",
    "\n",
    "    # Conversi√≥n de event_time a datetime UTC\n",
    "    chunk[\"event_time\"] = pd.to_datetime(chunk[\"event_time\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    # Manejo de valores nulos\n",
    "    chunk[\"category_code\"] = chunk[\"category_code\"].fillna(\"\")\n",
    "    chunk[\"brand\"] = chunk[\"brand\"].fillna(\"\")\n",
    "    chunk[\"category_id\"] = chunk[\"category_id\"].fillna(0).astype(\"int64\")\n",
    "    chunk[\"price\"] = chunk[\"price\"].fillna(0).astype(\"float64\")\n",
    "    chunk[\"user_id\"] = chunk[\"user_id\"].fillna(0).astype(\"int64\")\n",
    "\n",
    "    # Aqu√≠ podr√≠amos insertar directamente en MongoDB, Redis o HBase\n",
    "    # insert_to_mongo(chunk)  # Ejemplo\n",
    "    print(f\"‚úÖ Chunk {i+1} limpio y listo ({len(chunk)} filas)\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"‚è± Tiempo total de limpieza: {end_time - start_time:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22144696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Valores Nulos por Columna (Dataset Limpio) ===\n",
      "event_time             0\n",
      "order_id               0\n",
      "product_id             0\n",
      "category_id       431954\n",
      "category_code     612202\n",
      "brand             506005\n",
      "price             431954\n",
      "user_id          2069352\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verificaci√≥n de valores nulos en el dataset limpio\n",
    "print(\"=== Valores Nulos por Columna (Dataset Limpio) ===\")\n",
    "null_counts = df.isnull().sum()\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb590f",
   "metadata": {},
   "source": [
    "## üì• Inserci√≥n de datos en MongoDB\n",
    "\n",
    "Conexi√≥n y carga de datos en MongoDB usando `pymongo`.\n",
    "\n",
    "Pasos:\n",
    "1. Conectarse al servidor de MongoDB.\n",
    "2. Seleccionar la base de datos (`ecommerce_db`) y la colecci√≥n (`purchases`).\n",
    "3. Convertir el DataFrame de Pandas a diccionarios (`to_dict('records')`).\n",
    "4. Insertar los datos con `insert_many()`.\n",
    "\n",
    "> ‚ö†Ô∏è **Nota:** El contenedor MongoDB debe estar corriendo y accesible en el puerto `27017`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da27c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Colecci√≥n limpiada antes de la inserci√≥n.\n",
      "\n",
      "‚úÖ Bloque 1: 100000 registros (4.23 seg)\n",
      "‚úÖ Bloque 2: 100000 registros (4.72 seg)\n",
      "‚úÖ Bloque 3: 100000 registros (4.35 seg)\n",
      "‚úÖ Bloque 4: 100000 registros (4.28 seg)\n",
      "‚úÖ Bloque 5: 100000 registros (4.59 seg)\n",
      "‚úÖ Bloque 6: 100000 registros (4.16 seg)\n",
      "‚úÖ Bloque 7: 100000 registros (4.12 seg)\n",
      "‚úÖ Bloque 8: 100000 registros (4.49 seg)\n",
      "‚úÖ Bloque 9: 100000 registros (4.16 seg)\n",
      "‚úÖ Bloque 10: 100000 registros (4.07 seg)\n",
      "‚úÖ Bloque 11: 100000 registros (4.50 seg)\n",
      "‚úÖ Bloque 12: 100000 registros (4.19 seg)\n",
      "‚úÖ Bloque 13: 100000 registros (4.13 seg)\n",
      "‚úÖ Bloque 14: 100000 registros (4.59 seg)\n",
      "‚úÖ Bloque 15: 100000 registros (4.14 seg)\n",
      "‚úÖ Bloque 16: 100000 registros (4.37 seg)\n",
      "‚úÖ Bloque 17: 100000 registros (4.21 seg)\n",
      "‚úÖ Bloque 18: 100000 registros (4.31 seg)\n",
      "‚úÖ Bloque 19: 100000 registros (4.29 seg)\n",
      "‚úÖ Bloque 20: 100000 registros (4.25 seg)\n",
      "‚úÖ Bloque 21: 100000 registros (4.25 seg)\n",
      "‚úÖ Bloque 22: 100000 registros (4.62 seg)\n",
      "‚úÖ Bloque 23: 100000 registros (4.24 seg)\n",
      "‚úÖ Bloque 24: 100000 registros (4.23 seg)\n",
      "‚úÖ Bloque 25: 100000 registros (4.64 seg)\n",
      "‚úÖ Bloque 26: 100000 registros (4.31 seg)\n",
      "‚úÖ Bloque 27: 33521 registros (1.54 seg)\n",
      "\n",
      "‚è± Tiempo total: 113.98 seg\n",
      "üìä Total documentos insertados: 2633521\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# ==== Par√°metros de conexi√≥n ====\n",
    "MONGO_USER = \"admin\"\n",
    "MONGO_PASS = \"pass\"\n",
    "MONGO_HOST = \"localhost\"   # Si Jupyter y MongoDB est√°n en la misma VM\n",
    "MONGO_PORT = 27017\n",
    "DB_NAME = \"ecommerce_db\"\n",
    "COLLECTION_NAME = \"purchases\"\n",
    "\n",
    "# ==== Conexi√≥n a MongoDB ====\n",
    "client = MongoClient(f\"mongodb://{MONGO_USER}:{MONGO_PASS}@{MONGO_HOST}:{MONGO_PORT}/?authSource=admin\")\n",
    "db = client[DB_NAME]\n",
    "collection = db[COLLECTION_NAME]\n",
    "\n",
    "# ==== Limpieza de la colecci√≥n ====\n",
    "collection.drop()\n",
    "print(\"üßπ Colecci√≥n limpiada antes de la inserci√≥n.\\n\")\n",
    "\n",
    "# ==== Par√°metros de carga ====\n",
    "CHUNK_SIZE = 100_000  # N√∫mero de registros por bloque\n",
    "total_rows = len(df)  # df debe estar previamente cargado con el dataset\n",
    "total_start_time = time.time()\n",
    "\n",
    "# ==== Inserci√≥n por bloques ====\n",
    "for i in range(0, total_rows, CHUNK_SIZE):\n",
    "    chunk_start_time = time.time()\n",
    "\n",
    "    # Seleccionar un bloque de datos\n",
    "    df_chunk = df.iloc[i:i + CHUNK_SIZE]\n",
    "    data_dict = df_chunk.to_dict(\"records\")\n",
    "\n",
    "    # Insertar en MongoDB\n",
    "    result = collection.insert_many(data_dict)\n",
    "\n",
    "    chunk_end_time = time.time()\n",
    "    print(f\"‚úÖ Bloque {i // CHUNK_SIZE + 1}: {len(result.inserted_ids)} registros \"\n",
    "          f\"({chunk_end_time - chunk_start_time:.2f} seg)\")\n",
    "\n",
    "# ==== Tiempo total ====\n",
    "total_end_time = time.time()\n",
    "print(f\"\\n‚è± Tiempo total: {total_end_time - total_start_time:.2f} seg\")\n",
    "print(f\"üìä Total documentos insertados: {collection.count_documents({})}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d1dbe",
   "metadata": {},
   "source": [
    "## Carga del dataset en Redis\n",
    "\n",
    "En esta secci√≥n conectaremos con Redis desde Python utilizando la librer√≠a `redis-py`.\n",
    "Cargaremos los datos del dataset en formato **Hash** (`HSET`), de forma que cada\n",
    "registro tendr√° una clave √∫nica y todos sus campos estar√°n agrupados en una misma estructura.\n",
    "\n",
    "### Estrategia:\n",
    "- Usaremos la conexi√≥n al contenedor Redis que corre en la misma VM (`localhost`).\n",
    "- Guardaremos los datos como:\n",
    "  - **Clave:** `purchase:<id>`\n",
    "  - **Campos:** `category`, `brand`, `price`, `purchase_date`, etc.\n",
    "- Cargaremos en bloques de **100‚ÄØ000 registros** para no saturar la memoria.\n",
    "- Antes de insertar, limpiaremos el espacio de claves relacionadas con `purchase:` para evitar duplicados.\n",
    "- Mediremos el tiempo por bloque y el tiempo total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e68a938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conectado a Redis\n",
      "üì¶ Total de registros en dataset: 2,633,521\n",
      "üßπ Eliminadas 2,633,521 claves antiguas en Redis\n",
      "‚úÖ Bloque 1: 100,000 registros en 42.05 seg\n",
      "‚úÖ Bloque 2: 100,000 registros en 40.58 seg\n",
      "‚úÖ Bloque 3: 100,000 registros en 40.79 seg\n",
      "‚úÖ Bloque 4: 100,000 registros en 40.61 seg\n",
      "‚úÖ Bloque 5: 100,000 registros en 40.09 seg\n",
      "‚úÖ Bloque 6: 100,000 registros en 41.15 seg\n",
      "‚úÖ Bloque 7: 100,000 registros en 41.36 seg\n",
      "‚úÖ Bloque 8: 100,000 registros en 41.28 seg\n",
      "‚úÖ Bloque 9: 100,000 registros en 41.36 seg\n",
      "‚úÖ Bloque 10: 100,000 registros en 41.18 seg\n",
      "‚úÖ Bloque 11: 100,000 registros en 41.31 seg\n",
      "‚úÖ Bloque 12: 100,000 registros en 41.40 seg\n",
      "‚úÖ Bloque 13: 100,000 registros en 41.06 seg\n",
      "‚úÖ Bloque 14: 100,000 registros en 41.12 seg\n",
      "‚úÖ Bloque 15: 100,000 registros en 40.68 seg\n",
      "‚úÖ Bloque 16: 100,000 registros en 41.31 seg\n",
      "‚úÖ Bloque 17: 100,000 registros en 40.84 seg\n",
      "‚úÖ Bloque 18: 100,000 registros en 41.65 seg\n",
      "‚úÖ Bloque 19: 100,000 registros en 41.26 seg\n",
      "‚úÖ Bloque 20: 100,000 registros en 41.48 seg\n",
      "‚úÖ Bloque 21: 100,000 registros en 41.55 seg\n",
      "‚úÖ Bloque 22: 100,000 registros en 42.04 seg\n",
      "‚úÖ Bloque 23: 100,000 registros en 41.14 seg\n",
      "‚úÖ Bloque 24: 100,000 registros en 41.60 seg\n",
      "‚úÖ Bloque 25: 100,000 registros en 41.37 seg\n",
      "‚úÖ Bloque 26: 100,000 registros en 42.68 seg\n",
      "‚úÖ Bloque 27: 33,521 registros en 13.66 seg\n",
      "üèÅ Inserci√≥n total completada en 1086.61 segundos\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# ==== Par√°metros de conexi√≥n a Redis ====\n",
    "redis_host = \"localhost\"\n",
    "redis_port = 6379\n",
    "redis_pass = None  # Cambiar si se configur√≥ contrase√±a\n",
    "\n",
    "# ==== Conexi√≥n ====\n",
    "r = redis.Redis(host=redis_host, port=redis_port, password=redis_pass, decode_responses=True)\n",
    "\n",
    "# ==== Verificaci√≥n de conexi√≥n ====\n",
    "try:\n",
    "    r.ping()\n",
    "    print(\"‚úÖ Conectado a Redis\")\n",
    "except redis.exceptions.ConnectionError as e:\n",
    "    raise SystemExit(f\"‚ùå No se pudo conectar a Redis: {e}\")\n",
    "\n",
    "# ==== Ruta del dataset ====\n",
    "dataset_path = \"./datasets/ecommerce/kz.csv\"\n",
    "\n",
    "# ==== Cargar en DataFrame ====\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# ==== Par√°metros de carga ====\n",
    "CHUNK_SIZE = 100_000\n",
    "total_rows = len(df)\n",
    "print(f\"üì¶ Total de registros en dataset: {total_rows:,}\")\n",
    "\n",
    "# ==== Limpieza previa de datos ====\n",
    "keys_to_delete = r.keys(\"purchase:*\")\n",
    "if keys_to_delete:\n",
    "    for i in range(0, len(keys_to_delete), 10_000):  # evitar saturar delete\n",
    "        r.delete(*keys_to_delete[i:i+10_000])\n",
    "    print(f\"üßπ Eliminadas {len(keys_to_delete):,} claves antiguas en Redis\")\n",
    "\n",
    "# ==== Inserci√≥n por bloques ====\n",
    "start_total = time.time()\n",
    "for i in range(0, total_rows, CHUNK_SIZE):\n",
    "    chunk = df.iloc[i:i + CHUNK_SIZE]\n",
    "    start_chunk = time.time()\n",
    "\n",
    "    pipe = r.pipeline(transaction=False)\n",
    "    for idx, row in chunk.iterrows():\n",
    "        key = f\"purchase:{idx}\"\n",
    "        pipe.hset(key, mapping={k: (\"\" if pd.isna(v) else str(v)) for k, v in row.to_dict().items()})\n",
    "\n",
    "    pipe.execute()\n",
    "    elapsed_chunk = time.time() - start_chunk\n",
    "    print(f\"‚úÖ Bloque {i//CHUNK_SIZE + 1}: {len(chunk):,} registros en {elapsed_chunk:.2f} seg\")\n",
    "\n",
    "elapsed_total = time.time() - start_total\n",
    "print(f\"üèÅ Inserci√≥n total completada en {elapsed_total:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b42b1c",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Carga del Dataset en HBase\n",
    "\n",
    "En esta secci√≥n insertaremos el dataset limpio en **HBase**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Configuraci√≥n de la prueba\n",
    "- **Dataset:** `kz.csv` (2‚ÄØ633‚ÄØ521 registros)\n",
    "- **Bloques de inserci√≥n:** 100‚ÄØ000 registros (√∫ltimo bloque menor)\n",
    "- **HBase:** Contenedor Docker (`hbase:latest`) ejecutando en la misma VM\n",
    "- **Conexi√≥n:** Usaremos la librer√≠a `happybase` v√≠a `thriftpy2` para comunicaci√≥n con el servidor Thrift de HBase.\n",
    "- **VM:** `Standard_A4m_v2` ‚Äî 8 vCPU, 32‚ÄØGB RAM\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Consideraciones\n",
    "1. Antes de cargar datos, nos aseguraremos de que **la tabla no exista** o **se vac√≠e** si ya existe, para evitar duplicados.\n",
    "2. Cada registro se insertar√° como **una fila** en HBase, usando `order_id` como **row key**.\n",
    "3. Usaremos `batch()` para mejorar el rendimiento y reducir el overhead de conexi√≥n.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d317f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conectado a HBase\n",
      "üìÑ La tabla 'purchases' ya existe.\n",
      "üßπ Limpiando registros antiguos de la tabla...\n",
      "üßπ Tabla vac√≠a.\n",
      "\n",
      "üì¶ Total de registros en dataset: 2,633,521\n",
      "‚úÖ Bloque 1: 100,000 registros en 49.28 segundos\n",
      "‚úÖ Bloque 2: 100,000 registros en 47.64 segundos\n",
      "‚úÖ Bloque 3: 100,000 registros en 47.18 segundos\n",
      "‚úÖ Bloque 4: 100,000 registros en 47.28 segundos\n",
      "‚úÖ Bloque 5: 100,000 registros en 47.85 segundos\n",
      "‚úÖ Bloque 6: 100,000 registros en 47.16 segundos\n",
      "‚úÖ Bloque 7: 100,000 registros en 47.60 segundos\n",
      "‚úÖ Bloque 8: 100,000 registros en 47.45 segundos\n",
      "‚úÖ Bloque 9: 100,000 registros en 48.55 segundos\n",
      "‚úÖ Bloque 10: 100,000 registros en 49.36 segundos\n",
      "‚úÖ Bloque 11: 100,000 registros en 48.99 segundos\n",
      "‚úÖ Bloque 12: 100,000 registros en 47.98 segundos\n",
      "‚úÖ Bloque 13: 100,000 registros en 48.40 segundos\n",
      "‚úÖ Bloque 14: 100,000 registros en 46.95 segundos\n",
      "‚úÖ Bloque 15: 100,000 registros en 46.82 segundos\n",
      "‚úÖ Bloque 16: 100,000 registros en 47.52 segundos\n",
      "‚úÖ Bloque 17: 100,000 registros en 47.99 segundos\n",
      "‚úÖ Bloque 18: 100,000 registros en 48.58 segundos\n",
      "‚úÖ Bloque 19: 100,000 registros en 48.02 segundos\n",
      "‚úÖ Bloque 20: 100,000 registros en 47.55 segundos\n",
      "‚úÖ Bloque 21: 100,000 registros en 46.78 segundos\n",
      "‚úÖ Bloque 22: 100,000 registros en 47.08 segundos\n",
      "‚úÖ Bloque 23: 100,000 registros en 47.56 segundos\n",
      "‚úÖ Bloque 24: 100,000 registros en 46.85 segundos\n",
      "‚úÖ Bloque 25: 100,000 registros en 47.60 segundos\n",
      "‚úÖ Bloque 26: 100,000 registros en 48.93 segundos\n",
      "‚úÖ Bloque 27: 33,521 registros en 16.09 segundos\n",
      "\n",
      "üèÅ Inserci√≥n total completada en 1259.19 segundos\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import happybase\n",
    "import time\n",
    "\n",
    "# ==== Par√°metros de conexi√≥n ====\n",
    "HBASE_HOST = \"localhost\"\n",
    "HBASE_PORT = 9090   # Puerto del servicio Thrift de HBase\n",
    "TABLE_NAME = \"purchases\"\n",
    "\n",
    "# ==== Conectar a HBase ====\n",
    "connection = happybase.Connection(host=HBASE_HOST, port=HBASE_PORT)\n",
    "connection.open()\n",
    "\n",
    "print(\"‚úÖ Conectado a HBase\")\n",
    "\n",
    "# ==== Crear tabla si no existe ====\n",
    "families = {'cf': dict()}  # 'cf' = column family\n",
    "if TABLE_NAME.encode() not in connection.tables():\n",
    "    connection.create_table(TABLE_NAME, families)\n",
    "    print(f\"üÜï Tabla creada: {TABLE_NAME}\")\n",
    "else:\n",
    "    print(f\"üìÑ La tabla '{TABLE_NAME}' ya existe.\")\n",
    "\n",
    "table = connection.table(TABLE_NAME)\n",
    "\n",
    "# ==== Limpieza previa ====\n",
    "print(\"üßπ Limpiando registros antiguos de la tabla...\")\n",
    "# Nota: happybase no tiene truncate directo, as√≠ que borramos fila a fila\n",
    "for key, _ in table.scan():\n",
    "    table.delete(key)\n",
    "print(\"üßπ Tabla vac√≠a.\\n\")\n",
    "\n",
    "# ==== Cargar dataset ====\n",
    "dataset_path = \"./datasets/ecommerce/kz.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "CHUNK_SIZE = 100_000\n",
    "total_rows = len(df)\n",
    "print(f\"üì¶ Total de registros en dataset: {total_rows:,}\")\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for i in range(0, total_rows, CHUNK_SIZE):\n",
    "    chunk = df.iloc[i:i + CHUNK_SIZE]\n",
    "    start_chunk = time.time()\n",
    "\n",
    "    # Usar batch para inserci√≥n r√°pida\n",
    "    with table.batch(batch_size=CHUNK_SIZE) as b:\n",
    "        for _, row in chunk.iterrows():\n",
    "            row_key = str(row[\"order_id\"]).encode()\n",
    "            b.put(row_key, {\n",
    "                b\"cf:event_time\": str(row[\"event_time\"]).encode(),\n",
    "                b\"cf:product_id\": str(row[\"product_id\"]).encode(),\n",
    "                b\"cf:category_id\": str(row[\"category_id\"]).encode(),\n",
    "                b\"cf:category_code\": str(row[\"category_code\"]).encode() if pd.notna(row[\"category_code\"]) else b\"\",\n",
    "                b\"cf:brand\": str(row[\"brand\"]).encode() if pd.notna(row[\"brand\"]) else b\"\",\n",
    "                b\"cf:price\": str(row[\"price\"]).encode(),\n",
    "                b\"cf:user_id\": str(row[\"user_id\"]).encode() if pd.notna(row[\"user_id\"]) else b\"\"\n",
    "            })\n",
    "\n",
    "    elapsed_chunk = time.time() - start_chunk\n",
    "    print(f\"‚úÖ Bloque {i//CHUNK_SIZE + 1}: {len(chunk):,} registros en {elapsed_chunk:.2f} segundos\")\n",
    "\n",
    "total_elapsed = time.time() - total_start_time\n",
    "print(f\"\\nüèÅ Inserci√≥n total completada en {total_elapsed:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb0364",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Consulta: Categor√≠a m√°s vendida\n",
    "\n",
    "**Objetivo:** Determinar cu√°l categor√≠a (`category_code`) aparece con mayor frecuencia en las transacciones.\n",
    "\n",
    "Se ejecutar√° la misma consulta en:\n",
    "- **MongoDB**\n",
    "- **Redis**\n",
    "- **HBase**\n",
    "\n",
    "Adem√°s, se medir√° el tiempo de ejecuci√≥n en cada base de datos para su posterior comparaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "423485e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Categor√≠a m√°s vendida (MongoDB): nan con 612,202 ventas\n",
      "‚è± Tiempo: 8.5758 segundos\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Conexi√≥n MongoDB\n",
    "mongo_client = MongoClient(f\"mongodb://{MONGO_USER}:{MONGO_PASS}@{MONGO_HOST}:{MONGO_PORT}/?authSource=admin\")\n",
    "mongo_db = mongo_client[DB_NAME]\n",
    "mongo_col = mongo_db[COLLECTION_NAME]\n",
    "\n",
    "start_time = time.time()\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$category_code\", \"total\": {\"$sum\": 1}}},\n",
    "    {\"$sort\": {\"total\": -1}},\n",
    "    {\"$limit\": 1}\n",
    "]\n",
    "result = list(mongo_col.aggregate(pipeline))\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üìä Categor√≠a m√°s vendida (MongoDB): {result[0]['_id']} con {result[0]['total']:,} ventas\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a31e1ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Categor√≠a m√°s vendida (Redis): electronics.smartphone con 357,682 ventas\n",
      "‚è± Tiempo: 1238.2801 segundos\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Conexi√≥n Redis\n",
    "r = redis.Redis(host=redis_host, port=redis_port, password=redis_pass)\n",
    "\n",
    "start_time = time.time()\n",
    "category_counter = Counter()\n",
    "\n",
    "for key in r.scan_iter(\"purchase:*\"):\n",
    "    category_code = r.hget(key, \"category_code\")\n",
    "    if category_code:\n",
    "        category_counter[category_code.decode()] += 1\n",
    "\n",
    "top_category, top_count = category_counter.most_common(1)[0]\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üìä Categor√≠a m√°s vendida (Redis): {top_category} con {top_count:,} ventas\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c71bf86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Categor√≠a m√°s vendida (HBase): electronics.smartphone con 213,002 ventas\n",
      "‚è± Tiempo: 169.7527 segundos\n"
     ]
    }
   ],
   "source": [
    "import happybase\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Conexi√≥n HBase\n",
    "connection = happybase.Connection(host=\"localhost\", port=9090)\n",
    "table = connection.table(\"purchases\")\n",
    "\n",
    "start_time = time.time()\n",
    "category_counter = Counter()\n",
    "\n",
    "for key, data in table.scan():\n",
    "    category_code = data.get(b\"cf:category_code\")\n",
    "    if category_code:\n",
    "        category_counter[category_code.decode()] += 1\n",
    "\n",
    "top_category, top_count = category_counter.most_common(1)[0]\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üìä Categor√≠a m√°s vendida (HBase): {top_category} con {top_count:,} ventas\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd2446",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Consulta: Marca que gener√≥ m√°s ingresos brutos\n",
    "\n",
    "**Objetivo:** Determinar cu√°l `brand` gener√≥ la mayor suma de `price`.\n",
    "\n",
    "Se ejecutar√° en:\n",
    "- **MongoDB**\n",
    "- **Redis**\n",
    "- **HBase**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e24bb6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ Marca con m√°s ingresos (MongoDB): samsung con $90052821.66\n",
      "‚è± Tiempo: 9.7873 segundos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$brand\", \"total_ingresos\": {\"$sum\": \"$price\"}}},\n",
    "    {\"$sort\": {\"total_ingresos\": -1}},\n",
    "    {\"$limit\": 1}\n",
    "]\n",
    "result = list(mongo_col.aggregate(pipeline))\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üí∞ Marca con m√°s ingresos (MongoDB): {result[0]['_id']} con ${result[0]['total_ingresos']:.2f}\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07df4061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ Marca con m√°s ingresos (Redis): samsung con $90,052,821.66\n",
      "‚è± Tiempo: 2306.0426 segundos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "brand_revenue = Counter()\n",
    "\n",
    "for key in r.scan_iter(\"purchase:*\"):\n",
    "    brand = r.hget(key, \"brand\")\n",
    "    price = r.hget(key, \"price\")\n",
    "    if brand and price:\n",
    "        try:\n",
    "            brand_revenue[brand.decode()] += float(price)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "top_brand, top_revenue = brand_revenue.most_common(1)[0]\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üí∞ Marca con m√°s ingresos (Redis): {top_brand} con ${top_revenue:,.2f}\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d81fb986",
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     scan_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow_start\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m last_key\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Leer un bloque del escaneo\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscan_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rows:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/happybase/table.py:401\u001b[0m, in \u001b[0;36mTable.scan\u001b[0;34m(self, row_start, row_stop, row_prefix, columns, filter, timestamp, include_timestamp, batch_size, scan_batching, limit, sorted_columns, reverse)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# XXX: The \"batch_size\" can be slightly confusing to those\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;66;03m# familiar with the HBase Java API:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# cuts rows into multiple partial rows, can be set using the\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# slightly strange name scan_batching.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     scan \u001b[38;5;241m=\u001b[39m TScan(\n\u001b[1;32m    391\u001b[0m         startRow\u001b[38;5;241m=\u001b[39mrow_start,\n\u001b[1;32m    392\u001b[0m         stopRow\u001b[38;5;241m=\u001b[39mrow_stop,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28mreversed\u001b[39m\u001b[38;5;241m=\u001b[39mreverse,\n\u001b[1;32m    400\u001b[0m     )\n\u001b[0;32m--> 401\u001b[0m     scan_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscannerOpenWithScan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpened scanner (id=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, scan_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    406\u001b[0m n_returned \u001b[38;5;241m=\u001b[39m n_fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thriftpy2/thrift.py:211\u001b[0m, in \u001b[0;36mTClient._req\u001b[0;34m(self, _api, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TApplicationException(\n\u001b[1;32m    205\u001b[0m         TApplicationException\u001b[38;5;241m.\u001b[39mUNKNOWN_METHOD,\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{arg}\u001b[39;00m\u001b[38;5;124m is required argument for \u001b[39m\u001b[38;5;132;01m{service}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{api}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    207\u001b[0m             arg\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m], service\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, api\u001b[38;5;241m=\u001b[39m_api))\n\u001b[1;32m    209\u001b[0m result_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service, _api \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_result\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# wait result only if non-oneway\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result_cls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneway\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thriftpy2/thrift.py:225\u001b[0m, in \u001b[0;36mTClient._send\u001b[0;34m(self, _api, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m args\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oprot)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oprot\u001b[38;5;241m.\u001b[39mwrite_message_end()\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oprot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thriftpy2/transport/buffered/cybuffered.pyx:49\u001b[0m, in \u001b[0;36mthriftpy2.transport.buffered.cybuffered.TCyBufferedTransport.flush\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thriftpy2/transport/buffered/cybuffered.pyx:79\u001b[0m, in \u001b[0;36mthriftpy2.transport.buffered.cybuffered.TCyBufferedTransport.c_flush\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thriftpy2/transport/buffered/cybuffered.pyx:86\u001b[0m, in \u001b[0;36mthriftpy2.transport.buffered.cybuffered.TCyBufferedTransport.c_dump_wbuf\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thriftpy2/transport/socket.py:136\u001b[0m, in \u001b[0;36mTSocket.write\u001b[0;34m(self, buff)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, buff):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuff\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "brand_revenue = Counter()\n",
    "start_time = time.time()\n",
    "\n",
    "# Configurar tama√±o de bloque para scan\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "# Control de filas procesadas\n",
    "rows_processed = 0\n",
    "last_key = None\n",
    "\n",
    "while True:\n",
    "    # Si es la primera iteraci√≥n, no pasamos start_row\n",
    "    scan_args = {\"columns\": [b\"cf:brand\", b\"cf:price\"], \"batch_size\": BATCH_SIZE}\n",
    "    if last_key:\n",
    "        scan_args[\"row_start\"] = last_key\n",
    "\n",
    "    # Leer un bloque del escaneo\n",
    "    rows = list(table.scan(**scan_args, limit=BATCH_SIZE + 1))\n",
    "\n",
    "    if not rows:\n",
    "        break\n",
    "\n",
    "    # Procesar el bloque\n",
    "    for key, data in rows:\n",
    "        brand = data.get(b\"cf:brand\")\n",
    "        price = data.get(b\"cf:price\")\n",
    "        if brand and price:\n",
    "            try:\n",
    "                brand_revenue[brand.decode()] += float(price.decode())\n",
    "            except:\n",
    "                pass\n",
    "        last_key = key\n",
    "        rows_processed += 1\n",
    "\n",
    "    # Si se ley√≥ menos que el bloque esperado, hemos terminado\n",
    "    if len(rows) <= BATCH_SIZE:\n",
    "        break\n",
    "\n",
    "    print(f\"üì¶ Procesadas {rows_processed:,} filas...\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "if brand_revenue:\n",
    "    top_brand, top_revenue = brand_revenue.most_common(1)[0]\n",
    "    print(f\"üí∞ Marca con m√°s ingresos (HBase): {top_brand} con ${top_revenue:,.2f}\")\n",
    "    print(f\"‚è± Tiempo total: {elapsed_time:.2f} segundos\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se encontraron datos en la tabla.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9456ca2",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Consulta: Mes con m√°s ventas (UTC)\n",
    "\n",
    "**Objetivo:** Determinar en qu√© mes (`event_time`) se realizaron m√°s ventas.\n",
    "\n",
    "Se ejecutar√° en:\n",
    "- **MongoDB**\n",
    "- **Redis**\n",
    "- **HBase**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14b9deb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Mes con m√°s ventas (MongoDB): 2020-06 con 403,632 ventas\n",
      "‚è± Tiempo: 9.6747 segundos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pipeline = [\n",
    "    {\"$project\": {\"mes\": {\"$substr\": [\"$event_time\", 0, 7]}}},\n",
    "    {\"$group\": {\"_id\": \"$mes\", \"total\": {\"$sum\": 1}}},\n",
    "    {\"$sort\": {\"total\": -1}},\n",
    "    {\"$limit\": 1}\n",
    "]\n",
    "result = list(mongo_col.aggregate(pipeline))\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üìÖ Mes con m√°s ventas (MongoDB): {result[0]['_id']} con {result[0]['total']:,} ventas\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d574b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Mes con m√°s ventas (Redis): 2020-06 con 403,632 ventas\n",
      "‚è± Tiempo: 1224.5353 segundos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "month_counter = Counter()\n",
    "\n",
    "for key in r.scan_iter(\"purchase:*\"):\n",
    "    event_time = r.hget(key, \"event_time\")\n",
    "    if event_time:\n",
    "        month = event_time.decode()[:7]\n",
    "        month_counter[month] += 1\n",
    "\n",
    "top_month, top_count = month_counter.most_common(1)[0]\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üìÖ Mes con m√°s ventas (Redis): {top_month} con {top_count:,} ventas\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5d7f789",
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      2\u001b[0m month_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, data \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mscan():\n\u001b[1;32m      5\u001b[0m     event_time \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcf:event_time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m event_time:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/happybase/table.py:401\u001b[0m, in \u001b[0;36mTable.scan\u001b[0;34m(self, row_start, row_stop, row_prefix, columns, filter, timestamp, include_timestamp, batch_size, scan_batching, limit, sorted_columns, reverse)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# XXX: The \"batch_size\" can be slightly confusing to those\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;66;03m# familiar with the HBase Java API:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# cuts rows into multiple partial rows, can be set using the\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# slightly strange name scan_batching.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     scan \u001b[38;5;241m=\u001b[39m TScan(\n\u001b[1;32m    391\u001b[0m         startRow\u001b[38;5;241m=\u001b[39mrow_start,\n\u001b[1;32m    392\u001b[0m         stopRow\u001b[38;5;241m=\u001b[39mrow_stop,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28mreversed\u001b[39m\u001b[38;5;241m=\u001b[39mreverse,\n\u001b[1;32m    400\u001b[0m     )\n\u001b[0;32m--> 401\u001b[0m     scan_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscannerOpenWithScan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpened scanner (id=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, scan_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    406\u001b[0m n_returned \u001b[38;5;241m=\u001b[39m n_fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thriftpy2/thrift.py:211\u001b[0m, in \u001b[0;36mTClient._req\u001b[0;34m(self, _api, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TApplicationException(\n\u001b[1;32m    205\u001b[0m         TApplicationException\u001b[38;5;241m.\u001b[39mUNKNOWN_METHOD,\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{arg}\u001b[39;00m\u001b[38;5;124m is required argument for \u001b[39m\u001b[38;5;132;01m{service}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{api}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    207\u001b[0m             arg\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m], service\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, api\u001b[38;5;241m=\u001b[39m_api))\n\u001b[1;32m    209\u001b[0m result_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service, _api \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_result\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# wait result only if non-oneway\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result_cls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneway\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thriftpy2/thrift.py:225\u001b[0m, in \u001b[0;36mTClient._send\u001b[0;34m(self, _api, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m args\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oprot)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oprot\u001b[38;5;241m.\u001b[39mwrite_message_end()\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oprot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thriftpy2/transport/buffered/cybuffered.pyx:49\u001b[0m, in \u001b[0;36mthriftpy2.transport.buffered.cybuffered.TCyBufferedTransport.flush\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thriftpy2/transport/buffered/cybuffered.pyx:79\u001b[0m, in \u001b[0;36mthriftpy2.transport.buffered.cybuffered.TCyBufferedTransport.c_flush\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thriftpy2/transport/buffered/cybuffered.pyx:86\u001b[0m, in \u001b[0;36mthriftpy2.transport.buffered.cybuffered.TCyBufferedTransport.c_dump_wbuf\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thriftpy2/transport/socket.py:136\u001b[0m, in \u001b[0;36mTSocket.write\u001b[0;34m(self, buff)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, buff):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuff\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "month_counter = Counter()\n",
    "\n",
    "for key, data in table.scan():\n",
    "    event_time = data.get(b\"cf:event_time\")\n",
    "    if event_time:\n",
    "        month = event_time.decode()[:7]\n",
    "        month_counter[month] += 1\n",
    "\n",
    "top_month, top_count = month_counter.most_common(1)[0]\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üìÖ Mes con m√°s ventas (HBase): {top_month} con {top_count:,} ventas\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
