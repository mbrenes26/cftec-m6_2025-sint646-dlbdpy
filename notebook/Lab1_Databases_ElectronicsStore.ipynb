{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42591e1",
   "metadata": {},
   "source": [
    "# Laboratorio 1 ‚Äì Comparaci√≥n de Bases de Datos NoSQL (MongoDB, Redis, HBase)\n",
    "\n",
    "**Objetivo:**\n",
    "Este laboratorio tiene como objetivo montar servidores de **MongoDB**, **Redis** y **HBase** en contenedores Docker, y luego interactuar con ellos desde Python para:\n",
    "\n",
    "1. Cargar un dataset real de compras electr√≥nicas.\n",
    "2. Ejecutar consultas espec√≠ficas en cada base de datos:\n",
    "   - ¬øCu√°l es la **categor√≠a** m√°s vendida?\n",
    "   - ¬øCu√°l **marca** gener√≥ m√°s ingresos brutos?\n",
    "   - ¬øQu√© **mes** tuvo m√°s ventas? (en UTC)\n",
    "3. Medir y comparar los tiempos de ejecuci√≥n entre las tres bases de datos.\n",
    "\n",
    "**Dataset utilizado:**\n",
    "[Ecommerce Purchase History from Electronics Store](https://www.kaggle.com/datasets/mkechinov/ecommerce-purchase-history-from-electronics-store)\n",
    "\n",
    "**Pasos principales:**\n",
    "1. Preparar el entorno de Python en esta m√°quina virtual.\n",
    "2. Descargar y preparar el dataset.\n",
    "3. Insertar datos en **MongoDB**, **Redis** y **HBase**.\n",
    "4. Ejecutar las consultas en cada base de datos.\n",
    "5. Analizar los tiempos de respuesta.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a390b808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.local/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: pymongo in ./.local/lib/python3.8/site-packages (4.10.1)\n",
      "Requirement already satisfied: redis in ./.local/lib/python3.8/site-packages (6.1.1)\n",
      "Requirement already satisfied: happybase in ./.local/lib/python3.8/site-packages (1.2.0)\n",
      "Requirement already satisfied: thriftpy2 in ./.local/lib/python3.8/site-packages (0.5.3)\n",
      "Requirement already satisfied: kaggle in ./.local/lib/python3.8/site-packages (1.7.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in ./.local/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.local/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in ./.local/lib/python3.8/site-packages (from pymongo) (2.6.1)\n",
      "Requirement already satisfied: async-timeout>=4.0.3; python_full_version < \"3.11.3\" in ./.local/lib/python3.8/site-packages (from redis) (5.0.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from happybase) (1.14.0)\n",
      "Requirement already satisfied: ply<4.0,>=3.4 in ./.local/lib/python3.8/site-packages (from thriftpy2) (3.11)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /usr/lib/python3/dist-packages (from kaggle) (1.25.8)\n",
      "Requirement already satisfied: bleach in ./.local/lib/python3.8/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from kaggle) (2.8)\n",
      "Requirement already satisfied: charset-normalizer in ./.local/lib/python3.8/site-packages (from kaggle) (3.4.2)\n",
      "Requirement already satisfied: protobuf in ./.local/lib/python3.8/site-packages (from kaggle) (5.29.5)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/lib/python3/dist-packages (from kaggle) (2019.11.28)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from kaggle) (2.22.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/lib/python3/dist-packages (from kaggle) (45.2.0)\n",
      "Requirement already satisfied: python-slugify in ./.local/lib/python3.8/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: text-unidecode in ./.local/lib/python3.8/site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: webencodings in ./.local/lib/python3.8/site-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (from kaggle) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# Instalaci√≥n de librer√≠as necesarias\n",
    "!pip3 install --user pandas pymongo redis happybase thriftpy2 kaggle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8aa4cf",
   "metadata": {},
   "source": [
    "## üì• Descarga del Dataset desde Kaggle\n",
    "\n",
    "Para trabajar con el dataset real, utilizaremos la API de Kaggle.  \n",
    "Esto requiere que tengamos un **token de acceso** que puedes obtener en tu cuenta de Kaggle:\n",
    "\n",
    "1. Inicia sesi√≥n en [Kaggle](https://www.kaggle.com/).\n",
    "2. Ve a **Account** ‚Üí **Create New API Token**.\n",
    "3. Esto descargar√° un archivo llamado `kaggle.json`.\n",
    "4. Lo subiremos a esta m√°quina en la carpeta `~/.kaggle/`.\n",
    "\n",
    "Este archivo contiene:\n",
    "- `username`: tu usuario en Kaggle.\n",
    "- `key`: clave de acceso para autenticaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d45ac5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Carpeta .kaggle lista. Sube tu archivo kaggle.json a esta ruta en la VM:\n",
      "/home/azureuser/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Crear carpeta .kaggle si no existe\n",
    "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Carpeta .kaggle lista. Sube tu archivo kaggle.json a esta ruta en la VM:\")\n",
    "\n",
    "# Mostrar ruta destino\n",
    "print(os.path.expanduser(\"~/.kaggle/kaggle.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a60aab",
   "metadata": {},
   "source": [
    "## üîì Ajustar permisos y descargar dataset\n",
    "\n",
    "Despu√©s de subir el archivo `kaggle.json` a la carpeta `.kaggle/` debemos ajustar los permisos para que sea seguro.  \n",
    "Luego utilizaremos la librer√≠a `kaggle` para descargar el dataset directamente en esta m√°quina.\n",
    "\n",
    "El dataset se guardar√° en la carpeta `datasets/ecommerce/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6b62cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/mkechinov/ecommerce-purchase-history-from-electronics-store\n",
      "License(s): copyright-authors\n",
      "Downloading ecommerce-purchase-history-from-electronics-store.zip to ./datasets/ecommerce\n",
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 36.0M/50.5M [00:00<00:00, 377MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50.5M/50.5M [00:00<00:00, 377MB/s]\n",
      "‚úÖ Dataset descargado y descomprimido en ./datasets/ecommerce\n"
     ]
    }
   ],
   "source": [
    "# Ajustar permisos del token Kaggle\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Descargar el dataset desde Kaggle\n",
    "!kaggle datasets download -d mkechinov/ecommerce-purchase-history-from-electronics-store -p ./datasets/ecommerce --unzip\n",
    "\n",
    "print(\"‚úÖ Dataset descargado y descomprimido en ./datasets/ecommerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a67195c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kz.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"./datasets/ecommerce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f1791",
   "metadata": {},
   "source": [
    "## üìÇ Carga y vista previa del Dataset\n",
    "\n",
    "El dataset descargado desde Kaggle contiene informaci√≥n sobre el historial de compras en una tienda de electr√≥nica.  \n",
    "El archivo descargado se llama `kz.csv` y lo cargaremos usando **Pandas** para inspeccionarlo antes de insertarlo en las bases de datos.\n",
    "\n",
    "Pasos:\n",
    "1. Leer el archivo CSV.\n",
    "2. Mostrar las primeras filas para validar el contenido.\n",
    "3. Revisar la estructura de las columnas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300570e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_code</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-24 11:50:39 UTC</td>\n",
       "      <td>2294359932054536986</td>\n",
       "      <td>1515966223509089906</td>\n",
       "      <td>2.268105e+18</td>\n",
       "      <td>electronics.tablet</td>\n",
       "      <td>samsung</td>\n",
       "      <td>162.01</td>\n",
       "      <td>1.515916e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-24 11:50:39 UTC</td>\n",
       "      <td>2294359932054536986</td>\n",
       "      <td>1515966223509089906</td>\n",
       "      <td>2.268105e+18</td>\n",
       "      <td>electronics.tablet</td>\n",
       "      <td>samsung</td>\n",
       "      <td>162.01</td>\n",
       "      <td>1.515916e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-24 14:37:43 UTC</td>\n",
       "      <td>2294444024058086220</td>\n",
       "      <td>2273948319057183658</td>\n",
       "      <td>2.268105e+18</td>\n",
       "      <td>electronics.audio.headphone</td>\n",
       "      <td>huawei</td>\n",
       "      <td>77.52</td>\n",
       "      <td>1.515916e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-24 14:37:43 UTC</td>\n",
       "      <td>2294444024058086220</td>\n",
       "      <td>2273948319057183658</td>\n",
       "      <td>2.268105e+18</td>\n",
       "      <td>electronics.audio.headphone</td>\n",
       "      <td>huawei</td>\n",
       "      <td>77.52</td>\n",
       "      <td>1.515916e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-24 19:16:21 UTC</td>\n",
       "      <td>2294584263154074236</td>\n",
       "      <td>2273948316817424439</td>\n",
       "      <td>2.268105e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>karcher</td>\n",
       "      <td>217.57</td>\n",
       "      <td>1.515916e+18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                event_time             order_id           product_id  \\\n",
       "0  2020-04-24 11:50:39 UTC  2294359932054536986  1515966223509089906   \n",
       "1  2020-04-24 11:50:39 UTC  2294359932054536986  1515966223509089906   \n",
       "2  2020-04-24 14:37:43 UTC  2294444024058086220  2273948319057183658   \n",
       "3  2020-04-24 14:37:43 UTC  2294444024058086220  2273948319057183658   \n",
       "4  2020-04-24 19:16:21 UTC  2294584263154074236  2273948316817424439   \n",
       "\n",
       "    category_id                category_code    brand   price       user_id  \n",
       "0  2.268105e+18           electronics.tablet  samsung  162.01  1.515916e+18  \n",
       "1  2.268105e+18           electronics.tablet  samsung  162.01  1.515916e+18  \n",
       "2  2.268105e+18  electronics.audio.headphone   huawei   77.52  1.515916e+18  \n",
       "3  2.268105e+18  electronics.audio.headphone   huawei   77.52  1.515916e+18  \n",
       "4  2.268105e+18                          NaN  karcher  217.57  1.515916e+18  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo descargado\n",
    "file_path = \"./datasets/ecommerce/kz.csv\"\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Vista previa de las primeras filas\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9eebab",
   "metadata": {},
   "source": [
    "## üìä Inspecci√≥n de la estructura del Dataset\n",
    "\n",
    "Antes de proceder a insertar los datos en las bases de datos, es importante entender su estructura y contenido.\n",
    "\n",
    "Pasos:\n",
    "1. Revisar la informaci√≥n general (`info()`) para conocer n√∫mero de filas, columnas y tipos de datos.\n",
    "2. Verificar estad√≠sticas descriptivas (`describe()`) para entender rangos y distribuci√≥n de datos num√©ricos.\n",
    "3. Detectar posibles valores nulos o faltantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c49529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Informaci√≥n General del Dataset ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2633521 entries, 0 to 2633520\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   event_time     object \n",
      " 1   order_id       int64  \n",
      " 2   product_id     int64  \n",
      " 3   category_id    float64\n",
      " 4   category_code  object \n",
      " 5   brand          object \n",
      " 6   price          float64\n",
      " 7   user_id        float64\n",
      "dtypes: float64(3), int64(2), object(3)\n",
      "memory usage: 160.7+ MB\n",
      "None\n",
      "\n",
      "=== Estad√≠sticas Descriptivas ===\n",
      "           order_id    product_id   category_id         price       user_id\n",
      "count  2.633521e+06  2.633521e+06  2.201567e+06  2.201567e+06  5.641690e+05\n",
      "mean   2.361783e+18  1.674080e+18  2.273827e+18  1.540932e+02  1.515916e+18\n",
      "std    1.716538e+16  3.102249e+17  2.353247e+16  2.419421e+02  2.377083e+07\n",
      "min    2.294360e+18  1.515966e+18  2.268105e+18  0.000000e+00  1.515916e+18\n",
      "25%    2.348807e+18  1.515966e+18  2.268105e+18  1.456000e+01  1.515916e+18\n",
      "50%    2.353254e+18  1.515966e+18  2.268105e+18  5.553000e+01  1.515916e+18\n",
      "75%    2.383131e+18  1.515966e+18  2.268105e+18  1.967400e+02  1.515916e+18\n",
      "max    2.388441e+18  2.388434e+18  2.374499e+18  5.092590e+04  1.515916e+18\n",
      "\n",
      "=== Valores Nulos por Columna ===\n",
      "event_time             0\n",
      "order_id               0\n",
      "product_id             0\n",
      "category_id       431954\n",
      "category_code     612202\n",
      "brand             506005\n",
      "price             431954\n",
      "user_id          2069352\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Informaci√≥n general del dataset\n",
    "print(\"=== Informaci√≥n General del Dataset ===\")\n",
    "print(df.info())\n",
    "\n",
    "# Estad√≠sticas descriptivas de columnas num√©ricas\n",
    "print(\"\\n=== Estad√≠sticas Descriptivas ===\")\n",
    "print(df.describe())\n",
    "\n",
    "# Conteo de valores nulos por columna\n",
    "print(\"\\n=== Valores Nulos por Columna ===\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a94187",
   "metadata": {},
   "source": [
    "## Limpieza y Preprocesamiento del Dataset\n",
    "\n",
    "Antes de insertar en las bases de datos, limpiamos y normalizamos la informaci√≥n para:\n",
    "- Manejar valores nulos.\n",
    "- Convertir `event_time` a formato `datetime` UTC.\n",
    "- Garantizar compatibilidad con los formatos de cada base de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68313f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Procesando chunk 1...\n",
      "‚úÖ Chunk 1 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 2...\n",
      "‚úÖ Chunk 2 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 3...\n",
      "‚úÖ Chunk 3 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 4...\n",
      "‚úÖ Chunk 4 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 5...\n",
      "‚úÖ Chunk 5 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 6...\n",
      "‚úÖ Chunk 6 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 7...\n",
      "‚úÖ Chunk 7 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 8...\n",
      "‚úÖ Chunk 8 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 9...\n",
      "‚úÖ Chunk 9 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 10...\n",
      "‚úÖ Chunk 10 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 11...\n",
      "‚úÖ Chunk 11 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 12...\n",
      "‚úÖ Chunk 12 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 13...\n",
      "‚úÖ Chunk 13 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 14...\n",
      "‚úÖ Chunk 14 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 15...\n",
      "‚úÖ Chunk 15 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 16...\n",
      "‚úÖ Chunk 16 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 17...\n",
      "‚úÖ Chunk 17 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 18...\n",
      "‚úÖ Chunk 18 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 19...\n",
      "‚úÖ Chunk 19 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 20...\n",
      "‚úÖ Chunk 20 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 21...\n",
      "‚úÖ Chunk 21 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 22...\n",
      "‚úÖ Chunk 22 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 23...\n",
      "‚úÖ Chunk 23 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 24...\n",
      "‚úÖ Chunk 24 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 25...\n",
      "‚úÖ Chunk 25 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 26...\n",
      "‚úÖ Chunk 26 limpio y listo (100000 filas)\n",
      "üîÑ Procesando chunk 27...\n",
      "‚úÖ Chunk 27 limpio y listo (33521 filas)\n",
      "‚è± Tiempo total de limpieza: 73.03 segundos\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "CSV_PATH = \"./datasets/ecommerce/kz.csv\"\n",
    "CHUNK_SIZE = 100_000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(CSV_PATH, chunksize=CHUNK_SIZE)):\n",
    "    print(f\"üîÑ Procesando chunk {i+1}...\")\n",
    "\n",
    "    # Conversi√≥n de event_time a datetime UTC\n",
    "    chunk[\"event_time\"] = pd.to_datetime(chunk[\"event_time\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    # Manejo de valores nulos\n",
    "    chunk[\"category_code\"] = chunk[\"category_code\"].fillna(\"\")\n",
    "    chunk[\"brand\"] = chunk[\"brand\"].fillna(\"\")\n",
    "    chunk[\"category_id\"] = chunk[\"category_id\"].fillna(0).astype(\"int64\")\n",
    "    chunk[\"price\"] = chunk[\"price\"].fillna(0).astype(\"float64\")\n",
    "    chunk[\"user_id\"] = chunk[\"user_id\"].fillna(0).astype(\"int64\")\n",
    "\n",
    "    # Aqu√≠ podr√≠amos insertar directamente en MongoDB, Redis o HBase\n",
    "    # insert_to_mongo(chunk)  # Ejemplo\n",
    "    print(f\"‚úÖ Chunk {i+1} limpio y listo ({len(chunk)} filas)\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"‚è± Tiempo total de limpieza: {end_time - start_time:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22144696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Valores Nulos por Columna (Dataset Limpio) ===\n",
      "event_time             0\n",
      "order_id               0\n",
      "product_id             0\n",
      "category_id       431954\n",
      "category_code     612202\n",
      "brand             506005\n",
      "price             431954\n",
      "user_id          2069352\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verificaci√≥n de valores nulos en el dataset limpio\n",
    "print(\"=== Valores Nulos por Columna (Dataset Limpio) ===\")\n",
    "null_counts = df.isnull().sum()\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb590f",
   "metadata": {},
   "source": [
    "## üì• Inserci√≥n de datos en MongoDB\n",
    "\n",
    "Conexi√≥n y carga de datos en MongoDB usando `pymongo`.\n",
    "\n",
    "Pasos:\n",
    "1. Conectarse al servidor de MongoDB.\n",
    "2. Seleccionar la base de datos (`ecommerce_db`) y la colecci√≥n (`purchases`).\n",
    "3. Convertir el DataFrame de Pandas a diccionarios (`to_dict('records')`).\n",
    "4. Insertar los datos con `insert_many()`.\n",
    "\n",
    "> ‚ö†Ô∏è **Nota:** El contenedor MongoDB debe estar corriendo y accesible en el puerto `27017`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da27c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Colecci√≥n limpiada antes de la inserci√≥n.\n",
      "\n",
      "‚úÖ Bloque 1: 100000 registros (4.23 seg)\n",
      "‚úÖ Bloque 2: 100000 registros (4.72 seg)\n",
      "‚úÖ Bloque 3: 100000 registros (4.35 seg)\n",
      "‚úÖ Bloque 4: 100000 registros (4.28 seg)\n",
      "‚úÖ Bloque 5: 100000 registros (4.59 seg)\n",
      "‚úÖ Bloque 6: 100000 registros (4.16 seg)\n",
      "‚úÖ Bloque 7: 100000 registros (4.12 seg)\n",
      "‚úÖ Bloque 8: 100000 registros (4.49 seg)\n",
      "‚úÖ Bloque 9: 100000 registros (4.16 seg)\n",
      "‚úÖ Bloque 10: 100000 registros (4.07 seg)\n",
      "‚úÖ Bloque 11: 100000 registros (4.50 seg)\n",
      "‚úÖ Bloque 12: 100000 registros (4.19 seg)\n",
      "‚úÖ Bloque 13: 100000 registros (4.13 seg)\n",
      "‚úÖ Bloque 14: 100000 registros (4.59 seg)\n",
      "‚úÖ Bloque 15: 100000 registros (4.14 seg)\n",
      "‚úÖ Bloque 16: 100000 registros (4.37 seg)\n",
      "‚úÖ Bloque 17: 100000 registros (4.21 seg)\n",
      "‚úÖ Bloque 18: 100000 registros (4.31 seg)\n",
      "‚úÖ Bloque 19: 100000 registros (4.29 seg)\n",
      "‚úÖ Bloque 20: 100000 registros (4.25 seg)\n",
      "‚úÖ Bloque 21: 100000 registros (4.25 seg)\n",
      "‚úÖ Bloque 22: 100000 registros (4.62 seg)\n",
      "‚úÖ Bloque 23: 100000 registros (4.24 seg)\n",
      "‚úÖ Bloque 24: 100000 registros (4.23 seg)\n",
      "‚úÖ Bloque 25: 100000 registros (4.64 seg)\n",
      "‚úÖ Bloque 26: 100000 registros (4.31 seg)\n",
      "‚úÖ Bloque 27: 33521 registros (1.54 seg)\n",
      "\n",
      "‚è± Tiempo total: 113.98 seg\n",
      "üìä Total documentos insertados: 2633521\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# ==== Par√°metros de conexi√≥n ====\n",
    "MONGO_USER = \"admin\"\n",
    "MONGO_PASS = \"pass\"\n",
    "MONGO_HOST = \"localhost\"   # Si Jupyter y MongoDB est√°n en la misma VM\n",
    "MONGO_PORT = 27017\n",
    "DB_NAME = \"ecommerce_db\"\n",
    "COLLECTION_NAME = \"purchases\"\n",
    "\n",
    "# ==== Conexi√≥n a MongoDB ====\n",
    "client = MongoClient(f\"mongodb://{MONGO_USER}:{MONGO_PASS}@{MONGO_HOST}:{MONGO_PORT}/?authSource=admin\")\n",
    "db = client[DB_NAME]\n",
    "collection = db[COLLECTION_NAME]\n",
    "\n",
    "# ==== Limpieza de la colecci√≥n ====\n",
    "collection.drop()\n",
    "print(\"üßπ Colecci√≥n limpiada antes de la inserci√≥n.\\n\")\n",
    "\n",
    "# ==== Par√°metros de carga ====\n",
    "CHUNK_SIZE = 100_000  # N√∫mero de registros por bloque\n",
    "total_rows = len(df)  # df debe estar previamente cargado con el dataset\n",
    "total_start_time = time.time()\n",
    "\n",
    "# ==== Inserci√≥n por bloques ====\n",
    "for i in range(0, total_rows, CHUNK_SIZE):\n",
    "    chunk_start_time = time.time()\n",
    "\n",
    "    # Seleccionar un bloque de datos\n",
    "    df_chunk = df.iloc[i:i + CHUNK_SIZE]\n",
    "    data_dict = df_chunk.to_dict(\"records\")\n",
    "\n",
    "    # Insertar en MongoDB\n",
    "    result = collection.insert_many(data_dict)\n",
    "\n",
    "    chunk_end_time = time.time()\n",
    "    print(f\"‚úÖ Bloque {i // CHUNK_SIZE + 1}: {len(result.inserted_ids)} registros \"\n",
    "          f\"({chunk_end_time - chunk_start_time:.2f} seg)\")\n",
    "\n",
    "# ==== Tiempo total ====\n",
    "total_end_time = time.time()\n",
    "print(f\"\\n‚è± Tiempo total: {total_end_time - total_start_time:.2f} seg\")\n",
    "print(f\"üìä Total documentos insertados: {collection.count_documents({})}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d1dbe",
   "metadata": {},
   "source": [
    "## Carga del dataset en Redis\n",
    "\n",
    "En esta secci√≥n conectaremos con Redis desde Python utilizando la librer√≠a `redis-py`.\n",
    "Cargaremos los datos del dataset en formato **Hash** (`HSET`), de forma que cada\n",
    "registro tendr√° una clave √∫nica y todos sus campos estar√°n agrupados en una misma estructura.\n",
    "\n",
    "### Estrategia:\n",
    "- Usaremos la conexi√≥n al contenedor Redis que corre en la misma VM (`localhost`).\n",
    "- Guardaremos los datos como:\n",
    "  - **Clave:** `purchase:<id>`\n",
    "  - **Campos:** `category`, `brand`, `price`, `purchase_date`, etc.\n",
    "- Cargaremos en bloques de **100‚ÄØ000 registros** para no saturar la memoria.\n",
    "- Antes de insertar, limpiaremos el espacio de claves relacionadas con `purchase:` para evitar duplicados.\n",
    "- Mediremos el tiempo por bloque y el tiempo total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e68a938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conectado a Redis\n",
      "üì¶ Total de registros en dataset: 2,633,521\n",
      "üßπ Eliminadas 2,633,521 claves antiguas en Redis\n",
      "‚úÖ Bloque 1: 100,000 registros en 42.05 seg\n",
      "‚úÖ Bloque 2: 100,000 registros en 40.58 seg\n",
      "‚úÖ Bloque 3: 100,000 registros en 40.79 seg\n",
      "‚úÖ Bloque 4: 100,000 registros en 40.61 seg\n",
      "‚úÖ Bloque 5: 100,000 registros en 40.09 seg\n",
      "‚úÖ Bloque 6: 100,000 registros en 41.15 seg\n",
      "‚úÖ Bloque 7: 100,000 registros en 41.36 seg\n",
      "‚úÖ Bloque 8: 100,000 registros en 41.28 seg\n",
      "‚úÖ Bloque 9: 100,000 registros en 41.36 seg\n",
      "‚úÖ Bloque 10: 100,000 registros en 41.18 seg\n",
      "‚úÖ Bloque 11: 100,000 registros en 41.31 seg\n",
      "‚úÖ Bloque 12: 100,000 registros en 41.40 seg\n",
      "‚úÖ Bloque 13: 100,000 registros en 41.06 seg\n",
      "‚úÖ Bloque 14: 100,000 registros en 41.12 seg\n",
      "‚úÖ Bloque 15: 100,000 registros en 40.68 seg\n",
      "‚úÖ Bloque 16: 100,000 registros en 41.31 seg\n",
      "‚úÖ Bloque 17: 100,000 registros en 40.84 seg\n",
      "‚úÖ Bloque 18: 100,000 registros en 41.65 seg\n",
      "‚úÖ Bloque 19: 100,000 registros en 41.26 seg\n",
      "‚úÖ Bloque 20: 100,000 registros en 41.48 seg\n",
      "‚úÖ Bloque 21: 100,000 registros en 41.55 seg\n",
      "‚úÖ Bloque 22: 100,000 registros en 42.04 seg\n",
      "‚úÖ Bloque 23: 100,000 registros en 41.14 seg\n",
      "‚úÖ Bloque 24: 100,000 registros en 41.60 seg\n",
      "‚úÖ Bloque 25: 100,000 registros en 41.37 seg\n",
      "‚úÖ Bloque 26: 100,000 registros en 42.68 seg\n",
      "‚úÖ Bloque 27: 33,521 registros en 13.66 seg\n",
      "üèÅ Inserci√≥n total completada en 1086.61 segundos\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# ==== Par√°metros de conexi√≥n a Redis ====\n",
    "redis_host = \"localhost\"\n",
    "redis_port = 6379\n",
    "redis_pass = None  # Cambiar si se configur√≥ contrase√±a\n",
    "\n",
    "# ==== Conexi√≥n ====\n",
    "r = redis.Redis(host=redis_host, port=redis_port, password=redis_pass, decode_responses=True)\n",
    "\n",
    "# ==== Verificaci√≥n de conexi√≥n ====\n",
    "try:\n",
    "    r.ping()\n",
    "    print(\"‚úÖ Conectado a Redis\")\n",
    "except redis.exceptions.ConnectionError as e:\n",
    "    raise SystemExit(f\"‚ùå No se pudo conectar a Redis: {e}\")\n",
    "\n",
    "# ==== Ruta del dataset ====\n",
    "dataset_path = \"./datasets/ecommerce/kz.csv\"\n",
    "\n",
    "# ==== Cargar en DataFrame ====\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# ==== Par√°metros de carga ====\n",
    "CHUNK_SIZE = 100_000\n",
    "total_rows = len(df)\n",
    "print(f\"üì¶ Total de registros en dataset: {total_rows:,}\")\n",
    "\n",
    "# ==== Limpieza previa de datos ====\n",
    "keys_to_delete = r.keys(\"purchase:*\")\n",
    "if keys_to_delete:\n",
    "    for i in range(0, len(keys_to_delete), 10_000):  # evitar saturar delete\n",
    "        r.delete(*keys_to_delete[i:i+10_000])\n",
    "    print(f\"üßπ Eliminadas {len(keys_to_delete):,} claves antiguas en Redis\")\n",
    "\n",
    "# ==== Inserci√≥n por bloques ====\n",
    "start_total = time.time()\n",
    "for i in range(0, total_rows, CHUNK_SIZE):\n",
    "    chunk = df.iloc[i:i + CHUNK_SIZE]\n",
    "    start_chunk = time.time()\n",
    "\n",
    "    pipe = r.pipeline(transaction=False)\n",
    "    for idx, row in chunk.iterrows():\n",
    "        key = f\"purchase:{idx}\"\n",
    "        pipe.hset(key, mapping={k: (\"\" if pd.isna(v) else str(v)) for k, v in row.to_dict().items()})\n",
    "\n",
    "    pipe.execute()\n",
    "    elapsed_chunk = time.time() - start_chunk\n",
    "    print(f\"‚úÖ Bloque {i//CHUNK_SIZE + 1}: {len(chunk):,} registros en {elapsed_chunk:.2f} seg\")\n",
    "\n",
    "elapsed_total = time.time() - start_total\n",
    "print(f\"üèÅ Inserci√≥n total completada en {elapsed_total:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b42b1c",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Carga del Dataset en HBase\n",
    "\n",
    "En esta secci√≥n insertaremos el dataset limpio en **HBase**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Configuraci√≥n de la prueba\n",
    "- **Dataset:** `kz.csv` (2‚ÄØ633‚ÄØ521 registros)\n",
    "- **Bloques de inserci√≥n:** 100‚ÄØ000 registros (√∫ltimo bloque menor)\n",
    "- **HBase:** Contenedor Docker (`hbase:latest`) ejecutando en la misma VM\n",
    "- **Conexi√≥n:** Usaremos la librer√≠a `happybase` v√≠a `thriftpy2` para comunicaci√≥n con el servidor Thrift de HBase.\n",
    "- **VM:** `Standard_A4m_v2` ‚Äî 8 vCPU, 32‚ÄØGB RAM\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Consideraciones\n",
    "1. Antes de cargar datos, nos aseguraremos de que **la tabla no exista** o **se vac√≠e** si ya existe, para evitar duplicados.\n",
    "2. Cada registro se insertar√° como **una fila** en HBase, usando `order_id` como **row key**.\n",
    "3. Usaremos `batch()` para mejorar el rendimiento y reducir el overhead de conexi√≥n.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d317f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conectado a HBase\n",
      "üìÑ La tabla 'purchases' ya existe.\n",
      "üßπ Limpiando registros antiguos de la tabla...\n",
      "üßπ Tabla vac√≠a.\n",
      "\n",
      "üì¶ Total de registros en dataset: 2,633,521\n",
      "‚úÖ Bloque 1: 100,000 registros en 49.28 segundos\n",
      "‚úÖ Bloque 2: 100,000 registros en 47.64 segundos\n",
      "‚úÖ Bloque 3: 100,000 registros en 47.18 segundos\n",
      "‚úÖ Bloque 4: 100,000 registros en 47.28 segundos\n",
      "‚úÖ Bloque 5: 100,000 registros en 47.85 segundos\n",
      "‚úÖ Bloque 6: 100,000 registros en 47.16 segundos\n",
      "‚úÖ Bloque 7: 100,000 registros en 47.60 segundos\n",
      "‚úÖ Bloque 8: 100,000 registros en 47.45 segundos\n",
      "‚úÖ Bloque 9: 100,000 registros en 48.55 segundos\n",
      "‚úÖ Bloque 10: 100,000 registros en 49.36 segundos\n",
      "‚úÖ Bloque 11: 100,000 registros en 48.99 segundos\n",
      "‚úÖ Bloque 12: 100,000 registros en 47.98 segundos\n",
      "‚úÖ Bloque 13: 100,000 registros en 48.40 segundos\n",
      "‚úÖ Bloque 14: 100,000 registros en 46.95 segundos\n",
      "‚úÖ Bloque 15: 100,000 registros en 46.82 segundos\n",
      "‚úÖ Bloque 16: 100,000 registros en 47.52 segundos\n",
      "‚úÖ Bloque 17: 100,000 registros en 47.99 segundos\n",
      "‚úÖ Bloque 18: 100,000 registros en 48.58 segundos\n",
      "‚úÖ Bloque 19: 100,000 registros en 48.02 segundos\n",
      "‚úÖ Bloque 20: 100,000 registros en 47.55 segundos\n",
      "‚úÖ Bloque 21: 100,000 registros en 46.78 segundos\n",
      "‚úÖ Bloque 22: 100,000 registros en 47.08 segundos\n",
      "‚úÖ Bloque 23: 100,000 registros en 47.56 segundos\n",
      "‚úÖ Bloque 24: 100,000 registros en 46.85 segundos\n",
      "‚úÖ Bloque 25: 100,000 registros en 47.60 segundos\n",
      "‚úÖ Bloque 26: 100,000 registros en 48.93 segundos\n",
      "‚úÖ Bloque 27: 33,521 registros en 16.09 segundos\n",
      "\n",
      "üèÅ Inserci√≥n total completada en 1259.19 segundos\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import happybase\n",
    "import time\n",
    "\n",
    "# ==== Par√°metros de conexi√≥n ====\n",
    "HBASE_HOST = \"localhost\"\n",
    "HBASE_PORT = 9090   # Puerto del servicio Thrift de HBase\n",
    "TABLE_NAME = \"purchases\"\n",
    "\n",
    "# ==== Conectar a HBase ====\n",
    "connection = happybase.Connection(host=HBASE_HOST, port=HBASE_PORT)\n",
    "connection.open()\n",
    "\n",
    "print(\"‚úÖ Conectado a HBase\")\n",
    "\n",
    "# ==== Crear tabla si no existe ====\n",
    "families = {'cf': dict()}  # 'cf' = column family\n",
    "if TABLE_NAME.encode() not in connection.tables():\n",
    "    connection.create_table(TABLE_NAME, families)\n",
    "    print(f\"üÜï Tabla creada: {TABLE_NAME}\")\n",
    "else:\n",
    "    print(f\"üìÑ La tabla '{TABLE_NAME}' ya existe.\")\n",
    "\n",
    "table = connection.table(TABLE_NAME)\n",
    "\n",
    "# ==== Limpieza previa ====\n",
    "print(\"üßπ Limpiando registros antiguos de la tabla...\")\n",
    "# Nota: happybase no tiene truncate directo, as√≠ que borramos fila a fila\n",
    "for key, _ in table.scan():\n",
    "    table.delete(key)\n",
    "print(\"üßπ Tabla vac√≠a.\\n\")\n",
    "\n",
    "# ==== Cargar dataset ====\n",
    "dataset_path = \"./datasets/ecommerce/kz.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "CHUNK_SIZE = 100_000\n",
    "total_rows = len(df)\n",
    "print(f\"üì¶ Total de registros en dataset: {total_rows:,}\")\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for i in range(0, total_rows, CHUNK_SIZE):\n",
    "    chunk = df.iloc[i:i + CHUNK_SIZE]\n",
    "    start_chunk = time.time()\n",
    "\n",
    "    # Usar batch para inserci√≥n r√°pida\n",
    "    with table.batch(batch_size=CHUNK_SIZE) as b:\n",
    "        for _, row in chunk.iterrows():\n",
    "            row_key = str(row[\"order_id\"]).encode()\n",
    "            b.put(row_key, {\n",
    "                b\"cf:event_time\": str(row[\"event_time\"]).encode(),\n",
    "                b\"cf:product_id\": str(row[\"product_id\"]).encode(),\n",
    "                b\"cf:category_id\": str(row[\"category_id\"]).encode(),\n",
    "                b\"cf:category_code\": str(row[\"category_code\"]).encode() if pd.notna(row[\"category_code\"]) else b\"\",\n",
    "                b\"cf:brand\": str(row[\"brand\"]).encode() if pd.notna(row[\"brand\"]) else b\"\",\n",
    "                b\"cf:price\": str(row[\"price\"]).encode(),\n",
    "                b\"cf:user_id\": str(row[\"user_id\"]).encode() if pd.notna(row[\"user_id\"]) else b\"\"\n",
    "            })\n",
    "\n",
    "    elapsed_chunk = time.time() - start_chunk\n",
    "    print(f\"‚úÖ Bloque {i//CHUNK_SIZE + 1}: {len(chunk):,} registros en {elapsed_chunk:.2f} segundos\")\n",
    "\n",
    "total_elapsed = time.time() - total_start_time\n",
    "print(f\"\\nüèÅ Inserci√≥n total completada en {total_elapsed:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb0364",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Consulta: Categor√≠a m√°s vendida\n",
    "\n",
    "**Objetivo:** Determinar cu√°l categor√≠a (`category_code`) aparece con mayor frecuencia en las transacciones.\n",
    "\n",
    "Se ejecutar√° la misma consulta en:\n",
    "- **MongoDB**\n",
    "- **Redis**\n",
    "- **HBase**\n",
    "\n",
    "Adem√°s, se medir√° el tiempo de ejecuci√≥n en cada base de datos para su posterior comparaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "423485e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Categor√≠a m√°s vendida (MongoDB): nan con 612,202 ventas\n",
      "‚è± Tiempo: 8.5758 segundos\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Conexi√≥n MongoDB\n",
    "mongo_client = MongoClient(f\"mongodb://{MONGO_USER}:{MONGO_PASS}@{MONGO_HOST}:{MONGO_PORT}/?authSource=admin\")\n",
    "mongo_db = mongo_client[DB_NAME]\n",
    "mongo_col = mongo_db[COLLECTION_NAME]\n",
    "\n",
    "start_time = time.time()\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$category_code\", \"total\": {\"$sum\": 1}}},\n",
    "    {\"$sort\": {\"total\": -1}},\n",
    "    {\"$limit\": 1}\n",
    "]\n",
    "result = list(mongo_col.aggregate(pipeline))\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üìä Categor√≠a m√°s vendida (MongoDB): {result[0]['_id']} con {result[0]['total']:,} ventas\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a31e1ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Categor√≠a m√°s vendida (Redis): electronics.smartphone con 357,682 ventas\n",
      "‚è± Tiempo: 1238.2801 segundos\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Conexi√≥n Redis\n",
    "r = redis.Redis(host=redis_host, port=redis_port, password=redis_pass)\n",
    "\n",
    "start_time = time.time()\n",
    "category_counter = Counter()\n",
    "\n",
    "for key in r.scan_iter(\"purchase:*\"):\n",
    "    category_code = r.hget(key, \"category_code\")\n",
    "    if category_code:\n",
    "        category_counter[category_code.decode()] += 1\n",
    "\n",
    "top_category, top_count = category_counter.most_common(1)[0]\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üìä Categor√≠a m√°s vendida (Redis): {top_category} con {top_count:,} ventas\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c71bf86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Categor√≠a m√°s vendida (HBase): electronics.smartphone con 213,002 ventas\n",
      "‚è± Tiempo: 173.2701 segundos\n"
     ]
    }
   ],
   "source": [
    "import happybase\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Conexi√≥n HBase\n",
    "connection = happybase.Connection(host=\"localhost\", port=9090)\n",
    "table = connection.table(\"purchases\")\n",
    "\n",
    "start_time = time.time()\n",
    "category_counter = Counter()\n",
    "\n",
    "for key, data in table.scan():\n",
    "    category_code = data.get(b\"cf:category_code\")\n",
    "    if category_code:\n",
    "        category_counter[category_code.decode()] += 1\n",
    "\n",
    "top_category, top_count = category_counter.most_common(1)[0]\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üìä Categor√≠a m√°s vendida (HBase): {top_category} con {top_count:,} ventas\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd2446",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Consulta: Marca que gener√≥ m√°s ingresos brutos\n",
    "\n",
    "**Objetivo:** Determinar cu√°l `brand` gener√≥ la mayor suma de `price`.\n",
    "\n",
    "Se ejecutar√° en:\n",
    "- **MongoDB**\n",
    "- **Redis**\n",
    "- **HBase**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e24bb6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ Marca con m√°s ingresos (MongoDB): samsung con $90052821.66\n",
      "‚è± Tiempo: 9.7873 segundos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$brand\", \"total_ingresos\": {\"$sum\": \"$price\"}}},\n",
    "    {\"$sort\": {\"total_ingresos\": -1}},\n",
    "    {\"$limit\": 1}\n",
    "]\n",
    "result = list(mongo_col.aggregate(pipeline))\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üí∞ Marca con m√°s ingresos (MongoDB): {result[0]['_id']} con ${result[0]['total_ingresos']:.2f}\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07df4061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ Marca con m√°s ingresos (Redis): samsung con $90,052,821.66\n",
      "‚è± Tiempo: 2306.0426 segundos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "brand_revenue = Counter()\n",
    "\n",
    "for key in r.scan_iter(\"purchase:*\"):\n",
    "    brand = r.hget(key, \"brand\")\n",
    "    price = r.hget(key, \"price\")\n",
    "    if brand and price:\n",
    "        try:\n",
    "            brand_revenue[brand.decode()] += float(price)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "top_brand, top_revenue = brand_revenue.most_common(1)[0]\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üí∞ Marca con m√°s ingresos (Redis): {top_brand} con ${top_revenue:,.2f}\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d81fb986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Procesadas 5,001 filas...\n",
      "üì¶ Procesadas 10,002 filas...\n",
      "üì¶ Procesadas 15,003 filas...\n",
      "üì¶ Procesadas 20,004 filas...\n",
      "üì¶ Procesadas 25,005 filas...\n",
      "üì¶ Procesadas 30,006 filas...\n",
      "üì¶ Procesadas 35,007 filas...\n",
      "üì¶ Procesadas 40,008 filas...\n",
      "üì¶ Procesadas 45,009 filas...\n",
      "üì¶ Procesadas 50,010 filas...\n",
      "üì¶ Procesadas 55,011 filas...\n",
      "üì¶ Procesadas 60,012 filas...\n",
      "üì¶ Procesadas 65,013 filas...\n",
      "üì¶ Procesadas 70,014 filas...\n",
      "üì¶ Procesadas 75,015 filas...\n",
      "üì¶ Procesadas 80,016 filas...\n",
      "üì¶ Procesadas 85,017 filas...\n",
      "üì¶ Procesadas 90,018 filas...\n",
      "üì¶ Procesadas 95,019 filas...\n",
      "üì¶ Procesadas 100,020 filas...\n",
      "üì¶ Procesadas 105,021 filas...\n",
      "üì¶ Procesadas 110,022 filas...\n",
      "üì¶ Procesadas 115,023 filas...\n",
      "üì¶ Procesadas 120,024 filas...\n",
      "üì¶ Procesadas 125,025 filas...\n",
      "üì¶ Procesadas 130,026 filas...\n",
      "üì¶ Procesadas 135,027 filas...\n",
      "üì¶ Procesadas 140,028 filas...\n",
      "üì¶ Procesadas 145,029 filas...\n",
      "üì¶ Procesadas 150,030 filas...\n",
      "üì¶ Procesadas 155,031 filas...\n",
      "üì¶ Procesadas 160,032 filas...\n",
      "üì¶ Procesadas 165,033 filas...\n",
      "üì¶ Procesadas 170,034 filas...\n",
      "üì¶ Procesadas 175,035 filas...\n",
      "üì¶ Procesadas 180,036 filas...\n",
      "üì¶ Procesadas 185,037 filas...\n",
      "üì¶ Procesadas 190,038 filas...\n",
      "üì¶ Procesadas 195,039 filas...\n",
      "üì¶ Procesadas 200,040 filas...\n",
      "üì¶ Procesadas 205,041 filas...\n",
      "üì¶ Procesadas 210,042 filas...\n",
      "üì¶ Procesadas 215,043 filas...\n",
      "üì¶ Procesadas 220,044 filas...\n",
      "üì¶ Procesadas 225,045 filas...\n",
      "üì¶ Procesadas 230,046 filas...\n",
      "üì¶ Procesadas 235,047 filas...\n",
      "üì¶ Procesadas 240,048 filas...\n",
      "üì¶ Procesadas 245,049 filas...\n",
      "üì¶ Procesadas 250,050 filas...\n",
      "üì¶ Procesadas 255,051 filas...\n",
      "üì¶ Procesadas 260,052 filas...\n",
      "üì¶ Procesadas 265,053 filas...\n",
      "üì¶ Procesadas 270,054 filas...\n",
      "üì¶ Procesadas 275,055 filas...\n",
      "üì¶ Procesadas 280,056 filas...\n",
      "üì¶ Procesadas 285,057 filas...\n",
      "üì¶ Procesadas 290,058 filas...\n",
      "üì¶ Procesadas 295,059 filas...\n",
      "üì¶ Procesadas 300,060 filas...\n",
      "üì¶ Procesadas 305,061 filas...\n",
      "üì¶ Procesadas 310,062 filas...\n",
      "üì¶ Procesadas 315,063 filas...\n",
      "üì¶ Procesadas 320,064 filas...\n",
      "üì¶ Procesadas 325,065 filas...\n",
      "üì¶ Procesadas 330,066 filas...\n",
      "üì¶ Procesadas 335,067 filas...\n",
      "üì¶ Procesadas 340,068 filas...\n",
      "üì¶ Procesadas 345,069 filas...\n",
      "üì¶ Procesadas 350,070 filas...\n",
      "üì¶ Procesadas 355,071 filas...\n",
      "üì¶ Procesadas 360,072 filas...\n",
      "üì¶ Procesadas 365,073 filas...\n",
      "üì¶ Procesadas 370,074 filas...\n",
      "üì¶ Procesadas 375,075 filas...\n",
      "üì¶ Procesadas 380,076 filas...\n",
      "üì¶ Procesadas 385,077 filas...\n",
      "üì¶ Procesadas 390,078 filas...\n",
      "üì¶ Procesadas 395,079 filas...\n",
      "üì¶ Procesadas 400,080 filas...\n",
      "üì¶ Procesadas 405,081 filas...\n",
      "üì¶ Procesadas 410,082 filas...\n",
      "üì¶ Procesadas 415,083 filas...\n",
      "üì¶ Procesadas 420,084 filas...\n",
      "üì¶ Procesadas 425,085 filas...\n",
      "üì¶ Procesadas 430,086 filas...\n",
      "üì¶ Procesadas 435,087 filas...\n",
      "üì¶ Procesadas 440,088 filas...\n",
      "üì¶ Procesadas 445,089 filas...\n",
      "üì¶ Procesadas 450,090 filas...\n",
      "üì¶ Procesadas 455,091 filas...\n",
      "üì¶ Procesadas 460,092 filas...\n",
      "üì¶ Procesadas 465,093 filas...\n",
      "üì¶ Procesadas 470,094 filas...\n",
      "üì¶ Procesadas 475,095 filas...\n",
      "üì¶ Procesadas 480,096 filas...\n",
      "üì¶ Procesadas 485,097 filas...\n",
      "üì¶ Procesadas 490,098 filas...\n",
      "üì¶ Procesadas 495,099 filas...\n",
      "üì¶ Procesadas 500,100 filas...\n",
      "üì¶ Procesadas 505,101 filas...\n",
      "üì¶ Procesadas 510,102 filas...\n",
      "üì¶ Procesadas 515,103 filas...\n",
      "üì¶ Procesadas 520,104 filas...\n",
      "üì¶ Procesadas 525,105 filas...\n",
      "üì¶ Procesadas 530,106 filas...\n",
      "üì¶ Procesadas 535,107 filas...\n",
      "üì¶ Procesadas 540,108 filas...\n",
      "üì¶ Procesadas 545,109 filas...\n",
      "üì¶ Procesadas 550,110 filas...\n",
      "üì¶ Procesadas 555,111 filas...\n",
      "üì¶ Procesadas 560,112 filas...\n",
      "üì¶ Procesadas 565,113 filas...\n",
      "üì¶ Procesadas 570,114 filas...\n",
      "üì¶ Procesadas 575,115 filas...\n",
      "üì¶ Procesadas 580,116 filas...\n",
      "üì¶ Procesadas 585,117 filas...\n",
      "üì¶ Procesadas 590,118 filas...\n",
      "üì¶ Procesadas 595,119 filas...\n",
      "üì¶ Procesadas 600,120 filas...\n",
      "üì¶ Procesadas 605,121 filas...\n",
      "üì¶ Procesadas 610,122 filas...\n",
      "üì¶ Procesadas 615,123 filas...\n",
      "üì¶ Procesadas 620,124 filas...\n",
      "üì¶ Procesadas 625,125 filas...\n",
      "üì¶ Procesadas 630,126 filas...\n",
      "üì¶ Procesadas 635,127 filas...\n",
      "üì¶ Procesadas 640,128 filas...\n",
      "üì¶ Procesadas 645,129 filas...\n",
      "üì¶ Procesadas 650,130 filas...\n",
      "üì¶ Procesadas 655,131 filas...\n",
      "üì¶ Procesadas 660,132 filas...\n",
      "üì¶ Procesadas 665,133 filas...\n",
      "üì¶ Procesadas 670,134 filas...\n",
      "üì¶ Procesadas 675,135 filas...\n",
      "üì¶ Procesadas 680,136 filas...\n",
      "üì¶ Procesadas 685,137 filas...\n",
      "üì¶ Procesadas 690,138 filas...\n",
      "üì¶ Procesadas 695,139 filas...\n",
      "üì¶ Procesadas 700,140 filas...\n",
      "üì¶ Procesadas 705,141 filas...\n",
      "üì¶ Procesadas 710,142 filas...\n",
      "üì¶ Procesadas 715,143 filas...\n",
      "üì¶ Procesadas 720,144 filas...\n",
      "üì¶ Procesadas 725,145 filas...\n",
      "üì¶ Procesadas 730,146 filas...\n",
      "üì¶ Procesadas 735,147 filas...\n",
      "üì¶ Procesadas 740,148 filas...\n",
      "üì¶ Procesadas 745,149 filas...\n",
      "üì¶ Procesadas 750,150 filas...\n",
      "üì¶ Procesadas 755,151 filas...\n",
      "üì¶ Procesadas 760,152 filas...\n",
      "üì¶ Procesadas 765,153 filas...\n",
      "üì¶ Procesadas 770,154 filas...\n",
      "üì¶ Procesadas 775,155 filas...\n",
      "üì¶ Procesadas 780,156 filas...\n",
      "üì¶ Procesadas 785,157 filas...\n",
      "üì¶ Procesadas 790,158 filas...\n",
      "üì¶ Procesadas 795,159 filas...\n",
      "üì¶ Procesadas 800,160 filas...\n",
      "üì¶ Procesadas 805,161 filas...\n",
      "üì¶ Procesadas 810,162 filas...\n",
      "üì¶ Procesadas 815,163 filas...\n",
      "üì¶ Procesadas 820,164 filas...\n",
      "üì¶ Procesadas 825,165 filas...\n",
      "üì¶ Procesadas 830,166 filas...\n",
      "üì¶ Procesadas 835,167 filas...\n",
      "üì¶ Procesadas 840,168 filas...\n",
      "üì¶ Procesadas 845,169 filas...\n",
      "üì¶ Procesadas 850,170 filas...\n",
      "üì¶ Procesadas 855,171 filas...\n",
      "üì¶ Procesadas 860,172 filas...\n",
      "üì¶ Procesadas 865,173 filas...\n",
      "üì¶ Procesadas 870,174 filas...\n",
      "üì¶ Procesadas 875,175 filas...\n",
      "üì¶ Procesadas 880,176 filas...\n",
      "üì¶ Procesadas 885,177 filas...\n",
      "üì¶ Procesadas 890,178 filas...\n",
      "üì¶ Procesadas 895,179 filas...\n",
      "üì¶ Procesadas 900,180 filas...\n",
      "üì¶ Procesadas 905,181 filas...\n",
      "üì¶ Procesadas 910,182 filas...\n",
      "üì¶ Procesadas 915,183 filas...\n",
      "üì¶ Procesadas 920,184 filas...\n",
      "üì¶ Procesadas 925,185 filas...\n",
      "üì¶ Procesadas 930,186 filas...\n",
      "üì¶ Procesadas 935,187 filas...\n",
      "üì¶ Procesadas 940,188 filas...\n",
      "üì¶ Procesadas 945,189 filas...\n",
      "üì¶ Procesadas 950,190 filas...\n",
      "üì¶ Procesadas 955,191 filas...\n",
      "üì¶ Procesadas 960,192 filas...\n",
      "üì¶ Procesadas 965,193 filas...\n",
      "üì¶ Procesadas 970,194 filas...\n",
      "üì¶ Procesadas 975,195 filas...\n",
      "üì¶ Procesadas 980,196 filas...\n",
      "üì¶ Procesadas 985,197 filas...\n",
      "üì¶ Procesadas 990,198 filas...\n",
      "üì¶ Procesadas 995,199 filas...\n",
      "üì¶ Procesadas 1,000,200 filas...\n",
      "üì¶ Procesadas 1,005,201 filas...\n",
      "üì¶ Procesadas 1,010,202 filas...\n",
      "üì¶ Procesadas 1,015,203 filas...\n",
      "üì¶ Procesadas 1,020,204 filas...\n",
      "üì¶ Procesadas 1,025,205 filas...\n",
      "üì¶ Procesadas 1,030,206 filas...\n",
      "üì¶ Procesadas 1,035,207 filas...\n",
      "üì¶ Procesadas 1,040,208 filas...\n",
      "üì¶ Procesadas 1,045,209 filas...\n",
      "üì¶ Procesadas 1,050,210 filas...\n",
      "üì¶ Procesadas 1,055,211 filas...\n",
      "üì¶ Procesadas 1,060,212 filas...\n",
      "üì¶ Procesadas 1,065,213 filas...\n",
      "üì¶ Procesadas 1,070,214 filas...\n",
      "üì¶ Procesadas 1,075,215 filas...\n",
      "üì¶ Procesadas 1,080,216 filas...\n",
      "üì¶ Procesadas 1,085,217 filas...\n",
      "üì¶ Procesadas 1,090,218 filas...\n",
      "üì¶ Procesadas 1,095,219 filas...\n",
      "üì¶ Procesadas 1,100,220 filas...\n",
      "üì¶ Procesadas 1,105,221 filas...\n",
      "üì¶ Procesadas 1,110,222 filas...\n",
      "üì¶ Procesadas 1,115,223 filas...\n",
      "üì¶ Procesadas 1,120,224 filas...\n",
      "üì¶ Procesadas 1,125,225 filas...\n",
      "üì¶ Procesadas 1,130,226 filas...\n",
      "üì¶ Procesadas 1,135,227 filas...\n",
      "üì¶ Procesadas 1,140,228 filas...\n",
      "üì¶ Procesadas 1,145,229 filas...\n",
      "üì¶ Procesadas 1,150,230 filas...\n",
      "üì¶ Procesadas 1,155,231 filas...\n",
      "üì¶ Procesadas 1,160,232 filas...\n",
      "üì¶ Procesadas 1,165,233 filas...\n",
      "üì¶ Procesadas 1,170,234 filas...\n",
      "üì¶ Procesadas 1,175,235 filas...\n",
      "üì¶ Procesadas 1,180,236 filas...\n",
      "üì¶ Procesadas 1,185,237 filas...\n",
      "üì¶ Procesadas 1,190,238 filas...\n",
      "üì¶ Procesadas 1,195,239 filas...\n",
      "üì¶ Procesadas 1,200,240 filas...\n",
      "üì¶ Procesadas 1,205,241 filas...\n",
      "üì¶ Procesadas 1,210,242 filas...\n",
      "üì¶ Procesadas 1,215,243 filas...\n",
      "üì¶ Procesadas 1,220,244 filas...\n",
      "üì¶ Procesadas 1,225,245 filas...\n",
      "üì¶ Procesadas 1,230,246 filas...\n",
      "üì¶ Procesadas 1,235,247 filas...\n",
      "üì¶ Procesadas 1,240,248 filas...\n",
      "üì¶ Procesadas 1,245,249 filas...\n",
      "üì¶ Procesadas 1,250,250 filas...\n",
      "üì¶ Procesadas 1,255,251 filas...\n",
      "üì¶ Procesadas 1,260,252 filas...\n",
      "üì¶ Procesadas 1,265,253 filas...\n",
      "üì¶ Procesadas 1,270,254 filas...\n",
      "üì¶ Procesadas 1,275,255 filas...\n",
      "üì¶ Procesadas 1,280,256 filas...\n",
      "üì¶ Procesadas 1,285,257 filas...\n",
      "üì¶ Procesadas 1,290,258 filas...\n",
      "üì¶ Procesadas 1,295,259 filas...\n",
      "üì¶ Procesadas 1,300,260 filas...\n",
      "üì¶ Procesadas 1,305,261 filas...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Procesadas 1,310,262 filas...\n",
      "üì¶ Procesadas 1,315,263 filas...\n",
      "üì¶ Procesadas 1,320,264 filas...\n",
      "üì¶ Procesadas 1,325,265 filas...\n",
      "üì¶ Procesadas 1,330,266 filas...\n",
      "üì¶ Procesadas 1,335,267 filas...\n",
      "üì¶ Procesadas 1,340,268 filas...\n",
      "üì¶ Procesadas 1,345,269 filas...\n",
      "üì¶ Procesadas 1,350,270 filas...\n",
      "üì¶ Procesadas 1,355,271 filas...\n",
      "üì¶ Procesadas 1,360,272 filas...\n",
      "üì¶ Procesadas 1,365,273 filas...\n",
      "üì¶ Procesadas 1,370,274 filas...\n",
      "üì¶ Procesadas 1,375,275 filas...\n",
      "üì¶ Procesadas 1,380,276 filas...\n",
      "üì¶ Procesadas 1,385,277 filas...\n",
      "üì¶ Procesadas 1,390,278 filas...\n",
      "üì¶ Procesadas 1,395,279 filas...\n",
      "üì¶ Procesadas 1,400,280 filas...\n",
      "üì¶ Procesadas 1,405,281 filas...\n",
      "üì¶ Procesadas 1,410,282 filas...\n",
      "üì¶ Procesadas 1,415,283 filas...\n",
      "üì¶ Procesadas 1,420,284 filas...\n",
      "üì¶ Procesadas 1,425,285 filas...\n",
      "üì¶ Procesadas 1,430,286 filas...\n",
      "üì¶ Procesadas 1,435,287 filas...\n",
      "üí∞ Marca con m√°s ingresos (HBase): samsung con $54,047,304.62\n",
      "‚è± Tiempo total: 110.51 segundos\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "brand_revenue = Counter()\n",
    "start_time = time.time()\n",
    "\n",
    "# Configurar tama√±o de bloque para scan\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "# Control de filas procesadas\n",
    "rows_processed = 0\n",
    "last_key = None\n",
    "\n",
    "while True:\n",
    "    # Si es la primera iteraci√≥n, no pasamos start_row\n",
    "    scan_args = {\"columns\": [b\"cf:brand\", b\"cf:price\"], \"batch_size\": BATCH_SIZE}\n",
    "    if last_key:\n",
    "        scan_args[\"row_start\"] = last_key\n",
    "\n",
    "    # Leer un bloque del escaneo\n",
    "    rows = list(table.scan(**scan_args, limit=BATCH_SIZE + 1))\n",
    "\n",
    "    if not rows:\n",
    "        break\n",
    "\n",
    "    # Procesar el bloque\n",
    "    for key, data in rows:\n",
    "        brand = data.get(b\"cf:brand\")\n",
    "        price = data.get(b\"cf:price\")\n",
    "        if brand and price:\n",
    "            try:\n",
    "                brand_revenue[brand.decode()] += float(price.decode())\n",
    "            except:\n",
    "                pass\n",
    "        last_key = key\n",
    "        rows_processed += 1\n",
    "\n",
    "    # Si se ley√≥ menos que el bloque esperado, hemos terminado\n",
    "    if len(rows) <= BATCH_SIZE:\n",
    "        break\n",
    "\n",
    "    print(f\"üì¶ Procesadas {rows_processed:,} filas...\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "if brand_revenue:\n",
    "    top_brand, top_revenue = brand_revenue.most_common(1)[0]\n",
    "    print(f\"üí∞ Marca con m√°s ingresos (HBase): {top_brand} con ${top_revenue:,.2f}\")\n",
    "    print(f\"‚è± Tiempo total: {elapsed_time:.2f} segundos\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se encontraron datos en la tabla.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9456ca2",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Consulta: Mes con m√°s ventas (UTC)\n",
    "\n",
    "**Objetivo:** Determinar en qu√© mes (`event_time`) se realizaron m√°s ventas.\n",
    "\n",
    "Se ejecutar√° en:\n",
    "- **MongoDB**\n",
    "- **Redis**\n",
    "- **HBase**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14b9deb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Mes con m√°s ventas (MongoDB): 2020-06 con 403,632 ventas\n",
      "‚è± Tiempo: 9.6747 segundos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pipeline = [\n",
    "    {\"$project\": {\"mes\": {\"$substr\": [\"$event_time\", 0, 7]}}},\n",
    "    {\"$group\": {\"_id\": \"$mes\", \"total\": {\"$sum\": 1}}},\n",
    "    {\"$sort\": {\"total\": -1}},\n",
    "    {\"$limit\": 1}\n",
    "]\n",
    "result = list(mongo_col.aggregate(pipeline))\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üìÖ Mes con m√°s ventas (MongoDB): {result[0]['_id']} con {result[0]['total']:,} ventas\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d574b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Mes con m√°s ventas (Redis): 2020-06 con 403,632 ventas\n",
      "‚è± Tiempo: 1224.5353 segundos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "month_counter = Counter()\n",
    "\n",
    "for key in r.scan_iter(\"purchase:*\"):\n",
    "    event_time = r.hget(key, \"event_time\")\n",
    "    if event_time:\n",
    "        month = event_time.decode()[:7]\n",
    "        month_counter[month] += 1\n",
    "\n",
    "top_month, top_count = month_counter.most_common(1)[0]\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üìÖ Mes con m√°s ventas (Redis): {top_month} con {top_count:,} ventas\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5d7f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Mes con m√°s ventas (HBase): 2020-06 con 211,552 ventas\n",
      "‚è± Tiempo: 172.2478 segundos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "month_counter = Counter()\n",
    "\n",
    "for key, data in table.scan():\n",
    "    event_time = data.get(b\"cf:event_time\")\n",
    "    if event_time:\n",
    "        month = event_time.decode()[:7]\n",
    "        month_counter[month] += 1\n",
    "\n",
    "top_month, top_count = month_counter.most_common(1)[0]\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"üìÖ Mes con m√°s ventas (HBase): {top_month} con {top_count:,} ventas\")\n",
    "print(f\"‚è± Tiempo: {elapsed_time:.4f} segundos\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
