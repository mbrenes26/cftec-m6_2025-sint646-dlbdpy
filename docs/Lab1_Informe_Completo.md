# Arquitectura de C√≥mputo ‚Äî Laboratorio #1

## Descripci√≥n General

El objetivo principal del Laboratorio #1 es analizar el rendimiento de tres bases de datos NoSQL (HBase, MongoDB y Redis) al procesar y consultar un dataset de gran volumen (2 millones de registros), todo ello montado sobre una infraestructura reproducible y automatizada con herramientas modernas de despliegue.

---

## Componentes Principales

### 1. M√°quina Virtual (Azure VM)

| Recurso          | Detalle                                                                 |
|------------------|-------------------------------------------------------------------------|
| Tipo             | Azure Virtual Machine                                                   |
| Sistema Operativo| Ubuntu 22.04 LTS                                                        |
| Recursos         | 2 vCPU, 8 GB RAM *(estimado para pruebas de laboratorio)*               |
| Despliegue       | Automatizado v√≠a Terraform                                              |
| Scripts de gesti√≥n | `restart_lab_services.sh` (reinicio de servicios principales)         |
| Servicios instalados | Docker, Jupyter Notebook, Python, librer√≠as espec√≠ficas              |

---

### 2. Servicios de Base de Datos (Docker Containers)

Cada base de datos se ejecuta dentro de su propio contenedor Docker en la misma m√°quina virtual.

#### HBase

- Imagen utilizada: `harisekhon/hbase`
- Exposici√≥n: Puerto 16010 (UI), 2181 (Zookeeper), 8080 (API HBase REST)
- Acceso desde Python: librer√≠a `happybase` o `hbase-python`
- Configuraci√≥n: volumen persistente local

#### MongoDB

- Imagen utilizada: `mongo`
- Puerto: 27017
- Acceso desde Python: `pymongo`
- Configuraci√≥n: usuario/clave opcional, persistencia con volumen

#### Redis

- Imagen utilizada: `redis`
- Puerto: 6379
- Acceso desde Python: `redis-py`
- Configuraci√≥n: sin autenticaci√≥n (modo laboratorio)

---

### 3. Carga y Consulta de Datos (Python)

| Componente      | Descripci√≥n                                                                 |
|-----------------|-----------------------------------------------------------------------------|
| Dataset         | Datos simulados de una tienda de electr√≥nicos (2 millones de registros)     |
| Script de carga | Insertar los datos en HBase, MongoDB y Redis                                |
| Script de consulta | Consultas espec√≠ficas: categor√≠a m√°s vendida, marca con m√°s ingresos, mes con m√°s ventas |
| Plataforma      | Jupyter Notebooks (`Lab1_Databases_ElectronicsStore.ipynb`)                 |
| Librer√≠as clave | `pandas`, `datetime`, `pymongo`, `redis`, `happybase`, `matplotlib`         |

---

### 4. Automatizaci√≥n y DevOps

#### Terraform

- Ubicaci√≥n: `terraform/`
- Proveedores: `azurerm`
- Backend: local (`terraform.tfstate`)
- Recursos definidos:
  - Grupo de recursos (`azurerm_resource_group`)
  - Red virtual y subred (`azurerm_virtual_network`, `azurerm_subnet`)
  - Interfaz de red (`azurerm_network_interface`)
  - M√°quina virtual Linux (`azurerm_linux_virtual_machine`)

#### GitHub Actions

- Ubicaci√≥n: `.github/workflows/`
- Flujos:
  - `terraform.yml`: despliegue de la infraestructura
  - `restart-lab-services.yml`: reinicio de contenedores en la VM
  - `Manage_Lab_VM.yml`: control manual de encendido/apagado de la VM

---

## Esquema Visual (Resumen)

```
                     +---------------------------+
                     |     Azure Virtual Machine |
                     |---------------------------|
                     | OS: Ubuntu 22.04          |
                     | Docker + Python + Jupyter |
                     +-----------+---------------+
                                 |
              +----------------+-------------------+
              |                |                   |
       +------+-----+   +------+-----+      +------+-----+
       |  MongoDB   |   |   HBase    |      |   Redis     |
       | (pymongo)  |   | (happybase)|      | (redis-py)  |
       +------------+   +------------+      +------------+
              |                |                   |
        Consultas         Consultas           Consultas
        Dataset            Dataset             Dataset
```

---

## Detalle de Infraestructura como C√≥digo (IaC) con Terraform

### Proveedor y Backend

```hcl
provider "azurerm" {
  features {}
}

terraform {
  backend "local" {
    path = "terraform.tfstate"
  }
}
```

### Grupo de recursos

```hcl
resource "azurerm_resource_group" "lab" {
  name     = var.resource_group_name
  location = var.location
}
```

### Red

```hcl
resource "azurerm_virtual_network" "lab" {
  name                = var.vnet_name
  address_space       = [var.vnet_address_space]
  location            = azurerm_resource_group.lab.location
  resource_group_name = azurerm_resource_group.lab.name
}

resource "azurerm_subnet" "lab" {
  name                 = var.subnet_name
  resource_group_name  = azurerm_resource_group.lab.name
  virtual_network_name = azurerm_virtual_network.lab.name
  address_prefixes     = [var.subnet_prefix]
}
```

### M√°quina Virtual

```hcl
resource "azurerm_linux_virtual_machine" "lab" {
  name                  = var.vm_name
  location              = azurerm_resource_group.lab.location
  resource_group_name   = azurerm_resource_group.lab.name
  network_interface_ids = [azurerm_network_interface.lab.id]
  size                  = var.vm_size

  admin_username = var.admin_username

  admin_ssh_key {
    username   = var.admin_username
    public_key = file(var.ssh_public_key_path)
  }

  os_disk {
    caching              = "ReadWrite"
    storage_account_type = "Standard_LRS"
  }

  source_image_reference {
    publisher = "Canonical"
    offer     = "UbuntuServer"
    sku       = "22_04-lts"
    version   = "latest"
  }

  disable_password_authentication = true
}
```

---

## Repositorio del Proyecto

Este laboratorio est√° documentado y versionado en el siguiente repositorio de GitHub:

üîó [https://github.com/mbrenes26/cftec-m6_2025-sint646-dlbdpy](https://github.com/mbrenes26/cftec-m6_2025-sint646-dlbdpy)

---

## Nota final acerca de la Arquitectura

La arquitectura del laboratorio busca balancear simplicidad, aislamiento y reproducibilidad. Al contener los servicios en una sola VM con contenedores separados, se facilita el manejo de recursos, reinicio controlado de servicios, y an√°lisis comparativo justo, ya que todas las bases de datos se ejecutan bajo condiciones similares.

---

# Registro de Tareas y Bit√°cora T√©cnica


### Registro de tarea exitosa ‚Äî Conexi√≥n SSH a la VM en Azure
#### Objetivo

- Conectarse por SSH a la m√°quina virtual vm-cftec-m62025-SINT646-lab01 creada con Terraform, validando:

- Configuraci√≥n correcta de NSG (puerto 22 abierto).

- Clave p√∫blica SSH correctamente configurada.

- VM operativa y accesible desde internet.

#### Pasos ejecutados

**Confirmaci√≥n de IP p√∫blica**

Desde Azure Portal, en el recurso pip-cftec-m62025-SINT646-lab01, se verific√≥ que la IP p√∫blica asignada es:

```
4.155.211.247
```

**Verificaci√≥n de reglas de seguridad**

Se comprob√≥ que el Network Security Group (NSG) asociado a la Subnet incluye la regla:

```
Name: Allow-SSH
Direction: Inbound
Protocol: TCP
Port: 22
Action: Allow
Source: Any
```
Esto asegura que el puerto 22 est√° abierto para conexiones SSH desde cualquier origen (configuraci√≥n de laboratorio).

**Ejecuci√≥n del comando SSH**

Desde la m√°quina local (Windows con Git Bash), se ejecut√≥:

```bash

ssh -i ~/.ssh/id_rsa azureuser@4.155.211.247
```
Donde:

- -i ~/.ssh/id_rsa ‚Üí ruta de la clave privada correspondiente a la clave p√∫blica configurada en Terraform.

- azureuser ‚Üí usuario administrador definido en la VM.

- 4.155.211.247 ‚Üí IP p√∫blica de la VM.

- Aceptaci√≥n de huella digital

Como era la primera conexi√≥n, el sistema mostr√≥:

```
The authenticity of host '4.155.211.247' can't be established...
```

Se respondi√≥:

```
yes
```
El sistema agreg√≥ la huella a la lista de known hosts.

Ingreso de passphrase de la clave privada

Al haberse configurado la clave privada con passphrase, el cliente SSH solicit√≥:

```
Enter passphrase for key '/c/Users/mario.brenes/.ssh/id_rsa':
```
Se ingres√≥ la passphrase correcta.

- Acceso exitoso a la VM

La sesi√≥n SSH se estableci√≥ mostrando:

```
Welcome to Ubuntu 20.04.6 LTS (GNU/Linux 5.15.0-1089-azure x86_64)
```
Informaci√≥n del sistema:

OS: Ubuntu 20.04.6 LTS

IP interna: 10.0.1.4

Usuario: azureuser

Estado: sin actualizaciones cr√≠ticas pendientes (0 updates inmediatas).

Aviso de reinicio requerido por cambios en el sistema:

```
*** System restart required ***
```
Resultado: ‚úÖ Conexi√≥n SSH establecida con √©xito.

La VM es accesible remotamente desde internet y est√° lista para configuraci√≥n adicional (instalaci√≥n de Docker y otros servicios).

### Registro de tarea ‚Äî Verificaci√≥n y actualizaci√≥n del sistema operativo
#### Objetivo

Verificar y actualizar el sistema operativo Ubuntu 20.04.6 LTS de la VM vm-cftec-m62025-SINT646-lab01 para garantizar que se encuentra al d√≠a antes de instalar Docker y otros servicios.

#### Pasos ejecutados
- Conexi√≥n a la VM por SSH
```
ssh -i ~/.ssh/id_rsa azureuser@4.155.211.247
```
Actualizaci√≥n de listas de paquetes

Comando:

```
sudo apt update
```
Resultado:

```
Hit:1 http://azure.archive.ubuntu.com/ubuntu focal InRelease
Hit:2 http://azure.archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:3 http://azure.archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:4 http://azure.archive.ubuntu.com/ubuntu focal-security InRelease
Reading package lists... Done
Building dependency tree       
Reading state information... Done
All packages are up to date.
```
Interpretaci√≥n:

Todas las listas de paquetes est√°n actualizadas.

No hay actualizaciones pendientes en los repositorios est√°ndar de Ubuntu.

Actualizaci√≥n de paquetes instalados

```
sudo apt upgrade -y
```
Resultado:

```
Calculating upgrade... Done
The following security updates require Ubuntu Pro with 'esm-infra' enabled:
  <lista de paquetes ESM>
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
```
Interpretaci√≥n:

No hay actualizaciones disponibles en los repositorios est√°ndar.

Algunos paquetes con soporte extendido (ESM) requieren Ubuntu Pro para recibir parches de seguridad.

#### Conclusi√≥n

No fue necesario ejecutar:

```
sudo apt autoremove -y
sudo reboot
```
El sistema ya estaba en el estado m√°s reciente posible sin habilitar Ubuntu Pro.

Resultado
‚úÖ El sistema est√° actualizado y no presenta paquetes pendientes de actualizaci√≥n en los repositorios est√°ndar de Ubuntu 20.04.

### Registro de tarea ‚Äî Instalaci√≥n y configuraci√≥n de Docker en la VM
#### Objetivo
Instalar y habilitar Docker Engine en la VM vm-cftec-m62025-SINT646-lab01 para permitir la ejecuci√≥n de contenedores necesarios en el laboratorio.

Pasos ejecutados
Conexi√≥n a la VM por SSH
```
ssh -i ~/.ssh/id_rsa azureuser@4.155.211.247
```
Instalaci√≥n de Docker

Comando:
```
sudo apt install -y docker.io
```
Resultado:

```
docker.io is already the newest version (26.1.3-0ubuntu1~20.04.1).
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
```
Interpretaci√≥n:

Docker ya estaba instalado en la √∫ltima versi√≥n disponible.

Habilitar el servicio Docker para que arranque autom√°ticamente
```
sudo systemctl enable docker
```
Resultado:

```
docker
```
Iniciar el servicio Docker

```
sudo systemctl start docker
```
Verificar versi√≥n de Docker instalada

```
docker --version
```
Resultado:

```
Docker version 26.1.3, build 26.1.3-0ubuntu1~20.04.1
```
Agregar el usuario azureuser al grupo docker

```
sudo usermod -aG docker azureuser
```
Esto permite ejecutar comandos Docker sin sudo.

Cerrar la sesi√≥n SSH
```
exit
```
Es necesario reconectarse para que la pertenencia al grupo docker se aplique.

Resultado
‚úÖ Docker instalado, habilitado y configurado correctamente en la VM.
El usuario azureuser ya tiene permisos para usar Docker sin privilegios de administrador en la pr√≥xima sesi√≥n.

### Registro de tarea ‚Äî Prueba de funcionamiento de Docker con hello-world
### Objetivo

Verificar que Docker est√° instalado y operativo en la VM vm-cftec-m62025-SINT646-lab01, permitiendo la ejecuci√≥n de contenedores y la descarga de im√°genes desde Docker Hub.

### Pasos ejecutados
Conexi√≥n a la VM

```
ssh -i ~/.ssh/id_rsa azureuser@4.155.211.247
```
Ejecuci√≥n del contenedor de prueba

```
docker run hello-world
```
Acci√≥n:

- El cliente Docker (docker) envi√≥ la orden al demonio (dockerd).

- Como la imagen hello-world no estaba disponible localmente, Docker la descarg√≥ desde Docker Hub.

- Se cre√≥ un contenedor temporal que ejecut√≥ un script de verificaci√≥n.

- El mensaje de bienvenida confirm√≥ que todo est√° funcionando.

#### Salida obtenida:

```
Unable to find image 'hello-world:latest' locally
latest: Pulling from library/hello-world
e6590344b1a5: Pull complete
Digest: sha256:ec153840d1e635ac434fab5e377081f17e0e15afab27beb3f726c3265039cfff
Status: Downloaded newer image for hello-world:latest

Hello from Docker!
This message shows that your installation appears to be working correctly.
...
```
Interpretaci√≥n:

- Docker pudo comunicarse con el demonio y descargar im√°genes desde Internet.

- El contenedor se ejecut√≥ exitosamente.

- El usuario azureuser tiene permisos correctos para ejecutar Docker sin sudo.

Resultado
‚úÖ Docker operativo y listo para ejecutar contenedores para los servicios requeridos por el laboratorio (HBase, MongoDB, Redis).

### Registro de tarea ‚Äî Implementaci√≥n de MongoDB en contenedor Docker
#### Objetivo
Desplegar MongoDB en la VM vm-cftec-m62025-SINT646-lab01 como contenedor Docker para uso en el laboratorio.

#### Pasos ejecutados
Conexi√≥n a la VM

```
ssh -i ~/.ssh/id_rsa azureuser@4.155.211.247
```
Descarga de la imagen oficial de MongoDB
```
docker pull mongo:latest
```
Resultado:

```
latest: Pulling from library/mongo
32f112e3802c: Pull complete
...
Status: Downloaded newer image for mongo:latest
docker.io/library/mongo:latest
```
Creaci√≥n de volumen persistente para datos

```
docker volume create mongo_data
```
Resultado:

```
mongo_data
```
Ejecuci√≥n del contenedor MongoDB

```
docker run -d \
  --name mongodb \
  -p 27017:27017 \
  -v mongo_data:/data/db \
  -e MONGO_INITDB_ROOT_USERNAME=admin \
  -e MONGO_INITDB_ROOT_PASSWORD=admin123 \
  mongo:latest
```
Resultado:

```
030a105e7146f4a6d71207787b5a3488472c5478575258466b90c1d33db67e70
```
Verificaci√≥n de contenedor activo

```
docker ps
```
Resultado:

```
CONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS          PORTS                                           NAMES
030a105e7146   mongo:latest   "docker-entrypoint.s‚Ä¶"   23 seconds ago   Up 22 seconds   0.0.0.0:27017->27017/tcp, :::27017->27017/tcp   mongodb
```
Acceso a la consola de MongoDB

```
docker exec -it mongodb mongosh -u admin -p admin123
```
Resultado:

```
Connecting to: mongodb://<credentials>@127.0.0.1:27017
Using MongoDB: 8.0.12
Using Mongosh: 2.5.6
test>
```

Confirmaci√≥n de conexi√≥n local exitosa.

Resultado
‚úÖ MongoDB desplegado correctamente como contenedor en la VM.
Funciona en el puerto 27017 y es accesible localmente desde la propia VM.
Datos persistentes en el volumen Docker mongo_data.

### Registro de tarea ‚Äî Implementaci√≥n de Redis en contenedor Docker
#### Objetivo
Desplegar Redis en la VM vm-cftec-m62025-SINT646-lab01 como contenedor Docker con persistencia de datos.

#### Pasos ejecutados
Conexi√≥n a la VM

```
ssh -i ~/.ssh/id_rsa azureuser@4.155.211.247
```
Descarga de la imagen oficial de Redis

```
docker pull redis:latest
```
Resultado:

```
latest: Pulling from library/redis
59e22667830b: Pull complete
...
Status: Downloaded newer image for redis:latest
docker.io/library/redis:latest
```
Creaci√≥n de volumen persistente

```
docker volume create redis_data
```
Resultado:

```
redis_data
```
Ejecuci√≥n del contenedor Redis

```
docker run -d \
  --name redis \
  -p 6379:6379 \
  -v redis_data:/data \
  redis:latest
```
Resultado:

```
a7c3bd235592357277ed0396e550bc2fa13f86d7325fe5132900a53d221c2453
```
Verificaci√≥n de contenedor activo

```
docker ps
```
Resultado:

```
CONTAINER ID   IMAGE          COMMAND                  CREATED         STATUS         PORTS                                           NAMES
a7c3bd235592   redis:latest   "docker-entrypoint.s‚Ä¶"   7 seconds ago   Up 6 seconds   0.0.0.0:6379->6379/tcp, :::6379->6379/tcp       redis
030a105e7146   mongo:latest   "docker-entrypoint.s‚Ä¶"   2 minutes ago   Up 2 minutes   0.0.0.0:27017->27017/tcp, :::27017->27017/tcp   mongodb
```
Conexi√≥n al cliente Redis

docker exec -it redis redis-cli
Resultado:

```
127.0.0.1:6379>
```
Prueba de conexi√≥n
Dentro del cliente Redis:
```
ping
```
Resultado esperado:

```
PONG
```
Para salir:

```
exit
```
Resultado
‚úÖ Redis desplegado y funcionando en el puerto 6379, con persistencia de datos en el volumen redis_data.
Disponible para uso en el laboratorio junto con MongoDB.


### Registro de tarea ‚Äî Implementaci√≥n de HBase en contenedor Docker
#### Objetivo
Desplegar HBase en la VM vm-cftec-m62025-SINT646-lab01 como contenedor Docker en modo standalone para uso en el laboratorio.

#### Pasos ejecutados
Conexi√≥n a la VM

```
ssh -i ~/.ssh/id_rsa azureuser@4.155.211.247
```
Descarga de la imagen de HBase

```
docker pull harisekhon/hbase
```
Resultado:

```
Using default tag: latest
latest: Pulling from harisekhon/hbase
...
Status: Downloaded newer image for harisekhon/hbase:latest
docker.io/harisekhon/hbase:latest
```
Creaci√≥n de volumen persistente
```
docker volume create hbase_data
```
Resultado:
```
hbase_data
```
Ejecuci√≥n del contenedor HBase
```
docker run -d \
  --name hbase \
  -p 16000:16000 \
  -p 16010:16010 \
  -p 16020:16020 \
  -p 16030:16030 \
  -p 2181:2181 \
  -v hbase_data:/hbase-data \
  harisekhon/hbase
```
Resultado:
```
c218beae29577eba6ed9fbf29beef520fdbedbdbed2895ec4d6c4046d68ea4f8
```
Verificaci√≥n de contenedor activo

```
docker ps
```
Resultado:
```
CONTAINER ID   IMAGE              COMMAND                  CREATED          STATUS          PORTS                                                                                      NAMES
c218beae2957   harisekhon/hbase   "/entrypoint.sh"         8 seconds ago    Up 7 seconds    0.0.0.0:2181->2181/tcp, 0.0.0.0:16000->16000/tcp, 0.0.0.0:16010->16010/tcp, 0.0.0.0:16020->16020/tcp, 0.0.0.0:16030->16030/tcp   hbase
```
Revisi√≥n de logs
```
docker logs hbase --tail 20
```
Resultado:

- Se muestran mensajes de inicializaci√≥n de HMaster y HRegionServer.

- Zookeeper inici√≥ correctamente en el puerto 2181.

- HBase est√° listo para aceptar conexiones.

- Puertos expuestos:
  
  - 16000 ‚Üí HMaster
  - 16010 ‚Üí Web UI de HMaster (http://4.155.211.247:16010)
  - 16020 ‚Üí HRegionServer
  - 16030 ‚Üí Web UI de HRegionServer
  - 2181 ‚Üí Zookeeper

Resultado
‚úÖ HBase desplegado y operativo en la VM, con acceso local y remoto a sus puertos y persistencia de datos en el volumen hbase_data.

### Registro de tarea ‚Äî Implementaci√≥n de MongoDB y Mongo Express en contenedores Docker

#### Objetivo
Desplegar MongoDB y Mongo Express en la VM del laboratorio, utilizando contenedores Docker, para disponer de una base de datos NoSQL y su interfaz web de administraci√≥n, accesible desde la red p√∫blica del laboratorio.

#### Actividades realizadas
Verificaci√≥n de Docker en la VM

- Confirmado que Docker est√° instalado y operativo (docker --version).

- Verificado que el usuario azureuser pertenece al grupo docker para ejecutar sin sudo.

- Despliegue de MongoDB

Creado volumen persistente:

```
docker volume create mongo_data
```
Contenedor MongoDB:

```
docker run -d \
  --name mongodb \
  -p 27017:27017 \
  -v mongo_data:/data/db \
  -e MONGO_INITDB_ROOT_USERNAME=admin \
  -e MONGO_INITDB_ROOT_PASSWORD=admin123 \
  mongo:latest
```
Verificado acceso local con:
```
docker exec -it mongodb mongosh -u admin -p admin123 --authenticationDatabase admin
```
Despliegue de Mongo Express

- Inicialmente intentado con --link mongodb:mongo, pero fallaba la resoluci√≥n de hostname en Docker moderno.

- Confirmado que Mongo Express se ejecuta correctamente y expone el puerto 8081.

- Se mantuvo la autenticaci√≥n web por defecto de laboratorio (admin / pass).

Comando final utilizado:

```
docker run -d \
  --name mongo-express \
  -p 8081:8081 \
  -e ME_CONFIG_MONGODB_ADMINUSERNAME=admin \
  -e ME_CONFIG_MONGODB_ADMINPASSWORD=admin123 \
  -e ME_CONFIG_MONGODB_SERVER=mongodb \
  mongo-express:latest
```
Apertura de puerto en NSG

Modificado network.tf para agregar regla NSG que permita acceso HTTP en puerto 8081 solo desde mi IP p√∫blica.

- Aplicado cambio con terraform apply.

- Verificaci√≥n de acceso

Acceso exitoso a la interfaz web de Mongo Express desde:

```
http://4.155.211.247:8081
```
Autenticaci√≥n web:
```
Usuario: admin
Contrase√±a: pass
```
Confirmado listado de bases de datos admin, config y local.

![Mongo Express](img/mongo01.png)

Resultados

- MongoDB operativo y accesible en la VM del laboratorio.

- Mongo Express funcional como herramienta gr√°fica para administraci√≥n de MongoDB.

- Acceso restringido mediante NSG para mejorar seguridad en el laboratorio.


### Registro de tarea ‚Äî Implementaci√≥n de RedisInsight en contenedor Docker
#### Objetivo
Implementar la interfaz gr√°fica RedisInsight en un contenedor Docker dentro de la VM vm-cftec-m62025-SINT646-lab01, para administrar y monitorear la base de datos Redis desplegada previamente.

#### Procedimiento
1. Preparaci√≥n
- Verificar que Redis est√° corriendo en el contenedor redis en el puerto 6379.

- Confirmar que el puerto 8001 est√° abierto en el NSG para la IP del cliente:

```
terraform -chdir=terraform state show azurerm_network_security_group.lab_nsg | grep -A5 8001
```
2. Primer intento con imagen latest
```
docker run -d \
  --name redisinsight \
  -p 8001:8001 \
  --restart unless-stopped \
  redislabs/redisinsight:latest
```
Problema encontrado:

- El contenedor quedaba detenido en:

```
Running docker-entry.sh
```
El puerto 8001 aparec√≠a abierto, pero al acceder devolv√≠a:
```
ERR_CONNECTION_REFUSED
```
curl http://localhost:8001 devolv√≠a Connection reset by peer.

Logs sin informaci√≥n adicional despu√©s del arranque.

3. Diagn√≥stico

- Confirmado que no era un problema de firewall/NSG (regla ya habilitada).

- Revisado que el contenedor corr√≠a (docker ps) pero sin inicializar la UI.

Conclusi√≥n: bug en la imagen latest.

4. Soluci√≥n aplicada
Ejecutar RedisInsight con versi√≥n estable 1.14.0:

```
docker run -d \
  --name redisinsight \
  -p 8001:8001 \
  --restart unless-stopped \
  redislabs/redisinsight:1.14.0
```
Esta versi√≥n inicializ√≥ correctamente y mostr√≥ la UI.

5. Validaci√≥n de acceso
Acceso exitoso desde el navegador:

```
http://4.155.211.247:8001
```
Pantalla inicial solicitando conectar una base de datos Redis.

![Redis](img/redis01.png)

Verificaci√≥n de logs:

```
docker logs redisinsight
```
Mostr√≥ inicio correcto del servicio.

Resultado

-RedisInsight desplegado correctamente.

Interfaz disponible en http://4.155.211.247:8001.

Lista para configurar conexi√≥n al contenedor redis local.

Notas

- Mantener el puerto 8001 abierto solo para IP autorizada por seguridad.

- RedisInsight 1.x est√° en End of Life, pero se mantiene en este laboratorio por simplicidad y estabilidad.

### Registro de tarea ‚Äî Implementaci√≥n de HBase en contenedor Docker
#### Objetivo
Implementar Apache HBase en contenedor Docker con sus puertos de administraci√≥n accesibles v√≠a web, permitiendo monitorear el estado de Master y RegionServer.

#### Procedimiento
1. Preparaci√≥n
Verificar que Docker est√© instalado y corriendo.

Abrir puertos 16010 (Master UI) y 16030 (RegionServer UI) en el NSG para la IP autorizada 190.108.74.42.

2. Implementaci√≥n
```
docker pull harisekhon/hbase:latest
docker volume create hbase_data

docker run -d \
  --name hbase \
  -p 16000:16000 \
  -p 16010:16010 \
  -p 16020:16020 \
  -p 16030:16030 \
  -p 2181:2181 \
  -v hbase_data:/hbase-data \
  harisekhon/hbase
```
3. Validaci√≥n
Master UI: http://4.155.211.247:16010

RegionServer UI: http://4.155.211.247:16030

![Hbase](img/hbase01.png)

Logs:
```
docker logs hbase --tail 20
```
Mostraron inicio exitoso y disponibilidad de servicios.

Resultado
- HBase funcionando y accesible v√≠a web.

- Puertos seguros, expuestos solo a IP autorizada.

- Preparado para pruebas de integraci√≥n con otras BD del laboratorio.

### Registro de tarea ‚Äî Implementaci√≥n de Mongo Express en contenedor Docker
### Objetivo
Implementar Mongo Express como interfaz web para administrar la base de datos MongoDB ya desplegada en contenedor Docker.

#### Procedimiento
1. Preparaci√≥n
- Confirmar que MongoDB est√° en ejecuci√≥n (docker ps).

- Abrir puerto 8081 en el NSG para la IP 190.108.74.42.

2. Implementaci√≥n
```
docker run -d \
  --name mongo-express \
  -p 8081:8081 \
  -e ME_CONFIG_MONGODB_ADMINUSERNAME=admin \
  -e ME_CONFIG_MONGODB_ADMINPASSWORD=admin123 \
  -e ME_CONFIG_MONGODB_SERVER=mongodb \
  --link mongodb:mongodb \
  mongo-express:latest
```
Nota: En laboratorio se us√≥ admin / pass para simplificar autenticaci√≥n.

3. Validaci√≥n
Acceso web: http://4.155.211.247:8081

Visualizaci√≥n de bases de datos: admin, config, local.

Logs:
```
docker logs mongo-express --tail 20
```
Mostraron conexi√≥n establecida a MongoDB.

Resultado
- Interfaz web funcional para gesti√≥n de MongoDB.

- Puertos y acceso restringidos a IP autorizada.

- Configuraci√≥n v√°lida para fines de laboratorio.

### Registro de tarea ‚Äî Implementaci√≥n de RedisInsight en contenedor Docker
#### Objetivo
Implementar RedisInsight para administraci√≥n gr√°fica de Redis en contenedor Docker, facilitando visualizaci√≥n y configuraci√≥n.

#### Procedimiento
1. Preparaci√≥n
- Confirmar Redis en ejecuci√≥n (docker ps).

- Abrir puerto 8001 en el NSG para la IP 190.108.74.42.

2. Problemas detectados
Imagen latest de RedisInsight no inicializaba correctamente.

Logs mostraban:

```
Running docker-entry.sh
```
y no continuaba.

curl http://localhost:8001 devolv√≠a Connection reset by peer.

3. Soluci√≥n aplicada
Usar versi√≥n estable 1.14.0:

```
docker run -d \
  --name redisinsight \
  -p 8001:8001 \
  --restart unless-stopped \
  redislabs/redisinsight:1.14.0
```
4. Validaci√≥n
Acceso exitoso: http://4.155.211.247:8001

- Pantalla inicial solicitando conexi√≥n a Redis existente.

- Logs mostraron inicio correcto.

Resultado
- RedisInsight desplegado y accesible v√≠a web.

- Configuraci√≥n v√°lida para entornos de laboratorio.

- Puerto expuesto solo a IP autorizada.

### Registro de tarea ‚Äî Configuraci√≥n de Jupyter Notebook en VM para Laboratorio 1
#### Objetivo
Preparar un entorno Jupyter Notebook accesible v√≠a navegador, protegido por contrase√±a, para desarrollar el Laboratorio 1 de la asignatura SINT646 ‚Äî Deep Learning y Big Data con Python. El notebook servir√° para trabajar con MongoDB, Redis y HBase en contenedores Docker y realizar las pruebas solicitadas.

#### Acciones ejecutadas
- Instalaci√≥n de dependencias base

- Actualizaci√≥n de paquetes en la VM.

- Instalaci√≥n de python3, python3-pip y compiladores necesarios para librer√≠as Python.

- Instalaci√≥n de librer√≠as requeridas para el laboratorio:

```
pip3 install pymongo redis happybase thriftpy2 pandas
```
- Instalaci√≥n de Jupyter Notebook

- Instalaci√≥n de notebook<7 para evitar problemas de compatibilidad con Python 3.8.

- Ajuste de versiones de jinja2 y markupsafe para corregir error de importaci√≥n.

- Instalaci√≥n en el entorno de usuario (--user) para evitar conflictos con paquetes del sistema.

- Configuraci√≥n de Jupyter Notebook

Generaci√≥n de archivo de configuraci√≥n:

```
jupyter notebook --generate-config
```
Edici√≥n del archivo ~/.jupyter/jupyter_notebook_config.py para:

- Escuchar en todas las interfaces (0.0.0.0).

- Definir puerto fijo 8888.

- Deshabilitar token y usar contrase√±a hash.

- Deshabilitar apertura autom√°tica de navegador en el servidor.

Generaci√≥n de hash para contrase√±a simple pass:
```
from notebook.auth import passwd
passwd()
```
Hash aplicado:
```
argon2:$argon2id$v=19$m=10240,t=10,p=8$BcymMp8qCSRjbB29A7lACQ$xDsP/i38TtfPfRh6raE2z1QSRpDN7ZsiDKAzFyDc5Ik
```
Ejecuci√≥n persistente con tmux

- Instalaci√≥n y verificaci√≥n de tmux.

Creaci√≥n de sesi√≥n persistente:

```
tmux new -s jupyterlab
```
Ejecuci√≥n de Jupyter dentro de tmux:
```
jupyter notebook
```
Desacople de sesi√≥n (Ctrl+B luego D) para mantener el servicio activo tras cerrar SSH.

- Acceso desde el navegador

Acceso v√≠a:


http://4.155.211.247:8888
Login con contrase√±a simple: pass.

Confirmaci√≥n de funcionamiento correcto y acceso a interfaz vac√≠a lista para cargar notebooks.

![Jupiter Notebook](img/jupiter02.png)
![Jupiter Notebook](img/jupiter01.png)

#### Problemas enfrentados y soluciones
Error de importaci√≥n soft_unicode en MarkupSafe
üîπ Soluci√≥n: fijar versi√≥n compatible de jinja2==3.0.3 y markupsafe==2.0.1.

Versi√≥n de Notebook 7 no compatible con Python 3.8
üîπ Soluci√≥n: instalar notebook<7.

P√©rdida del token al ejecutar en SSH
üîπ Soluci√≥n: configuraci√≥n sin token y con contrase√±a hash.

Riesgo de detener Jupyter al cerrar SSH
üîπ Soluci√≥n: uso de tmux para mantener la sesi√≥n persistente.


---
### Las URLs de acceso quedar√≠an as√≠:

| Servicio |	Puerto	|URL de acceso |
| :------ | :------ | :------ |
|Mongo Express |	8081|	http://4.155.211.247:8081|
|RedisInsight	| 8001	| http://4.155.211.247:8001|
|HBase Master UI |	16010	| http://4.155.211.247:16010 |
| HBase RegionServer UI	| 16030	| http://4.155.211.247:16030 |
| Jupyter Notebook |	8888	| http://4.155.211.247:8888 |

### Credenciales

- Mongo Express ‚Üí Usuario: admin / Clave: pass

- RedisInsight ‚Üí Sin clave inicial (se configura al entrar)

- Jupyter Notebook ‚Üí Clave: pass

---

### Registro de tarea ‚Äì Jupyter Notebook no disponible tras reinicio de VM

#### Resumen
Despu√©s de reiniciar la m√°quina virtual, el servicio de Jupyter Notebook dej√≥ de responder en el puerto 8888 (ERR_CONNECTION_REFUSED).

Esto se debe a que el proceso de Jupyter no se inicia autom√°ticamente al reiniciar la VM y tampoco estaba corriendo en un proceso persistente.

#### Acciones ejecutadas
Verificaci√≥n de conectividad:

- Confirmado que el puerto 8888 est√° permitido en el NSG para la IP del usuario.

- Verificado que no hay reglas de firewall adicionales que bloqueen el acceso.

#### An√°lisis del proceso:

- Validado que no existe ninguna sesi√≥n tmux activa con Jupyter Notebook (tmux attach -t jupyterlab ‚Üí no sessions).

- Determinado que el proceso no sobrevive a reinicios.

**Propuesta de soluci√≥n inmediata:**

- Iniciar manualmente Jupyter en una nueva sesi√≥n tmux:

```
tmux new -s jupyterlab
jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser
```
Luego salir de tmux con Ctrl+B y D.

**Propuesta de soluci√≥n permanente:**

Modificar el script restart_lab_services.sh para que Jupyter Notebook se ejecute autom√°ticamente en un tmux al reiniciar la VM:

```
tmux new -d -s jupyterlab "jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser"
```
Definici√≥n del problema (perspectiva cliente)
Despu√©s de reiniciar la VM, los servicios de laboratorio deben estar disponibles sin intervenci√≥n manual. Actualmente, MongoDB, Redis, HBase y RedisInsight se inician autom√°ticamente, pero Jupyter Notebook requiere ejecuci√≥n manual.


---

### Registro de tarea ‚Äî Limpieza y Preprocesamiento del Dataset
#### Resumen:
Se realiz√≥ la limpieza y preprocesamiento del dataset kz.csv proveniente del conjunto E-commerce Purchase History from Electronics Store. El dataset original conten√≠a 2,633,521 registros y 8 columnas. Se identificaron valores nulos en varias columnas, especialmente en category_id, category_code, brand, price y user_id.

#### Acciones ejecutadas:

- Carga inicial del dataset en un DataFrame de Pandas.

- An√°lisis exploratorio inicial para identificar:

- Tipo de datos por columna.

- N√∫mero total de registros.

- Cantidad de valores nulos por columna.

- Estrategia de preprocesamiento definida:
  - Mantener todas las columnas para consistencia con las otras bases de datos.
  - Eliminar o imputar valores nulos solo si impactan en los c√°lculos solicitados.
  - No realizar transformaciones destructivas sobre price o brand sin an√°lisis posterior.
  - Verificaci√≥n de memoria para asegurar que las operaciones no saturen la VM.

- Guardado del DataFrame limpio para su uso posterior en inserci√≥n en MongoDB, Redis y HBase.


---

### Registro de tarea ‚Äî Limpieza y Preprocesamiento del Dataset (Actualizaci√≥n)
#### Hallazgos tras la verificaci√≥n de valores nulos:

|Columna	| Valores Nulos| 	% del Total aprox.|
| :----- | :----- | :----- |
|event_time	| 0	| 0.00% |
|order_id	| 0	| 0.00% |
|product_id	|0	|0.00% |
|category_id	| 431,954 |	16.40%|
|category_code	| 612,202	| 23.20%|
|brand	|506,005	|19.20% |
|price	|431,954	|16.40%|
|user_id	|2,069,352	|78.60%

**Conclusiones de esta verificaci√≥n:**

- event_time, order_id y product_id est√°n completos.

- user_id presenta una ausencia significativa (~78%), lo que lo hace poco confiable para an√°lisis directos.

- Las columnas de categor√≠a (category_id, category_code), brand y price presentan un porcentaje relevante de nulos.

- No se ha aplicado imputaci√≥n o eliminaci√≥n de registros todav√≠a para conservar la integridad y representatividad del dataset.

**Pr√≥ximo paso inmediato:**

- Mantener el dataset tal cual para inserci√≥n en MongoDB, Redis y HBase, documentando el porcentaje de nulos para que se considere en el an√°lisis de consultas.

- Evaluar m√°s adelante si se imputan o eliminan estos nulos dependiendo de los requisitos de las consultas comparativas.

---
### Registro de Tarea ‚Äî Carga del dataset en MongoDB

#### Actividad: Inserci√≥n del dataset limpio en MongoDB en bloques de 100‚ÄØ000 registros utilizando insert_many() para optimizar el rendimiento.

#### Objetivo: Cargar el dataset completo midiendo tiempos por bloque y consumo de recursos.

#### ‚öôÔ∏è Configuraci√≥n de prueba
Dataset: 2‚ÄØ633‚ÄØ521 registros (kz.csv)

- Bloques de inserci√≥n: 100‚ÄØ000 registros por batch (√∫ltimo bloque de 33‚ÄØ521)

- MongoDB: Contenedor Docker mongo:6.0 con autenticaci√≥n admin / pass

- VM: Standard_A4m_v2 ‚Äî 8 vCPU, 32‚ÄØGB RAM

- Script: Python con pymongo, limpieza previa de la colecci√≥n (drop()) para evitar duplicados

#### üìä Tiempos de inserci√≥n
- Promedio por bloque: ~4.3 segundos
- Tiempo total: 114.76 segundos
- Total documentos insertados: 2‚ÄØ633‚ÄØ521

Ejemplo de ejecuci√≥n:

```
üßπ Colecci√≥n limpiada antes de la inserci√≥n.
‚úÖ Bloque 1: 100000 registros (4.19 seg)
‚úÖ Bloque 2: 100000 registros (4.67 seg)
...
‚úÖ Bloque 26: 100000 registros (4.36 seg)
‚úÖ Bloque 27: 33521 registros (1.56 seg)
‚è± Tiempo total: 114.76 seg
üìä Total documentos insertados: 2633521

```
#### M√©tricas de rendimiento (Azure Monitor)

- Periodo observado: durante la inserci√≥n del dataset:
  
  |M√©trica	| Valor Promedio	| Observaciones |
  | :----- | :----- | :----- |
  | CPU (Percentage CPU)	| ~20‚Äì25‚ÄØ% | (picos >50‚ÄØ%)	Actividad constante durante cada batch. |
  | Memoria disponible (Available Memory %)	| ~80‚Äì82‚ÄØ%	| MongoDB usa memoria para cache/buffers, estable en la prueba. |
  |Data Disk IOPS Consumed %	| Moderado	| Picos coinciden con inserciones en disco.|
  |Data Disk Latency	| Baja	| MongoDB maneja escritura r√°pida con journaling activo. |
  |Data Disk Read/Write Bytes/Sec	| Escrituras constantes	| La escritura crece proporcional al tama√±o del batch insertado.|

  ![MongoDB](img/vm_performance_01_InsertingToMongo.png)

üìù Notas
- MongoDB consume m√°s I/O que Redis en la carga inicial debido a la persistencia inmediata en disco.

- La carga por lotes de 100‚ÄØ000 evita saturaci√≥n y mantiene uso estable de CPU y memoria.

- Es fundamental limpiar la colecci√≥n antes de una nueva inserci√≥n para evitar duplicados (drop()).

---
### Registro de tarea ‚Äî Carga del dataset en Redis
#### Objetivo
Cargar el dataset E-commerce Purchase History en Redis de forma controlada, optimizando el rendimiento y evitando la duplicaci√≥n de datos provenientes de ejecuciones anteriores.

#### Acciones ejecutadas
- Conexi√≥n a Redis usando redis-py (redis.Redis()), con verificaci√≥n de disponibilidad mediante ping().

- Lectura del dataset en pandas.DataFrame desde la ruta ./datasets/ecommerce/kz.csv.

- Definici√≥n de CHUNK_SIZE = 100_000 para realizar inserciones en bloques y reducir el riesgo de saturar la VM.

- Eliminaci√≥n previa de datos antiguos:
  - Identificaci√≥n de claves con patr√≥n purchase:*.
  - Eliminaci√≥n en lotes de hasta 10‚ÄØ000 claves por operaci√≥n para no saturar Redis.

- Inserci√≥n de datos en Redis:
  - Uso de pipeline para agrupar m√∫ltiples operaciones y mejorar el rendimiento.
  - Conversi√≥n de valores NaN a cadenas vac√≠as ("") para evitar incompatibilidades.
  - Almacenamiento de cada registro como un hash en Redis con clave purchase:<√≠ndice>.
  - Bloques de inserci√≥n: 100‚ÄØ000 registros por batch

Medici√≥n de tiempos:
```
Total de registros en dataset: 2,633,521

Tiempo por bloque.
‚úÖ Bloque 1: 100,000 registros en 42.05 seg
‚úÖ Bloque 2: 100,000 registros en 40.58 seg
‚úÖ Bloque 3: 100,000 registros en 40.79 seg
‚úÖ Bloque 4: 100,000 registros en 40.61 seg
‚úÖ Bloque 5: 100,000 registros en 40.09 seg
...
‚úÖ Bloque 24: 100,000 registros en 41.60 seg
‚úÖ Bloque 25: 100,000 registros en 41.37 seg
‚úÖ Bloque 26: 100,000 registros en 42.68 seg
‚úÖ Bloque 27: 33,521 registros en 13.66 seg

Tiempo total de inserci√≥n.
üèÅ Inserci√≥n total completada en 1086.61 segundos
```
**Definici√≥n del problema (desde la perspectiva del laboratorio)**
- La carga de un dataset de m√°s de 2.6 millones de registros en Redis puede provocar:

- Saturaci√≥n de CPU y memoria si se intenta insertar todo en una sola operaci√≥n.

- Duplicaci√≥n de datos si no se eliminan cargas anteriores.

- Latencia en inserci√≥n si no se optimiza la escritura.

#### Resultados
- Conexi√≥n: Redis acept√≥ conexiones desde la VM sin errores.

- Borrado de datos previos: Eliminadas todas las claves antiguas purchase:* antes de la nueva carga.

- Inserci√≥n optimizada: Uso de pipeline y carga en bloques permiti√≥ procesar el dataset sin saturar la VM.

- Datos accesibles: Los registros son consultables con comandos como:

```
redis-cli HGETALL purchase:0
```

---


üìä M√©tricas de rendimiento (Azure Monitor)
Periodo observado: durante toda la inserci√≥n del dataset.

| M√©trica |	Valor Promedio	| Observaciones |
| :----- | :----- | :----- |
| CPU (Percentage CPU)	| ~20‚ÄØ% | (picos 65‚ÄØ%)	Incrementos durante los batches, con ca√≠das entre lotes.|
|Memoria disponible (Available Memory %)	| ~84‚ÄØ%	| Uso moderado; Redis maneja los datos en memoria eficientemente.|
| Data Disk IOPS Consumed %	| Bajo	| No hubo saturaci√≥n de IOPS, Redis es predominantemente in-memory.|
| Data Disk Latency	| Casi nulo	| Escritura muy r√°pida por ser en memoria; m√≠nima espera en disco.|
|Data Disk Read/Write Bytes/Sec	| Lectura m√≠nima / Escritura muy baja	| No hubo dependencia fuerte de disco persistente.|


üìù Notas
- La carga en bloques evita saturar CPU y memoria.
- Redis respondi√≥ r√°pidamente debido a su naturaleza en memoria, con baja latencia.
- Redis no se comporta mejor que MongoDB en t√©rminos de uso de CPU y disco para esta etapa, aunque la persistencia depende de snapshots y AOF si se habilitan.
- En cargas repetidas es clave eliminar previamente claves antiguas para evitar duplicados (DEL purchase:*).
- Redis es significativamente m√°s lento que MongoDB en esta carga debido a la inserci√≥n de hashes individuales para cada registro.
- El uso de pipeline redujo la latencia de red, pero la operaci√≥n sigue siendo CPU-bound y single-threaded en el proceso de escritura.
- Eliminar las claves antiguas antes de la inserci√≥n es esencial para evitar duplicados y consumo excesivo de memoria.
- Si se prioriza la velocidad sobre la persistencia, se podr√≠a desactivar temporalmente el guardado RDB/AOF durante la carga.


---

### Registro de Tarea ‚Äî Carga del Dataset en HBase
#### Objetivo: Insertar el dataset limpio de compras electr√≥nicas en HBase utilizando inserci√≥n por bloques para evitar saturaci√≥n de recursos.

#### Acciones Ejecutadas
Conexi√≥n a HBase mediante happybase (Thrift en puerto 9090).

- Creaci√≥n de la tabla purchases con familia de columnas cf si no exist√≠a.
- Limpieza previa de la tabla (eliminaci√≥n de registros antiguos) para evitar duplicados.
- Carga del dataset kz.csv (2‚ÄØ633‚ÄØ521 registros) usando bloques de 100‚ÄØ000 registros.
- Inserci√≥n optimizada utilizando batch() para reducir overhead de conexi√≥n.

Ejecuci√≥n
```
‚úÖ Conectado a HBase
üÜï Tabla creada: purchases
üßπ Limpiando registros antiguos de la tabla...
üßπ Tabla vac√≠a.

üì¶ Total de registros en dataset: 2,633,521
‚úÖ Bloque 1: 100,000 registros en 50.46 segundos
‚úÖ Bloque 2: 100,000 registros en 45.50 segundos
‚úÖ Bloque 3: 100,000 registros en 45.49 segundos
‚úÖ Bloque 4: 100,000 registros en 46.09 segundos
‚úÖ Bloque 5: 100,000 registros en 46.36 segundos
...
‚úÖ Bloque 24: 100,000 registros en 45.44 segundos
‚úÖ Bloque 25: 100,000 registros en 46.87 segundos
‚úÖ Bloque 26: 100,000 registros en 45.85 segundos
‚úÖ Bloque 27: 33,521 registros en 15.18 segundos

üèÅ Inserci√≥n total completada en 1227.67 segundos
```
#### Observaciones de Rendimiento
- Inserci√≥n estable en la mayor√≠a de bloques (~45‚Äì47 segundos/bloque).
- Bloques iniciales ligeramente m√°s lentos por la creaci√≥n y preparaci√≥n de la tabla.
- Uso de batch() en HappyBase ayud√≥ a mantener la latencia de escritura constante.
- El rendimiento general fue m√°s lento que en MongoDB y Redis, consistente con el dise√±o de HBase orientado a escritura masiva distribuida.

---

### Registro de tarea ‚Äî Ajuste de configuraci√≥n HBase en contenedor Docker y validaci√≥n de lectura desde Jupyter Notebook
#### Objetivo
- Ajustar la configuraci√≥n de HBase en el contenedor hbase para evitar errores de conexi√≥n (Broken pipe) durante consultas masivas desde Python usando happybase.
- Validar que despu√©s del cambio la base de datos es accesible y que la tabla purchases puede ser le√≠da desde el laboratorio en Jupyter Notebook.

#### Pasos ejecutados
1. Identificaci√≥n del contenedor HBase en la VM
Desde la sesi√≥n SSH en la VM vm-cftec-m62025-SINT646-lab01 se listaron los contenedores activos:

```
docker ps --format "table {{.ID}}\t{{.Image}}\t{{.Names}}\t{{.Status}}\t{{.Ports}}"
```
Se identific√≥ el contenedor de HBase:

```
CONTAINER ID   IMAGE                   NAMES
4bbf4771109c   harisekhon/hbase:latest hbase
```
2. Acceso al contenedor HBase
Se abri√≥ una sesi√≥n interactiva en el contenedor:

```
docker exec -it hbase bash
```
3. Localizaci√≥n del archivo de configuraci√≥n
Dentro del contenedor se localiz√≥ hbase-site.xml:
```
find / -name "hbase-site.xml" 2>/dev/null
```
Resultado:

```
/hbase-2.1.3/conf/hbase-site.xml
```
4. Edici√≥n de la configuraci√≥n
Se edit√≥ el archivo con vi:

```
vi /hbase-2.1.3/conf/hbase-site.xml
```
A√±adiendo / modificando los par√°metros para aumentar tiempo de espera y reducir tama√±o de lotes:

```
<property>
    <name>hbase.rpc.timeout</name>
    <value>600000</value>
</property>

<property>
    <name>hbase.client.scanner.timeout.period</name>
    <value>600000</value>
</property>

<property>
    <name>hbase.regionserver.lease.period</name>
    <value>600000</value>
</property>

<property>
    <name>hbase.client.scanner.caching</name>
    <value>500</value>
</property>
```
- Nota: Los valores est√°n en milisegundos (600000 = 10 minutos).
scanner.caching ajustado a 500 para evitar env√≠o de lotes muy grandes.

5. Reinicio del contenedor HBase
Se guardaron los cambios y se reinici√≥ el contenedor:
```
docker restart hbase
```
Confirmando que volvi√≥ a estar activo:
```
docker ps
```
6. Validaci√≥n de conexi√≥n desde Jupyter Notebook
En el laboratorio .ipynb se verific√≥ la conexi√≥n:

```
import happybase
connection = happybase.Connection(host='localhost', port=9090)
connection.open()
print(connection.tables())
```
Salida esperada:
```
[b'purchases']
```
7. Prueba de lectura
Se consultaron las primeras 3 filas:

```
table = connection.table('purchases')
for key, data in table.scan(limit=3):
    print(key, data)
```
Resultado exitoso:
```
b'2294359932054536986' {b'cf:brand': b'samsung', b'cf:price': b'162.01', ...}
b'2294444024058086220' {b'cf:brand': b'huawei', b'cf:price': b'77.52', ...}
b'2294584263154074236' {b'cf:brand': b'karcher', b'cf:price': b'217.57', ...}
```
Resultado
- Ajustes de configuraci√≥n aplicados correctamente en HBase.
- Conexi√≥n establecida con √©xito desde Jupyter Notebook.
- Lectura de la tabla purchases funcionando sin errores.

## Consultas a los motores de bases de datos
## üìä Comparativa de Resultados por Base de Datos

| Consulta                          | Motor     | Resultado                          | Valor / Ventas        | Tiempo (seg)   |
|----------------------------------|-----------|------------------------------------|------------------------|----------------|
| **Categor√≠a m√°s vendida**        | MongoDB   | `nan`                              | 612,202 ventas         | 8.5758         |
|                                  | Redis     | `electronics.smartphone`           | 357,682 ventas         | 1238.2801      |
|                                  | HBase     | `electronics.smartphone`           | 213,002 ventas         | 173.2701       |
| **Marca con m√°s ingresos brutos**| MongoDB   | `samsung`                          | $90,052,821.66         | 9.7873         |
|                                  | Redis     | `samsung`                          | $90,052,821.66         | 2306.0426      |
|                                  | HBase     | `samsung`                          | $54,047,304.62         | 110.5100       |
| **Mes con m√°s ventas (UTC)**     | MongoDB   | `2020-06`                          | 403,632 ventas         | 9.6747         |
|                                  | Redis     | `2020-06`                          | 403,632 ventas         | 1224.5353      |
|                                  | HBase     | `2020-06`                          | 211,552 ventas         | 172.2478       |

![Grafico](img/tiempos_consulta_por_motor.png)

## üìå Conclusiones y An√°lisis de Rendimiento por Motor

### üéØ Observaciones del gr√°fico

- **MongoDB** fue consistentemente el motor con menor tiempo de respuesta en las tres consultas clave, con tiempos inferiores a 10 segundos en todos los casos.
- **Redis**, a pesar de su arquitectura en memoria, present√≥ los tiempos m√°s altos de ejecuci√≥n, particularmente en la consulta de ingresos por marca (m√°s de 2300 segundos).
- **HBase** mostr√≥ una latencia intermedia. Aunque fue m√°s lento que MongoDB, super√≥ a Redis en dos de las tres consultas.

Estas diferencias pueden atribuirse a:

- La optimizaci√≥n nativa de MongoDB para consultas agregadas y filtrado directo sobre documentos JSON.
- Redis no est√° dise√±ado para consultas complejas; su modelo clave-valor requiere estructuras adicionales y procesamiento m√°s intensivo en el cliente.
- HBase, al estar orientado a escritura y procesamiento distribuido, requiere tiempo para escanear y recuperar datos, especialmente en volumen.

---

### üìä Hip√≥tesis por motor de base de datos

#### MongoDB

- **Hip√≥tesis nula (H‚ÇÄ):**  
  *MongoDB no es significativamente m√°s r√°pido que los otros motores en las consultas evaluadas.*

- **Hip√≥tesis alternativa (H‚ÇÅ):**  
  *MongoDB es significativamente m√°s r√°pido que Redis y HBase en todas las consultas evaluadas.*

‚úÖ Los datos observados apoyan rechazar H‚ÇÄ a favor de H‚ÇÅ.

---

#### Redis

- **Hip√≥tesis nula (H‚ÇÄ):**  
  *Redis ofrece tiempos de respuesta comparables a MongoDB y HBase en consultas agregadas.*

- **Hip√≥tesis alternativa (H‚ÇÅ):**  
  *Redis tiene tiempos de respuesta significativamente mayores debido a su estructura y necesidad de recorrer datos manualmente.*

‚úÖ La evidencia sugiere que Redis no est√° optimizado para este tipo de consultas, por lo que H‚ÇÄ se rechaza a favor de H‚ÇÅ.

---

#### HBase

- **Hip√≥tesis nula (H‚ÇÄ):**  
  *HBase tiene un rendimiento inferior en consultas complejas en comparaci√≥n con MongoDB.*

- **Hip√≥tesis alternativa (H‚ÇÅ):**  
  *HBase tiene un rendimiento intermedio, superando a Redis pero no alcanzando a MongoDB en velocidad de respuesta.*

‚úÖ Se acepta H‚ÇÅ con base en los tiempos observados.

---

### üß† Reflexi√≥n final

La elecci√≥n del motor debe estar alineada con el tipo de consultas esperadas. MongoDB es superior para an√°lisis r√°pido de datos semiestructurados; Redis requiere optimizaciones adicionales para escalar consultas complejas; HBase resulta √∫til cuando se espera carga de escritura masiva, pero sus lecturas deben ser cuidadosamente dise√±adas.

