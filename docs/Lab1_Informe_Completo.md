# Arquitectura de C√≥mputo ‚Äî Laboratorio #1

## Descripci√≥n General

El objetivo principal del Laboratorio #1 es analizar el rendimiento de tres bases de datos NoSQL (HBase, MongoDB y Redis) al procesar y consultar un dataset de gran volumen (2 millones de registros), todo ello montado sobre una infraestructura reproducible y automatizada con herramientas modernas de despliegue.

---

## Componentes Principales

### 1. M√°quina Virtual (Azure VM)

| Recurso          | Detalle                                                                 |
|------------------|-------------------------------------------------------------------------|
| Tipo             | Azure Virtual Machine                                                   |
| Sistema Operativo| Ubuntu 22.04 LTS                                                        |
| Recursos         | 2 vCPU, 8 GB RAM *(estimado para pruebas de laboratorio)*               |
| Despliegue       | Automatizado v√≠a Terraform                                              |
| Scripts de gesti√≥n | `restart_lab_services.sh` (reinicio de servicios principales)         |
| Servicios instalados | Docker, Jupyter Notebook, Python, librer√≠as espec√≠ficas              |

---

### 2. Servicios de Base de Datos (Docker Containers)

Cada base de datos se ejecuta dentro de su propio contenedor Docker en la misma m√°quina virtual.

#### HBase

- Imagen utilizada: `harisekhon/hbase`
- Exposici√≥n: Puerto 16010 (UI), 2181 (Zookeeper), 8080 (API HBase REST)
- Acceso desde Python: librer√≠a `happybase` o `hbase-python`
- Configuraci√≥n: volumen persistente local

#### MongoDB

- Imagen utilizada: `mongo`
- Puerto: 27017
- Acceso desde Python: `pymongo`
- Configuraci√≥n: usuario/clave opcional, persistencia con volumen

#### Redis

- Imagen utilizada: `redis`
- Puerto: 6379
- Acceso desde Python: `redis-py`
- Configuraci√≥n: sin autenticaci√≥n (modo laboratorio)

---

### 3. Carga y Consulta de Datos (Python)

| Componente      | Descripci√≥n                                                                 |
|-----------------|-----------------------------------------------------------------------------|
| Dataset         | Datos simulados de una tienda de electr√≥nicos (2 millones de registros)     |
| Script de carga | Insertar los datos en HBase, MongoDB y Redis                                |
| Script de consulta | Consultas espec√≠ficas: categor√≠a m√°s vendida, marca con m√°s ingresos, mes con m√°s ventas |
| Plataforma      | Jupyter Notebooks (`Lab1_Databases_ElectronicsStore.ipynb`)                 |
| Librer√≠as clave | `pandas`, `datetime`, `pymongo`, `redis`, `happybase`, `matplotlib`         |

---

### 4. Automatizaci√≥n y DevOps

#### Terraform

- Ubicaci√≥n: `terraform/`
- Proveedores: `azurerm`
- Backend: local (`terraform.tfstate`)
- Recursos definidos:
  - Grupo de recursos (`azurerm_resource_group`)
  - Red virtual y subred (`azurerm_virtual_network`, `azurerm_subnet`)
  - Interfaz de red (`azurerm_network_interface`)
  - M√°quina virtual Linux (`azurerm_linux_virtual_machine`)

#### GitHub Actions

- Ubicaci√≥n: `.github/workflows/`
- Flujos:
  - `terraform.yml`: despliegue de la infraestructura
  - `restart-lab-services.yml`: reinicio de contenedores en la VM
  - `Manage_Lab_VM.yml`: control manual de encendido/apagado de la VM

---

## Esquema Visual (Resumen)

```
                     +---------------------------+
                     |     Azure Virtual Machine |
                     |---------------------------|
                     | OS: Ubuntu 22.04          |
                     | Docker + Python + Jupyter |
                     +-----------+---------------+
                                 |
              +----------------+-------------------+
              |                |                   |
       +------+-----+   +------+-----+      +------+-----+
       |  MongoDB   |   |   HBase    |      |   Redis     |
       | (pymongo)  |   | (happybase)|      | (redis-py)  |
       +------------+   +------------+      +------------+
              |                |                   |
        Consultas         Consultas           Consultas
        Dataset            Dataset             Dataset
```

---

## Detalle de Infraestructura como C√≥digo (IaC) con Terraform

### Proveedor y Backend

```hcl
provider "azurerm" {
  features {}
}

terraform {
  backend "local" {
    path = "terraform.tfstate"
  }
}
```

### Grupo de recursos

```hcl
resource "azurerm_resource_group" "lab" {
  name     = var.resource_group_name
  location = var.location
}
```

### Red

```hcl
resource "azurerm_virtual_network" "lab" {
  name                = var.vnet_name
  address_space       = [var.vnet_address_space]
  location            = azurerm_resource_group.lab.location
  resource_group_name = azurerm_resource_group.lab.name
}

resource "azurerm_subnet" "lab" {
  name                 = var.subnet_name
  resource_group_name  = azurerm_resource_group.lab.name
  virtual_network_name = azurerm_virtual_network.lab.name
  address_prefixes     = [var.subnet_prefix]
}
```

### M√°quina Virtual

```hcl
resource "azurerm_linux_virtual_machine" "lab" {
  name                  = var.vm_name
  location              = azurerm_resource_group.lab.location
  resource_group_name   = azurerm_resource_group.lab.name
  network_interface_ids = [azurerm_network_interface.lab.id]
  size                  = var.vm_size

  admin_username = var.admin_username

  admin_ssh_key {
    username   = var.admin_username
    public_key = file(var.ssh_public_key_path)
  }

  os_disk {
    caching              = "ReadWrite"
    storage_account_type = "Standard_LRS"
  }

  source_image_reference {
    publisher = "Canonical"
    offer     = "UbuntuServer"
    sku       = "22_04-lts"
    version   = "latest"
  }

  disable_password_authentication = true
}
```

---

## Repositorio del Proyecto

Este laboratorio est√° documentado y versionado en el siguiente repositorio de GitHub:

üîó [https://github.com/mbrenes26/cftec-m6_2025-sint646-dlbdpy](https://github.com/mbrenes26/cftec-m6_2025-sint646-dlbdpy)

---

## Conclusi√≥n

La arquitectura del laboratorio busca balancear simplicidad, aislamiento y reproducibilidad. Al contener los servicios en una sola VM con contenedores separados, se facilita el manejo de recursos, reinicio controlado de servicios, y an√°lisis comparativo justo, ya que todas las bases de datos se ejecutan bajo condiciones similares.

---

# Registro de Tareas y Bit√°cora T√©cnica


Registro de tarea exitosa ‚Äî Conexi√≥n SSH a la VM en Azure
Objetivo
Conectarse por SSH a la m√°quina virtual vm-cftec-m62025-SINT646-lab01 creada con Terraform, validando:

Configuraci√≥n correcta de NSG (puerto 22 abierto).

Clave p√∫blica SSH correctamente configurada.

VM operativa y accesible desde internet.

Pasos ejecutados
Confirmaci√≥n de IP p√∫blica

Desde Azure Portal, en el recurso pip-cftec-m62025-SINT646-lab01, se verific√≥ que la IP p√∫blica asignada es:

Copy
Edit
4.155.211.247
Verificaci√≥n de reglas de seguridad

Se comprob√≥ que el Network Security Group (NSG) asociado a la Subnet incluye la regla:

makefile
Copy
Edit
Name: Allow-SSH
Direction: Inbound
Protocol: TCP
Port: 22
Action: Allow
Source: Any
Esto asegura que el puerto 22 est√° abierto para conexiones SSH desde cualquier origen (configuraci√≥n de laboratorio).

Ejecuci√≥n del comando SSH

Desde la m√°quina local (Windows con Git Bash), se ejecut√≥:

bash
Copy
Edit
ssh -i ~/.ssh/id_rsa azureuser@4.155.211.247
Donde:

-i ~/.ssh/id_rsa ‚Üí ruta de la clave privada correspondiente a la clave p√∫blica configurada en Terraform.

azureuser ‚Üí usuario administrador definido en la VM.

4.155.211.247 ‚Üí IP p√∫blica de la VM.

Aceptaci√≥n de huella digital

Como era la primera conexi√≥n, el sistema mostr√≥:

nginx
Copy
Edit
The authenticity of host '4.155.211.247' can't be established...
Se respondi√≥:

bash
Copy
Edit
yes
El sistema agreg√≥ la huella a la lista de known hosts.

Ingreso de passphrase de la clave privada

Al haberse configurado la clave privada con passphrase, el cliente SSH solicit√≥:

swift
Copy
Edit
Enter passphrase for key '/c/Users/mario.brenes/.ssh/id_rsa':
Se ingres√≥ la passphrase correcta.

Acceso exitoso a la VM

La sesi√≥n SSH se estableci√≥ mostrando:

css
Copy
Edit
Welcome to Ubuntu 20.04.6 LTS (GNU/Linux 5.15.0-1089-azure x86_64)
Informaci√≥n del sistema:

OS: Ubuntu 20.04.6 LTS

IP interna: 10.0.1.4

Usuario: azureuser

Estado: sin actualizaciones cr√≠ticas pendientes (0 updates inmediatas).

Aviso de reinicio requerido por cambios en el sistema:

pgsql
Copy
Edit
*** System restart required ***
Resultado
‚úÖ Conexi√≥n SSH establecida con √©xito.
La VM es accesible remotamente desde internet y est√° lista para configuraci√≥n adicional (instalaci√≥n de Docker y otros servicios).

Registro de tarea ‚Äî Verificaci√≥n y actualizaci√≥n del sistema operativo
Objetivo
Verificar y actualizar el sistema operativo Ubuntu 20.04.6 LTS de la VM vm-cftec-m62025-SINT646-lab01 para garantizar que se encuentra al d√≠a antes de instalar Docker y otros servicios.

Pasos ejecutados
Conexi√≥n a la VM por SSH

Desde la m√°quina local:

bash
Copy
Edit
ssh -i ~/.ssh/id_rsa azureuser@4.155.211.247
Actualizaci√≥n de listas de paquetes

Comando:

bash
Copy
Edit
sudo apt update
Resultado:

perl
Copy
Edit
Hit:1 http://azure.archive.ubuntu.com/ubuntu focal InRelease
Hit:2 http://azure.archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:3 http://azure.archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:4 http://azure.archive.ubuntu.com/ubuntu focal-security InRelease
Reading package lists... Done
Building dependency tree       
Reading state information... Done
All packages are up to date.
Interpretaci√≥n:

Todas las listas de paquetes est√°n actualizadas.

No hay actualizaciones pendientes en los repositorios est√°ndar de Ubuntu.

Actualizaci√≥n de paquetes instalados

Comando:

bash
Copy
Edit
sudo apt upgrade -y
Resultado:

javascript
Copy
Edit
Calculating upgrade... Done
The following security updates require Ubuntu Pro with 'esm-infra' enabled:
  <lista de paquetes ESM>
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
Interpretaci√≥n:

No hay actualizaciones disponibles en los repositorios est√°ndar.

Algunos paquetes con soporte extendido (ESM) requieren Ubuntu Pro para recibir parches de seguridad.

Conclusi√≥n

No fue necesario ejecutar:

bash
Copy
Edit
sudo apt autoremove -y
sudo reboot
El sistema ya estaba en el estado m√°s reciente posible sin habilitar Ubuntu Pro.

Resultado
‚úÖ El sistema est√° actualizado y no presenta paquetes pendientes de actualizaci√≥n en los repositorios est√°ndar de Ubuntu 20.04.

Registro de tarea ‚Äî Instalaci√≥n y configuraci√≥n de Docker en la VM
Objetivo
Instalar y habilitar Docker Engine en la VM vm-cftec-m62025-SINT646-lab01 para permitir la ejecuci√≥n de contenedores necesarios en el laboratorio.

Pasos ejecutados
Conexi√≥n a la VM por SSH

bash
Copy
Edit
ssh -i ~/.ssh/id_rsa azureuser@4.155.211.247
Instalaci√≥n de Docker

Comando:

bash
Copy
Edit
sudo apt install -y docker.io
Resultado:

pgsql
Copy
Edit
docker.io is already the newest version (26.1.3-0ubuntu1~20.04.1).
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
Interpretaci√≥n:

Docker ya estaba instalado en la √∫ltima versi√≥n disponible.

Habilitar el servicio Docker para que arranque autom√°ticamente

bash
Copy
Edit
sudo systemctl enable docker
Resultado:

nginx
Copy
Edit
docker
Iniciar el servicio Docker

bash
Copy
Edit
sudo systemctl start docker
Verificar versi√≥n de Docker instalada

bash
Copy
Edit
docker --version
Resultado:

nginx
Copy
Edit
Docker version 26.1.3, build 26.1.3-0ubuntu1~20.04.1
Agregar el usuario azureuser al grupo docker

bash
Copy
Edit
sudo usermod -aG docker azureuser
Esto permite ejecutar comandos Docker sin sudo.

Cerrar la sesi√≥n SSH

bash
Copy
Edit
exit
Es necesario reconectarse para que la pertenencia al grupo docker se aplique.

Resultado
‚úÖ Docker instalado, habilitado y configurado correctamente en la VM.
El usuario azureuser ya tiene permisos para usar Docker sin privilegios de administrador en la pr√≥xima sesi√≥n.

Registro de tarea ‚Äî Prueba de funcionamiento de Docker con hello-world
Objetivo
Verificar que Docker est√° instalado y operativo en la VM vm-cftec-m62025-SINT646-lab01, permitiendo la ejecuci√≥n de contenedores y la descarga de im√°genes desde Docker Hub.

Pasos ejecutados
Conexi√≥n a la VM

bash
Copy
Edit
ssh -i ~/.ssh/id_rsa azureuser@4.155.211.247
Ejecuci√≥n del contenedor de prueba

bash
Copy
Edit
docker run hello-world
Acci√≥n:

El cliente Docker (docker) envi√≥ la orden al demonio (dockerd).

Como la imagen hello-world no estaba disponible localmente, Docker la descarg√≥ desde Docker Hub.

Se cre√≥ un contenedor temporal que ejecut√≥ un script de verificaci√≥n.

El mensaje de bienvenida confirm√≥ que todo est√° funcionando.

Salida obtenida:

vbnet
Copy
Edit
Unable to find image 'hello-world:latest' locally
latest: Pulling from library/hello-world
e6590344b1a5: Pull complete
Digest: sha256:ec153840d1e635ac434fab5e377081f17e0e15afab27beb3f726c3265039cfff
Status: Downloaded newer image for hello-world:latest

Hello from Docker!
This message shows that your installation appears to be working correctly.
...
Interpretaci√≥n:

Docker pudo comunicarse con el demonio y descargar im√°genes desde Internet.

El contenedor se ejecut√≥ exitosamente.

El usuario azureuser tiene permisos correctos para ejecutar Docker sin sudo.

Resultado
‚úÖ Docker operativo y listo para ejecutar contenedores para los servicios requeridos por el laboratorio (HBase, MongoDB, Redis).

Registro de tarea ‚Äî Implementaci√≥n de MongoDB en contenedor Docker
Objetivo
Desplegar MongoDB en la VM vm-cftec-m62025-SINT646-lab01 como contenedor Docker para uso en el laboratorio.

Pasos ejecutados
Conexi√≥n a la VM

bash
Copy
Edit
ssh -i ~/.ssh/id_rsa azureuser@4.155.211.247
Descarga de la imagen oficial de MongoDB

bash
Copy
Edit
docker pull mongo:latest
Resultado:

makefile
Copy
Edit
latest: Pulling from library/mongo
32f112e3802c: Pull complete
...
Status: Downloaded newer image for mongo:latest
docker.io/library/mongo:latest
Creaci√≥n de volumen persistente para datos

bash
Copy
Edit
docker volume create mongo_data
Resultado:

nginx
Copy
Edit
mongo_data
Ejecuci√≥n del contenedor MongoDB

bash
Copy
Edit
docker run -d \
  --name mongodb \
  -p 27017:27017 \
  -v mongo_data:/data/db \
  -e MONGO_INITDB_ROOT_USERNAME=admin \
  -e MONGO_INITDB_ROOT_PASSWORD=admin123 \
  mongo:latest
Resultado:

Copy
Edit
030a105e7146f4a6d71207787b5a3488472c5478575258466b90c1d33db67e70
Verificaci√≥n de contenedor activo

bash
Copy
Edit
docker ps
Resultado:

bash
Copy
Edit
CONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS          PORTS                                           NAMES
030a105e7146   mongo:latest   "docker-entrypoint.s‚Ä¶"   23 seconds ago   Up 22 seconds   0.0.0.0:27017->27017/tcp, :::27017->27017/tcp   mongodb
Acceso a la consola de MongoDB

bash
Copy
Edit
docker exec -it mongodb mongosh -u admin -p admin123
Resultado:

sql
Copy
Edit
Connecting to: mongodb://<credentials>@127.0.0.1:27017
Using MongoDB: 8.0.12
Using Mongosh: 2.5.6
test>
Confirmaci√≥n de conexi√≥n local exitosa.

Resultado
‚úÖ MongoDB desplegado correctamente como contenedor en la VM.
Funciona en el puerto 27017 y es accesible localmente desde la propia VM.
Datos persistentes en el volumen Docker mongo_data.

Registro de tarea ‚Äî Implementaci√≥n de Redis en contenedor Docker
Objetivo
Desplegar Redis en la VM vm-cftec-m62025-SINT646-lab01 como contenedor Docker con persistencia de datos.

Pasos ejecutados
Conexi√≥n a la VM

bash
Copy
Edit
ssh -i ~/.ssh/id_rsa azureuser@4.155.211.247
Descarga de la imagen oficial de Redis

bash
Copy
Edit
docker pull redis:latest
Resultado:

makefile
Copy
Edit
latest: Pulling from library/redis
59e22667830b: Pull complete
...
Status: Downloaded newer image for redis:latest
docker.io/library/redis:latest
Creaci√≥n de volumen persistente

bash
Copy
Edit
docker volume create redis_data
Resultado:

nginx
Copy
Edit
redis_data
Ejecuci√≥n del contenedor Redis

bash
Copy
Edit
docker run -d \
  --name redis \
  -p 6379:6379 \
  -v redis_data:/data \
  redis:latest
Resultado:

nginx
Copy
Edit
a7c3bd235592357277ed0396e550bc2fa13f86d7325fe5132900a53d221c2453
Verificaci√≥n de contenedor activo

bash
Copy
Edit
docker ps
Resultado:

bash
Copy
Edit
CONTAINER ID   IMAGE          COMMAND                  CREATED         STATUS         PORTS                                           NAMES
a7c3bd235592   redis:latest   "docker-entrypoint.s‚Ä¶"   7 seconds ago   Up 6 seconds   0.0.0.0:6379->6379/tcp, :::6379->6379/tcp       redis
030a105e7146   mongo:latest   "docker-entrypoint.s‚Ä¶"   2 minutes ago   Up 2 minutes   0.0.0.0:27017->27017/tcp, :::27017->27017/tcp   mongodb
Conexi√≥n al cliente Redis

bash
Copy
Edit
docker exec -it redis redis-cli
Resultado:

makefile
Copy
Edit
127.0.0.1:6379>
Prueba de conexi√≥n
Dentro del cliente Redis:

bash
Copy
Edit
ping
Resultado esperado:

nginx
Copy
Edit
PONG
Para salir:

bash
Copy
Edit
exit
Resultado
‚úÖ Redis desplegado y funcionando en el puerto 6379, con persistencia de datos en el volumen redis_data.
Disponible para uso en el laboratorio junto con MongoDB.


Registro de tarea ‚Äî Implementaci√≥n de HBase en contenedor Docker
Objetivo
Desplegar HBase en la VM vm-cftec-m62025-SINT646-lab01 como contenedor Docker en modo standalone para uso en el laboratorio.

Pasos ejecutados
Conexi√≥n a la VM

bash
Copy
Edit
ssh -i ~/.ssh/id_rsa azureuser@4.155.211.247
Descarga de la imagen de HBase

bash
Copy
Edit
docker pull harisekhon/hbase
Resultado:

vbnet
Copy
Edit
Using default tag: latest
latest: Pulling from harisekhon/hbase
...
Status: Downloaded newer image for harisekhon/hbase:latest
docker.io/harisekhon/hbase:latest
Creaci√≥n de volumen persistente

bash
Copy
Edit
docker volume create hbase_data
Resultado:

nginx
Copy
Edit
hbase_data
Ejecuci√≥n del contenedor HBase

bash
Copy
Edit
docker run -d \
  --name hbase \
  -p 16000:16000 \
  -p 16010:16010 \
  -p 16020:16020 \
  -p 16030:16030 \
  -p 2181:2181 \
  -v hbase_data:/hbase-data \
  harisekhon/hbase
Resultado:

nginx
Copy
Edit
c218beae29577eba6ed9fbf29beef520fdbedbdbed2895ec4d6c4046d68ea4f8
Verificaci√≥n de contenedor activo

bash
Copy
Edit
docker ps
Resultado:

bash
Copy
Edit
CONTAINER ID   IMAGE              COMMAND                  CREATED          STATUS          PORTS                                                                                      NAMES
c218beae2957   harisekhon/hbase   "/entrypoint.sh"         8 seconds ago    Up 7 seconds    0.0.0.0:2181->2181/tcp, 0.0.0.0:16000->16000/tcp, 0.0.0.0:16010->16010/tcp, 0.0.0.0:16020->16020/tcp, 0.0.0.0:16030->16030/tcp   hbase
Revisi√≥n de logs

bash
Copy
Edit
docker logs hbase --tail 20
Resultado:

Se muestran mensajes de inicializaci√≥n de HMaster y HRegionServer.

Zookeeper inici√≥ correctamente en el puerto 2181.

HBase est√° listo para aceptar conexiones.

Puertos expuestos
16000 ‚Üí HMaster

16010 ‚Üí Web UI de HMaster (http://4.155.211.247:16010)

16020 ‚Üí HRegionServer

16030 ‚Üí Web UI de HRegionServer

2181 ‚Üí Zookeeper

Resultado
‚úÖ HBase desplegado y operativo en la VM, con acceso local y remoto a sus puertos y persistencia de datos en el volumen hbase_data.

üìÑ Registro de tarea ‚Äî Implementaci√≥n de MongoDB y Mongo Express en contenedores Docker
Fecha: 06/agosto/2025
Responsable: Mario Brenes

Objetivo
Desplegar MongoDB y Mongo Express en la VM del laboratorio, utilizando contenedores Docker, para disponer de una base de datos NoSQL y su interfaz web de administraci√≥n, accesible desde la red p√∫blica del laboratorio.

Actividades realizadas
Verificaci√≥n de Docker en la VM

Confirmado que Docker est√° instalado y operativo (docker --version).

Verificado que el usuario azureuser pertenece al grupo docker para ejecutar sin sudo.

Despliegue de MongoDB

Creado volumen persistente:

bash
Copy
Edit
docker volume create mongo_data
Contenedor MongoDB:

bash
Copy
Edit
docker run -d \
  --name mongodb \
  -p 27017:27017 \
  -v mongo_data:/data/db \
  -e MONGO_INITDB_ROOT_USERNAME=admin \
  -e MONGO_INITDB_ROOT_PASSWORD=admin123 \
  mongo:latest
Verificado acceso local con:

bash
Copy
Edit
docker exec -it mongodb mongosh -u admin -p admin123 --authenticationDatabase admin
Despliegue de Mongo Express

Inicialmente intentado con --link mongodb:mongo, pero fallaba la resoluci√≥n de hostname en Docker moderno.

Confirmado que Mongo Express se ejecuta correctamente y expone el puerto 8081.

Se mantuvo la autenticaci√≥n web por defecto de laboratorio (admin / pass).

Comando final utilizado:

bash
Copy
Edit
docker run -d \
  --name mongo-express \
  -p 8081:8081 \
  -e ME_CONFIG_MONGODB_ADMINUSERNAME=admin \
  -e ME_CONFIG_MONGODB_ADMINPASSWORD=admin123 \
  -e ME_CONFIG_MONGODB_SERVER=mongodb \
  mongo-express:latest
Apertura de puerto en NSG

Modificado network.tf para agregar regla NSG que permita acceso HTTP en puerto 8081 solo desde mi IP p√∫blica.

Aplicado cambio con terraform apply.

Verificaci√≥n de acceso

Acceso exitoso a la interfaz web de Mongo Express desde:

cpp
Copy
Edit
http://4.155.211.247:8081
Autenticaci√≥n web:

makefile
Copy
Edit
Usuario: admin
Contrase√±a: pass
Confirmado listado de bases de datos admin, config y local.

Resultados
MongoDB operativo y accesible en la VM del laboratorio.

Mongo Express funcional como herramienta gr√°fica para administraci√≥n de MongoDB.

Acceso restringido mediante NSG para mejorar seguridad en el laboratorio.

Pr√≥ximos pasos
Implementar RedisInsight para Redis con interfaz web en puerto 8001.

Documentar el despliegue de herramientas gr√°ficas para Redis y validar conectividad.

Consolidar documentaci√≥n para entrega final del laboratorio.

Registro de tarea ‚Äî Implementaci√≥n de RedisInsight en contenedor Docker
Objetivo
Implementar la interfaz gr√°fica RedisInsight en un contenedor Docker dentro de la VM vm-cftec-m62025-SINT646-lab01, para administrar y monitorear la base de datos Redis desplegada previamente.

Procedimiento
1. Preparaci√≥n
Verificar que Redis est√° corriendo en el contenedor redis en el puerto 6379.

Confirmar que el puerto 8001 est√° abierto en el NSG para la IP del cliente:

bash
Copy
Edit
terraform -chdir=terraform state show azurerm_network_security_group.lab_nsg | grep -A5 8001
2. Primer intento con imagen latest
bash
Copy
Edit
docker run -d \
  --name redisinsight \
  -p 8001:8001 \
  --restart unless-stopped \
  redislabs/redisinsight:latest
Problema encontrado:

El contenedor quedaba detenido en:

sql
Copy
Edit
Running docker-entry.sh
El puerto 8001 aparec√≠a abierto, pero al acceder devolv√≠a:

nginx
Copy
Edit
ERR_CONNECTION_REFUSED
curl http://localhost:8001 devolv√≠a Connection reset by peer.

Logs sin informaci√≥n adicional despu√©s del arranque.

3. Diagn√≥stico
Confirmado que no era un problema de firewall/NSG (regla ya habilitada).

Revisado que el contenedor corr√≠a (docker ps) pero sin inicializar la UI.

Conclusi√≥n: bug en la imagen latest.

4. Soluci√≥n aplicada
Ejecutar RedisInsight con versi√≥n estable 1.14.0:

bash
Copy
Edit
docker run -d \
  --name redisinsight \
  -p 8001:8001 \
  --restart unless-stopped \
  redislabs/redisinsight:1.14.0
Esta versi√≥n inicializ√≥ correctamente y mostr√≥ la UI.

5. Validaci√≥n de acceso
Acceso exitoso desde el navegador:

cpp
Copy
Edit
http://4.155.211.247:8001
Pantalla inicial solicitando conectar una base de datos Redis.

Verificaci√≥n de logs:

bash
Copy
Edit
docker logs redisinsight
Mostr√≥ inicio correcto del servicio.

Resultado
RedisInsight desplegado correctamente.

Interfaz disponible en http://4.155.211.247:8001.

Lista para configurar conexi√≥n al contenedor redis local.

Notas
Para producci√≥n, considerar versi√≥n 2.x de RedisInsight (requiere ajustes de imagen y compatibilidad).

Mantener el puerto 8001 abierto solo para IP autorizada por seguridad.

RedisInsight 1.x est√° en End of Life, pero se mantiene en este laboratorio por simplicidad y estabilidad.

Registro de tarea ‚Äî Implementaci√≥n de HBase en contenedor Docker
Objetivo
Implementar Apache HBase en contenedor Docker con sus puertos de administraci√≥n accesibles v√≠a web, permitiendo monitorear el estado de Master y RegionServer.

Procedimiento
1. Preparaci√≥n
Verificar que Docker est√© instalado y corriendo.

Abrir puertos 16010 (Master UI) y 16030 (RegionServer UI) en el NSG para la IP autorizada 190.108.74.42.

2. Implementaci√≥n
bash
Copy
Edit
docker pull harisekhon/hbase:latest
docker volume create hbase_data

docker run -d \
  --name hbase \
  -p 16000:16000 \
  -p 16010:16010 \
  -p 16020:16020 \
  -p 16030:16030 \
  -p 2181:2181 \
  -v hbase_data:/hbase-data \
  harisekhon/hbase
3. Validaci√≥n
Master UI: http://4.155.211.247:16010

RegionServer UI: http://4.155.211.247:16030

Logs:

bash
Copy
Edit
docker logs hbase --tail 20
Mostraron inicio exitoso y disponibilidad de servicios.

Resultado
HBase funcionando y accesible v√≠a web.

Puertos seguros, expuestos solo a IP autorizada.

Preparado para pruebas de integraci√≥n con otras BD del laboratorio.

Registro de tarea ‚Äî Implementaci√≥n de Mongo Express en contenedor Docker
Objetivo
Implementar Mongo Express como interfaz web para administrar la base de datos MongoDB ya desplegada en contenedor Docker.

Procedimiento
1. Preparaci√≥n
Confirmar que MongoDB est√° en ejecuci√≥n (docker ps).

Abrir puerto 8081 en el NSG para la IP 190.108.74.42.

2. Implementaci√≥n
bash
Copy
Edit
docker run -d \
  --name mongo-express \
  -p 8081:8081 \
  -e ME_CONFIG_MONGODB_ADMINUSERNAME=admin \
  -e ME_CONFIG_MONGODB_ADMINPASSWORD=admin123 \
  -e ME_CONFIG_MONGODB_SERVER=mongodb \
  --link mongodb:mongodb \
  mongo-express:latest
Nota: En laboratorio se us√≥ admin / pass para simplificar autenticaci√≥n.

3. Validaci√≥n
Acceso web: http://4.155.211.247:8081

Visualizaci√≥n de bases de datos: admin, config, local.

Logs:

bash
Copy
Edit
docker logs mongo-express --tail 20
Mostraron conexi√≥n establecida a MongoDB.

Resultado
Interfaz web funcional para gesti√≥n de MongoDB.

Puertos y acceso restringidos a IP autorizada.

Configuraci√≥n v√°lida para fines de laboratorio.

Registro de tarea ‚Äî Implementaci√≥n de RedisInsight en contenedor Docker
Objetivo
Implementar RedisInsight para administraci√≥n gr√°fica de Redis en contenedor Docker, facilitando visualizaci√≥n y configuraci√≥n.

Procedimiento
1. Preparaci√≥n
Confirmar Redis en ejecuci√≥n (docker ps).

Abrir puerto 8001 en el NSG para la IP 190.108.74.42.

2. Problemas detectados
Imagen latest de RedisInsight no inicializaba correctamente.

Logs mostraban:

sql
Copy
Edit
Running docker-entry.sh
y no continuaba.

curl http://localhost:8001 devolv√≠a Connection reset by peer.

3. Soluci√≥n aplicada
Usar versi√≥n estable 1.14.0:

bash
Copy
Edit
docker run -d \
  --name redisinsight \
  -p 8001:8001 \
  --restart unless-stopped \
  redislabs/redisinsight:1.14.0
4. Validaci√≥n
Acceso exitoso: http://4.155.211.247:8001

Pantalla inicial solicitando conexi√≥n a Redis existente.

Logs mostraron inicio correcto.

Resultado
RedisInsight desplegado y accesible v√≠a web.

Configuraci√≥n v√°lida para entornos de laboratorio.

Puerto expuesto solo a IP autorizada.

Registro de tarea ‚Äî Configuraci√≥n de Jupyter Notebook en VM para Laboratorio 1
Objetivo
Preparar un entorno Jupyter Notebook accesible v√≠a navegador, protegido por contrase√±a, para desarrollar el Laboratorio 1 de la asignatura SINT646 ‚Äî Deep Learning y Big Data con Python. El notebook servir√° para trabajar con MongoDB, Redis y HBase en contenedores Docker y realizar las pruebas solicitadas.

Acciones ejecutadas
Instalaci√≥n de dependencias base

Actualizaci√≥n de paquetes en la VM.

Instalaci√≥n de python3, python3-pip y compiladores necesarios para librer√≠as Python.

Instalaci√≥n de librer√≠as requeridas para el laboratorio:

bash
Copy
Edit
pip3 install pymongo redis happybase thriftpy2 pandas
Instalaci√≥n de Jupyter Notebook

Instalaci√≥n de notebook<7 para evitar problemas de compatibilidad con Python 3.8.

Ajuste de versiones de jinja2 y markupsafe para corregir error de importaci√≥n.

Instalaci√≥n en el entorno de usuario (--user) para evitar conflictos con paquetes del sistema.

Configuraci√≥n de Jupyter Notebook

Generaci√≥n de archivo de configuraci√≥n:

bash
Copy
Edit
jupyter notebook --generate-config
Edici√≥n del archivo ~/.jupyter/jupyter_notebook_config.py para:

Escuchar en todas las interfaces (0.0.0.0).

Definir puerto fijo 8888.

Deshabilitar token y usar contrase√±a hash.

Deshabilitar apertura autom√°tica de navegador en el servidor.

Generaci√≥n de hash para contrase√±a simple pass:

bash
Copy
Edit
from notebook.auth import passwd
passwd()
Hash aplicado:

perl
Copy
Edit
argon2:$argon2id$v=19$m=10240,t=10,p=8$BcymMp8qCSRjbB29A7lACQ$xDsP/i38TtfPfRh6raE2z1QSRpDN7ZsiDKAzFyDc5Ik
Ejecuci√≥n persistente con tmux

Instalaci√≥n y verificaci√≥n de tmux.

Creaci√≥n de sesi√≥n persistente:

bash
Copy
Edit
tmux new -s jupyterlab
Ejecuci√≥n de Jupyter dentro de tmux:

bash
Copy
Edit
jupyter notebook
Desacople de sesi√≥n (Ctrl+B luego D) para mantener el servicio activo tras cerrar SSH.

Acceso desde el navegador

Acceso v√≠a:

cpp
Copy
Edit
http://4.155.211.247:8888
Login con contrase√±a simple: pass.

Confirmaci√≥n de funcionamiento correcto y acceso a interfaz vac√≠a lista para cargar notebooks.

Problemas enfrentados y soluciones
Error de importaci√≥n soft_unicode en MarkupSafe
üîπ Soluci√≥n: fijar versi√≥n compatible de jinja2==3.0.3 y markupsafe==2.0.1.

Versi√≥n de Notebook 7 no compatible con Python 3.8
üîπ Soluci√≥n: instalar notebook<7.

P√©rdida del token al ejecutar en SSH
üîπ Soluci√≥n: configuraci√≥n sin token y con contrase√±a hash.

Riesgo de detener Jupyter al cerrar SSH
üîπ Soluci√≥n: uso de tmux para mantener la sesi√≥n persistente.

Pr√≥ximos pasos
Crear el notebook base para Laboratorio 1.

Configurar conexiones Python a MongoDB, Redis y HBase en dicho notebook.

Implementar carga del dataset y consultas solicitadas.

Comparar tiempos y documentar resultados.

---
las URLs de acceso quedar√≠an as√≠:

Servicio	Puerto	URL de acceso
Mongo Express	8081	http://4.155.211.247:8081
RedisInsight	8001	http://4.155.211.247:8001
HBase Master UI	16010	http://4.155.211.247:16010
HBase RegionServer UI	16030	http://4.155.211.247:16030
Jupyter Notebook	8888	http://4.155.211.247:8888

‚ö†Ô∏è Credenciales

Mongo Express ‚Üí Usuario: admin / Clave: pass

RedisInsight ‚Üí Sin clave inicial (se configura al entrar)

Jupyter Notebook ‚Üí Clave: pass

---

Registro de tarea ‚Äì Jupyter Notebook no disponible tras reinicio de VM
Fecha/Hora: 2025-08-06
Recurso afectado: vm-cftec-m62025-SINT646-lab01
Servicio(s) implicado(s): Jupyter Notebook, MongoDB, Redis, HBase, RedisInsight

Resumen
Despu√©s de reiniciar la m√°quina virtual, el servicio de Jupyter Notebook dej√≥ de responder en el puerto 8888 (ERR_CONNECTION_REFUSED).
Esto se debe a que el proceso de Jupyter no se inicia autom√°ticamente al reiniciar la VM y tampoco estaba corriendo en un proceso persistente.

Acciones ejecutadas
Verificaci√≥n de conectividad:

Confirmado que el puerto 8888 est√° permitido en el NSG para la IP del usuario.

Verificado que no hay reglas de firewall adicionales que bloqueen el acceso.

An√°lisis del proceso:

Validado que no existe ninguna sesi√≥n tmux activa con Jupyter Notebook (tmux attach -t jupyterlab ‚Üí no sessions).

Determinado que el proceso no sobrevive a reinicios.

Propuesta de soluci√≥n inmediata:

Iniciar manualmente Jupyter en una nueva sesi√≥n tmux:

bash
Copy
Edit
tmux new -s jupyterlab
jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser
Luego salir de tmux con Ctrl+B y D.

Propuesta de soluci√≥n permanente:

Modificar el script restart_lab_services.sh para que Jupyter Notebook se ejecute autom√°ticamente en un tmux al reiniciar la VM:

bash
Copy
Edit
tmux new -d -s jupyterlab "jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser"
Definici√≥n del problema (perspectiva cliente)
Despu√©s de reiniciar la VM, los servicios de laboratorio deben estar disponibles sin intervenci√≥n manual. Actualmente, MongoDB, Redis, HBase y RedisInsight se inician autom√°ticamente, pero Jupyter Notebook requiere ejecuci√≥n manual.

Pr√≥ximos pasos acordados
Actualizar restart_lab_services.sh para incluir el arranque autom√°tico de Jupyter en tmux.

Validar despu√©s del pr√≥ximo reinicio que Jupyter est√© disponible en el puerto 8888 sin intervenci√≥n manual.

Documentar procedimiento para levantarlo manualmente en caso de emergencia.

---

Registro de tarea ‚Äî Limpieza y Preprocesamiento del Dataset
Resumen:
Se realiz√≥ la limpieza y preprocesamiento del dataset kz.csv proveniente del conjunto E-commerce Purchase History from Electronics Store. El dataset original conten√≠a 2,633,521 registros y 8 columnas. Se identificaron valores nulos en varias columnas, especialmente en category_id, category_code, brand, price y user_id.

Acciones ejecutadas:

Carga inicial del dataset en un DataFrame de Pandas.

An√°lisis exploratorio inicial para identificar:

Tipo de datos por columna.

N√∫mero total de registros.

Cantidad de valores nulos por columna.

Estrategia de preprocesamiento definida:

Mantener todas las columnas para consistencia con las otras bases de datos.

Eliminar o imputar valores nulos solo si impactan en los c√°lculos solicitados.

No realizar transformaciones destructivas sobre price o brand sin an√°lisis posterior.

Verificaci√≥n de memoria para asegurar que las operaciones no saturen la VM.

Guardado del DataFrame limpio para su uso posterior en inserci√≥n en MongoDB, Redis y HBase.

Definici√≥n del problema desde la perspectiva del cliente:
Necesitamos garantizar que el dataset est√© limpio y consistente para que los resultados de las consultas comparativas entre bases de datos sean fiables y no est√©n sesgados por datos faltantes o inconsistentes.

Pr√≥ximos pasos:

Verificar nuevamente valores nulos en el dataset limpio.

Realizar inserci√≥n por bloques en MongoDB, Redis y HBase para evitar saturar la VM.

Documentar tiempos de inserci√≥n y respuesta para consultas clave.

---

Registro de tarea ‚Äî Limpieza y Preprocesamiento del Dataset (Actualizaci√≥n)
Hallazgos tras la verificaci√≥n de valores nulos:

Columna	Valores Nulos	% del Total aprox.
event_time	0	0.00%
order_id	0	0.00%
product_id	0	0.00%
category_id	431,954	16.40%
category_code	612,202	23.20%
brand	506,005	19.20%
price	431,954	16.40%
user_id	2,069,352	78.60%

Conclusiones de esta verificaci√≥n:

event_time, order_id y product_id est√°n completos.

user_id presenta una ausencia significativa (~78%), lo que lo hace poco confiable para an√°lisis directos.

Las columnas de categor√≠a (category_id, category_code), brand y price presentan un porcentaje relevante de nulos.

No se ha aplicado imputaci√≥n o eliminaci√≥n de registros todav√≠a para conservar la integridad y representatividad del dataset.

Pr√≥ximo paso inmediato:

Mantener el dataset tal cual para inserci√≥n en MongoDB, Redis y HBase, documentando el porcentaje de nulos para que se considere en el an√°lisis de consultas.

Evaluar m√°s adelante si se imputan o eliminan estos nulos dependiendo de los requisitos de las consultas comparativas.

---

Registro de tarea ‚Äî Carga del dataset en Redis
Objetivo
Cargar el dataset E-commerce Purchase History en Redis de forma controlada, optimizando el rendimiento y evitando la duplicaci√≥n de datos provenientes de ejecuciones anteriores.

Acciones ejecutadas
Conexi√≥n a Redis usando redis-py (redis.Redis()), con verificaci√≥n de disponibilidad mediante ping().

Lectura del dataset en pandas.DataFrame desde la ruta ./datasets/ecommerce/kz.csv.

Definici√≥n de CHUNK_SIZE = 100_000 para realizar inserciones en bloques y reducir el riesgo de saturar la VM.

Eliminaci√≥n previa de datos antiguos:

Identificaci√≥n de claves con patr√≥n purchase:*.

Eliminaci√≥n en lotes de hasta 10‚ÄØ000 claves por operaci√≥n para no saturar Redis.

Inserci√≥n de datos en Redis:

Uso de pipeline para agrupar m√∫ltiples operaciones y mejorar el rendimiento.

Conversi√≥n de valores NaN a cadenas vac√≠as ("") para evitar incompatibilidades.

Almacenamiento de cada registro como un hash en Redis con clave purchase:<√≠ndice>.

Medici√≥n de tiempos:

Tiempo por bloque.

Tiempo total de inserci√≥n.

Definici√≥n del problema (desde la perspectiva del laboratorio)
La carga de un dataset de m√°s de 2.6 millones de registros en Redis puede provocar:

Saturaci√≥n de CPU y memoria si se intenta insertar todo en una sola operaci√≥n.

Duplicaci√≥n de datos si no se eliminan cargas anteriores.

Latencia en inserci√≥n si no se optimiza la escritura.

Resultados
Conexi√≥n: Redis acept√≥ conexiones desde la VM sin errores.

Borrado de datos previos: Eliminadas todas las claves antiguas purchase:* antes de la nueva carga.

Inserci√≥n optimizada: Uso de pipeline y carga en bloques permiti√≥ procesar el dataset sin saturar la VM.

Datos accesibles: Los registros son consultables con comandos como:

bash
Copy
Edit
redis-cli HGETALL purchase:0
Tiempo total: Registrado al finalizar el proceso, junto con tiempos por bloque.

Pr√≥ximos pasos
Repetir el procedimiento para HBase siguiendo la misma estrategia de:

Limpieza previa.

Inserci√≥n por bloques.

Medici√≥n de tiempos.

Comparar los tiempos de inserci√≥n y consulta entre MongoDB, Redis y HBase.

---
üìÑ Registro de Tarea ‚Äî Carga del dataset en Redis
Actividad: Inserci√≥n del dataset limpio en Redis en bloques de 100‚ÄØ000 registros utilizando pipeline para optimizar el rendimiento.
Objetivo: Medir rendimiento y consumo de recursos durante la carga.

‚öôÔ∏è Configuraci√≥n de prueba
Dataset: 2‚ÄØ633‚ÄØ521 registros (kz.csv)

Bloques de inserci√≥n: 100‚ÄØ000 registros por batch

Redis: Contenedor Docker redis:latest

VM: Standard_A4m_v2 ‚Äî 8 vCPU, 32‚ÄØGB RAM

Script: Python con redis-py y pipeline(transaction=False)

üìä M√©tricas de rendimiento (Azure Monitor)
Periodo observado: durante toda la inserci√≥n del dataset.

M√©trica	Valor Promedio	Observaciones
CPU (Percentage CPU)	~20‚ÄØ% (picos 65‚ÄØ%)	Incrementos durante los batches, con ca√≠das entre lotes.
Memoria disponible (Available Memory %)	~84‚ÄØ%	Uso moderado; Redis maneja los datos en memoria eficientemente.
Data Disk IOPS Consumed %	Bajo	No hubo saturaci√≥n de IOPS, Redis es predominantemente in-memory.
Data Disk Latency	Casi nulo	Escritura muy r√°pida por ser en memoria; m√≠nima espera en disco.
Data Disk Read/Write Bytes/Sec	Lectura m√≠nima / Escritura muy baja	No hubo dependencia fuerte de disco persistente.

üìù Notas
La carga en bloques evita saturar CPU y memoria.

Redis respondi√≥ r√°pidamente debido a su naturaleza en memoria, con baja latencia.

Redis se comporta mejor que MongoDB en t√©rminos de uso de CPU y disco para esta etapa, aunque la persistencia depende de snapshots y AOF si se habilitan.

En cargas repetidas es clave eliminar previamente claves antiguas para evitar duplicados (DEL purchase:*).

---

üìÑ Registro de Tarea ‚Äî Carga del dataset en MongoDB
Actividad: Inserci√≥n del dataset limpio en MongoDB en bloques de 100‚ÄØ000 registros utilizando insert_many() para optimizar el rendimiento.
Objetivo: Cargar el dataset completo midiendo tiempos por bloque y consumo de recursos.

‚öôÔ∏è Configuraci√≥n de prueba
Dataset: 2‚ÄØ633‚ÄØ521 registros (kz.csv)

Bloques de inserci√≥n: 100‚ÄØ000 registros por batch (√∫ltimo bloque de 33‚ÄØ521)

MongoDB: Contenedor Docker mongo:6.0 con autenticaci√≥n admin / pass

VM: Standard_A4m_v2 ‚Äî 8 vCPU, 32‚ÄØGB RAM

Script: Python con pymongo, limpieza previa de la colecci√≥n (drop()) para evitar duplicados

üìä Tiempos de inserci√≥n
Promedio por bloque: ~4.3 segundos
Tiempo total: 114.76 segundos
Total documentos insertados: 2‚ÄØ633‚ÄØ521

Ejemplo de ejecuci√≥n:

yaml
Copy
Edit
üßπ Colecci√≥n limpiada antes de la inserci√≥n.
‚úÖ Bloque 1: 100000 registros (4.19 seg)
‚úÖ Bloque 2: 100000 registros (4.67 seg)
...
‚úÖ Bloque 26: 100000 registros (4.36 seg)
‚úÖ Bloque 27: 33521 registros (1.56 seg)
‚è± Tiempo total: 114.76 seg
üìä Total documentos insertados: 2633521
üìä M√©tricas de rendimiento (Azure Monitor)
Periodo observado: durante la inserci√≥n del dataset.

M√©trica	Valor Promedio	Observaciones
CPU (Percentage CPU)	~20‚Äì25‚ÄØ% (picos >50‚ÄØ%)	Actividad constante durante cada batch.
Memoria disponible (Available Memory %)	~80‚Äì82‚ÄØ%	MongoDB usa memoria para cache/buffers, estable en la prueba.
Data Disk IOPS Consumed %	Moderado	Picos coinciden con inserciones en disco.
Data Disk Latency	Baja	MongoDB maneja escritura r√°pida con journaling activo.
Data Disk Read/Write Bytes/Sec	Escrituras constantes	La escritura crece proporcional al tama√±o del batch insertado.

üìù Notas
MongoDB consume m√°s I/O que Redis en la carga inicial debido a la persistencia inmediata en disco.

La carga por lotes de 100‚ÄØ000 evita saturaci√≥n y mantiene uso estable de CPU y memoria.

Es fundamental limpiar la colecci√≥n antes de una nueva inserci√≥n para evitar duplicados (drop()).

---

üìÑ Registro de Tarea ‚Äî Carga del dataset en Redis
Actividad: Inserci√≥n del dataset limpio en Redis en bloques de 100‚ÄØ000 registros utilizando pipeline.hset() para maximizar el rendimiento.
Objetivo: Cargar el dataset completo en Redis, midiendo tiempos por bloque y monitoreando el consumo de recursos de la VM.

‚öôÔ∏è Configuraci√≥n de prueba
Dataset: 2‚ÄØ633‚ÄØ521 registros (kz.csv)

Bloques de inserci√≥n: 100‚ÄØ000 registros por batch (√∫ltimo bloque de 33‚ÄØ521)

Redis: Contenedor Docker redis:7

VM: Standard_A4m_v2 ‚Äî 8 vCPU, 32‚ÄØGB RAM

Script: Python con redis-py, limpieza previa de claves (r.keys("purchase:*")) para evitar duplicados.

üìä Tiempos de inserci√≥n
Promedio por bloque: ~40.5 segundos
Tiempo total: 1‚ÄØ065.80 segundos (~17.8 min)
Total documentos insertados: 2‚ÄØ633‚ÄØ521

Ejemplo de ejecuci√≥n:

yaml
Copy
Edit
‚úÖ Conectado a Redis
üì¶ Total de registros en dataset: 2,633,521
üßπ Eliminadas 1,200,000 claves antiguas en Redis
‚úÖ Bloque 1: 100,000 registros en 41.48 seg
...
‚úÖ Bloque 26: 100,000 registros en 40.33 seg
‚úÖ Bloque 27: 33,521 registros en 13.37 seg
üèÅ Inserci√≥n total completada en 1065.80 segundos
üìä M√©tricas de rendimiento (Azure Monitor)
Periodo observado: durante la inserci√≥n del dataset.

M√©trica	Valor Promedio	Observaciones
CPU (Percentage CPU)	~20‚Äì35‚ÄØ% (picos cercanos a 60‚ÄØ%)	Picos al inicio de cada bloque.
Memoria disponible (Available Memory %)	~80‚Äì84‚ÄØ%	Redis almacena todo en memoria, estable durante la carga.
Data Disk IOPS Consumed %	Bajo	Redis es in-memory, poca escritura directa a disco.
Data Disk Latency	Muy baja	Sin impacto notable en el rendimiento.
Data Disk Write Bytes/Sec	Bajo	Ligero aumento por persistencia de snapshots (RDB).

üìù Notas
Redis es significativamente m√°s lento que MongoDB en esta carga debido a la inserci√≥n de hashes individuales para cada registro.

El uso de pipeline redujo la latencia de red, pero la operaci√≥n sigue siendo CPU-bound y single-threaded en el proceso de escritura.

Eliminar las claves antiguas antes de la inserci√≥n es esencial para evitar duplicados y consumo excesivo de memoria.

Si se prioriza la velocidad sobre la persistencia, se podr√≠a desactivar temporalmente el guardado RDB/AOF durante la carga.

---

Registro de Tarea ‚Äî Carga del Dataset en HBase
Objetivo: Insertar el dataset limpio de compras electr√≥nicas en HBase utilizando inserci√≥n por bloques para evitar saturaci√≥n de recursos.

Acciones Ejecutadas
Conexi√≥n a HBase mediante happybase (Thrift en puerto 9090).

Creaci√≥n de la tabla purchases con familia de columnas cf si no exist√≠a.

Limpieza previa de la tabla (eliminaci√≥n de registros antiguos) para evitar duplicados.

Carga del dataset kz.csv (2‚ÄØ633‚ÄØ521 registros) usando bloques de 100‚ÄØ000 registros.

Inserci√≥n optimizada utilizando batch() para reducir overhead de conexi√≥n.

Ejecuci√≥n
yaml
Copy
Edit
‚úÖ Conectado a HBase
üÜï Tabla creada: purchases
üßπ Limpiando registros antiguos de la tabla...
üßπ Tabla vac√≠a.

üì¶ Total de registros en dataset: 2,633,521
‚úÖ Bloque 1: 100,000 registros en 50.46 segundos
‚úÖ Bloque 2: 100,000 registros en 45.50 segundos
‚úÖ Bloque 3: 100,000 registros en 45.49 segundos
‚úÖ Bloque 4: 100,000 registros en 46.09 segundos
‚úÖ Bloque 5: 100,000 registros en 46.36 segundos
‚úÖ Bloque 6: 100,000 registros en 46.98 segundos
‚úÖ Bloque 7: 100,000 registros en 47.41 segundos
‚úÖ Bloque 8: 100,000 registros en 46.85 segundos
‚úÖ Bloque 9: 100,000 registros en 46.06 segundos
‚úÖ Bloque 10: 100,000 registros en 45.91 segundos
‚úÖ Bloque 11: 100,000 registros en 46.08 segundos
‚úÖ Bloque 12: 100,000 registros en 45.59 segundos
‚úÖ Bloque 13: 100,000 registros en 47.15 segundos
‚úÖ Bloque 14: 100,000 registros en 45.59 segundos
‚úÖ Bloque 15: 100,000 registros en 45.02 segundos
‚úÖ Bloque 16: 100,000 registros en 47.15 segundos
‚úÖ Bloque 17: 100,000 registros en 47.44 segundos
‚úÖ Bloque 18: 100,000 registros en 49.85 segundos
‚úÖ Bloque 19: 100,000 registros en 47.66 segundos
‚úÖ Bloque 20: 100,000 registros en 46.70 segundos
‚úÖ Bloque 21: 100,000 registros en 45.63 segundos
‚úÖ Bloque 22: 100,000 registros en 46.35 segundos
‚úÖ Bloque 23: 100,000 registros en 46.87 segundos
‚úÖ Bloque 24: 100,000 registros en 45.44 segundos
‚úÖ Bloque 25: 100,000 registros en 46.87 segundos
‚úÖ Bloque 26: 100,000 registros en 45.85 segundos
‚úÖ Bloque 27: 33,521 registros en 15.18 segundos

üèÅ Inserci√≥n total completada en 1227.67 segundos
Observaciones de Rendimiento
Inserci√≥n estable en la mayor√≠a de bloques (~45‚Äì47 segundos/bloque).

Bloques iniciales ligeramente m√°s lentos por la creaci√≥n y preparaci√≥n de la tabla.

Uso de batch() en HappyBase ayud√≥ a mantener la latencia de escritura constante.

El rendimiento general fue m√°s lento que en MongoDB y Redis, consistente con el dise√±o de HBase orientado a escritura masiva distribuida.

---

Registro de tarea exitosa ‚Äî Ajuste de configuraci√≥n HBase en contenedor Docker y validaci√≥n de lectura desde Jupyter Notebook
Objetivo
Ajustar la configuraci√≥n de HBase en el contenedor hbase para evitar errores de conexi√≥n (Broken pipe) durante consultas masivas desde Python usando happybase.
Validar que despu√©s del cambio la base de datos es accesible y que la tabla purchases puede ser le√≠da desde el laboratorio en Jupyter Notebook.

Pasos ejecutados
1. Identificaci√≥n del contenedor HBase en la VM
Desde la sesi√≥n SSH en la VM vm-cftec-m62025-SINT646-lab01 se listaron los contenedores activos:

bash
Copy
Edit
docker ps --format "table {{.ID}}\t{{.Image}}\t{{.Names}}\t{{.Status}}\t{{.Ports}}"
Se identific√≥ el contenedor de HBase:

bash
Copy
Edit
CONTAINER ID   IMAGE                   NAMES
4bbf4771109c   harisekhon/hbase:latest hbase
2. Acceso al contenedor HBase
Se abri√≥ una sesi√≥n interactiva en el contenedor:

bash
Copy
Edit
docker exec -it hbase bash
3. Localizaci√≥n del archivo de configuraci√≥n
Dentro del contenedor se localiz√≥ hbase-site.xml:

bash
Copy
Edit
find / -name "hbase-site.xml" 2>/dev/null
Resultado:

bash
Copy
Edit
/hbase-2.1.3/conf/hbase-site.xml
4. Edici√≥n de la configuraci√≥n
Se edit√≥ el archivo con vi:

bash
Copy
Edit
vi /hbase-2.1.3/conf/hbase-site.xml
A√±adiendo / modificando los par√°metros para aumentar tiempo de espera y reducir tama√±o de lotes:

xml
Copy
Edit
<property>
    <name>hbase.rpc.timeout</name>
    <value>600000</value>
</property>

<property>
    <name>hbase.client.scanner.timeout.period</name>
    <value>600000</value>
</property>

<property>
    <name>hbase.regionserver.lease.period</name>
    <value>600000</value>
</property>

<property>
    <name>hbase.client.scanner.caching</name>
    <value>500</value>
</property>
Nota: Los valores est√°n en milisegundos (600000 = 10 minutos).
scanner.caching ajustado a 500 para evitar env√≠o de lotes muy grandes.

5. Reinicio del contenedor HBase
Se guardaron los cambios y se reinici√≥ el contenedor:

bash
Copy
Edit
docker restart hbase
Confirmando que volvi√≥ a estar activo:

bash
Copy
Edit
docker ps
6. Validaci√≥n de conexi√≥n desde Jupyter Notebook
En el laboratorio .ipynb se verific√≥ la conexi√≥n:

python
Copy
Edit
import happybase
connection = happybase.Connection(host='localhost', port=9090)
connection.open()
print(connection.tables())
Salida esperada:

css
Copy
Edit
[b'purchases']
7. Prueba de lectura
Se consultaron las primeras 3 filas:

python
Copy
Edit
table = connection.table('purchases')
for key, data in table.scan(limit=3):
    print(key, data)
Resultado exitoso:

bash
Copy
Edit
b'2294359932054536986' {b'cf:brand': b'samsung', b'cf:price': b'162.01', ...}
b'2294444024058086220' {b'cf:brand': b'huawei', b'cf:price': b'77.52', ...}
b'2294584263154074236' {b'cf:brand': b'karcher', b'cf:price': b'217.57', ...}
Resultado
‚úÖ Ajustes de configuraci√≥n aplicados correctamente en HBase.
‚úÖ Conexi√≥n establecida con √©xito desde Jupyter Notebook.
‚úÖ Lectura de la tabla purchases funcionando sin errores.

Si quieres, puedo ahora prepararte otro registro de tarea exitosa para la ejecuci√≥n del script optimizado que probaremos despu√©s para verificar que el Broken pipe ya no ocurre.